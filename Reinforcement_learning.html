
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Reinforcement Learning &#8212; Future Financial Planning Tools for Consumers</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Financial Applications of Reinforcement Learning" href="Financial_application.html" />
    <link rel="prev" title="Abstract" href="abstract.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      <h1 class="site-logo" id="site-title">Future Financial Planning Tools for Consumers</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="abstract.html">
   Abstract
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Financial_application.html">
   Financial Applications of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Discussion.html">
   discussion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Reinforcement_learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Reinforcement_learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finite-markov-decision-processes">
   Finite Markov Decision Processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-policy-iteration-model-based-rl-and-model-free-rl">
   Generalized Policy Iteration, Model-based RL and Model-free RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dynamic-programming-monte-carlo-methods-and-temporal-difference-learning">
     Dynamic Programming, Monte Carlo Methods and Temporal-Difference Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensions-of-a-reinforcement-learning-algorithm">
   Dimensions of a Reinforcement Learning Algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#curse-of-dimensionality-and-generalization">
   Curse of Dimensionality and generalization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#function-approximation">
     Function Approximation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#g-learning-a-stochastic-adaptation-on-q-learning">
   G-learning, a stochastic adaptation on Q-learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-deep-backward-stochastic-differential-equation-method">
   The Deep Backward Stochastic Differential Equation Method
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="reinforcement-learning">
<h1>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">¶</a></h1>
<p>Supervised and unsupervised learning are the two most widely studied and researched branches of Machine Learning (ML). Besides these two, there is also a third subcategory in ML called Reinforcement Learning (RL). The three branches have fundamental differences. Supervised learning for example is designed to learn from a training set of labeled data, where each element of the training set describes a certain situation and is linked to a label/action the supervisor has provided <span id="id1">[<a class="reference internal" href="Appendix.html#id68">Ham18</a>]</span>. On the other hand, RL is a method in which the machine tries to map situations to actions by maximizing a reward signal <span id="id2">[<a class="reference internal" href="Appendix.html#id71">ADBB17</a>]</span>. The two methods are fundamentally different from each other in the fact that in RL there is no supervisor which provides the label/action the machine needs to take, rather there is a reward system set up from which the machine can learn the correct action/label <span id="id3">[<a class="reference internal" href="Appendix.html#id68">Ham18</a>]</span>. Contrarily to supervised learning, unsupervised learning tries to find hidden structures within an unlabeled dataset. This might seem similar to RL as both methods work with unlabeled datasets, but RL tries to maximize a reward signal instead of finding only hidden structures in the data <span id="id4">[<a class="reference internal" href="Appendix.html#id71">ADBB17</a>]</span>. Unsupervised learning on the other hand learns about how the data is distributed <span id="id5">[<a class="reference internal" href="Appendix.html#id61">SBLL19</a>]</span>.</p>
<p>RL finds its roots in multiple research fields. Each of these fields contributes to the RL in its own unique way <span id="id6">[<a class="reference internal" href="Appendix.html#id68">Ham18</a>]</span>. For example, RL is similar to natural learning processes where the learning method is by experiencing many failures and successes. Therefore psychologists have used RL to mimic psychological processes when an organism makes choices based on experienced rewards/punishments <span id="id7">[<a class="reference internal" href="Appendix.html#id76">EWC21</a>]</span>. While psychologists are mimicking psychological processes, neuroscientists are using RL to focus on a well-defined network of regions of the brain that implement value learning <span id="id8">[<a class="reference internal" href="Appendix.html#id76">EWC21</a>]</span>. In Robotics it is used to help learn drones fly in mid-air <span id="id9">[<a class="reference internal" href="Appendix.html#id30">AKAM+21</a>]</span>. RL finds also applications in health care, Natural Language Processing, Management of Limited Resources, etc… <span id="id10">[<a class="reference internal" href="Appendix.html#id29">NRC20</a>]</span>. The main subfields of RL are presented in <a class="reference internal" href="#tree-fig"><span class="std std-numref">Figure 1</span></a>. The reason why RL can be applied to a range of research fields is that it uses a natural learning process based on a trial and error method as we will see in the next subsection.</p>
<div class="figure align-default" id="tree-fig">
<a class="reference internal image-reference" href="_images/tree.png"><img alt="_images/tree.png" src="_images/tree.png" style="width: 600px; height: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">research fields involved in reinforcement learning</span><a class="headerlink" href="#tree-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="finite-markov-decision-processes">
<h2>Finite Markov Decision Processes<a class="headerlink" href="#finite-markov-decision-processes" title="Permalink to this headline">¶</a></h2>
<p>RL can be represented in finite Markov Decision Processes (MDPs), which are classical formalizations of sequential decision making. More specifically, MPDs give rise to a structure in which delayed rewards can be balanced with immediate rewards <span id="id11">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>. Moreover, it enables a straightforward framing for learning from an environment to achieve a goal <span id="id12">[<a class="reference internal" href="Appendix.html#id17">Lev18</a>]</span>. In its simplest form, RL works with an Agent-Environment Interface. The agent is exposed to some representation of the environment’s state <span class="math notranslate nohighlight">\(S_t \in \mathrm{S}\)</span>. From this representation the agent needs to choose an action <span class="math notranslate nohighlight">\( A_t \in \mathcal{A}(s)\)</span>, which will result in a numerical reward <span class="math notranslate nohighlight">\(R_{t+1} \in 	\mathbb{R} \)</span> and a new state <span class="math notranslate nohighlight">\(S_{t+1}\)</span> (see <a class="reference internal" href="#standard-model-fig"><span class="std std-numref">Figure 2</span></a>)  <span id="id13">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>. The goal for the agent is to learn a mapping from states to action called a policy <span class="math notranslate nohighlight">\(\pi\)</span> that maximizes the expected rewards:</p>
<div class="math notranslate nohighlight">
\[ \pi^* = argmax_{\pi} E[R|\pi] \]</div>
<p>If the MPDs are finite and discrete, the sets of states, actions, and rewards (<span class="math notranslate nohighlight">\(S\)</span>, <span class="math notranslate nohighlight">\(A\)</span>, and <span class="math notranslate nohighlight">\(R\)</span>) all have a finite number of elements. The agent-environment interaction can then be subdivided into episode <span id="id14">[<a class="reference internal" href="Appendix.html#id71">ADBB17</a>]</span>.  The agent’s goal is to find the policy which maximizes the expected discounted cumulative return in the episode <span id="id15">[<a class="reference internal" href="Appendix.html#id79">FranccoisLHI+18</a>]</span>.</p>
<div class="math notranslate nohighlight" id="equation-return">
<span class="eqno">(1)<a class="headerlink" href="#equation-return" title="Permalink to this equation">¶</a></span>\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... + \gamma^{T-t-1}R_T = \sum_{k=0}^T \gamma^k R_{t+k+1}\]</div>
<div class="math notranslate nohighlight">
\[ \pi^* = argmax_{\pi} E[\sum_{k=0}^T \gamma^k R_{t+k+1}|\pi]\]</div>
<p>Where T indicates the terminal state and <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount rate. The terminal state <span class="math notranslate nohighlight">\(S_T\)</span> is often followed by a reset to a starting state or sample from a starting distribution of states <span id="id16">[<a class="reference internal" href="Appendix.html#id79">FranccoisLHI+18</a>]</span>. An episode ends once the reset has occurred. The discount rate represents the present value of future rewards. If the dicount rate is zero <span class="math notranslate nohighlight">\(\gamma = 0\)</span>, the agent is myopic and is only concerned with maximizing the immediate rewards. The agent can consequently be considered greedy <span id="id17">[<a class="reference internal" href="Appendix.html#id61">SBLL19</a>]</span>. If the returns are rewritten in a dynamic programming approach, the return of the myopic agent becomes:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1bed6acb-284b-4b61-92e1-90d84b453217">
<span class="eqno">(2)<a class="headerlink" href="#equation-1bed6acb-284b-4b61-92e1-90d84b453217" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\begin{split}
G_t &amp; =  R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ... + \gamma^{T-t-2}R_T) 
\\
&amp; = R_{t+1} + \gamma G_{t+1} \hspace{3cm}  \text{with } \gamma = 0 
\\
 &amp; = R_{t+1} 
\end{split}
\end{equation}\]</div>
<div class="figure align-default" id="standard-model-fig">
<a class="reference internal image-reference" href="_images/standard_model.png"><img alt="_images/standard_model.png" src="_images/standard_model.png" style="width: 600px; height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">standard model reinforcement learning</span><a class="headerlink" href="#standard-model-fig" title="Permalink to this image">¶</a></p>
</div>
<p>A key concept of MPDs is the Markov property: Only the current state affects the next state <span id="id18">[<a class="reference internal" href="Appendix.html#id79">FranccoisLHI+18</a>]</span>. The random variables <span class="math notranslate nohighlight">\(R_t\)</span> and <span class="math notranslate nohighlight">\(S_t\)</span> have with the Markov property well defined discrete transition probability distributions dependent only on the previous state and action:</p>
<div class="math notranslate nohighlight">
\[ p(s', t| s, a) = Pr(S_t = s', R_t = r | S_{t-1} = s, A_{t-1}=a) \]</div>
<p>For all <span class="math notranslate nohighlight">\(s', s \in \mathrm{S} , r \in 	\mathbb{R}, a \in \mathrm{A}(s) \)</span>. The probability of each element in the sets <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(R\)</span> completely characterizes the environment <span id="id19">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>. This is an unrealistic assumption to make, and several algorithms relax the Markov property. The Partial Observable Markov Decision Process (POMDP) algorithm, for example, maintains a belief over the current state given the previous belief state, the action taken, and the current observation <span id="id20">[<a class="reference internal" href="Appendix.html#id71">ADBB17</a>]</span>.  Once <span class="math notranslate nohighlight">\(p\)</span> is known, the environment is fully described and functions like a transition function <span class="math notranslate nohighlight">\(T : D \times A \to p(S)\)</span> and a reward function <span class="math notranslate nohighlight">\(R: S \times A \times S \to \mathbb{R}\)</span> can be deducted <span id="id21">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>.</p>
<p>Most algorithms in RL use a value function to estimate the value of a given state for the agent. Value functions are defined by the policy <span class="math notranslate nohighlight">\(\pi\)</span> the agent decided to take. As mentioned previously, <span class="math notranslate nohighlight">\(\pi\)</span> is the mapping of states to actions. More precisely, the policy <span class="math notranslate nohighlight">\(\pi\)</span> provides us with the probability of selecting the action <span class="math notranslate nohighlight">\(a\)</span> given the state <span class="math notranslate nohighlight">\(s\)</span>. The value function also describes the probability of a reward given the action taken and by consequence, the value function <span class="math notranslate nohighlight">\(v_{\pi}(s)\)</span> in a state <span class="math notranslate nohighlight">\(s\)</span> following a policy <span class="math notranslate nohighlight">\(\pi\)</span> is as follows:</p>
<div class="math notranslate nohighlight" id="equation-value">
<span class="eqno">(3)<a class="headerlink" href="#equation-value" title="Permalink to this equation">¶</a></span>\[v_{\pi}(s) = E_{\pi}[G_t | S_t = s] = E_{\pi}[\sum_{k=0}^T \gamma^kR_{t+k+1} | S_t=s]\]</div>
<p>This can also be rewritten in a dynamic programming approach:</p>
<div class="math notranslate nohighlight" id="equation-bell">
<span class="eqno">(4)<a class="headerlink" href="#equation-bell" title="Permalink to this equation">¶</a></span>\[\begin{split}    v_{\pi}(s) &amp; = E_{\pi}[G_t | S_t = s]
\\
    &amp; = E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] 
\\
    &amp; = \sum_a \pi(a|s) \sum_{s'} \sum_r p(s', r|s,a)[r + \gamma E_{pi}[G_{t+1} | S_{t+1} = s']
\\
    &amp; =  \sum_a \pi(a|s) \sum_{s', r}p(s', r|s,a)[r + \gamma v_{\pi}(s')| S_{t+1} = s'] \end{split}\]</div>
<p>Equation <a class="reference internal" href="#equation-bell">(4)</a> is called the Bellman equation of <span class="math notranslate nohighlight">\(v_{\pi}\)</span>. It describes the relationship between the value of a state and the values of its successor states given a certain policy <span class="math notranslate nohighlight">\(\pi\)</span>. The relation can also be represented by a backup diagram (see <a class="reference internal" href="#backup-diagram-fig"><span class="std std-numref">Figure 3</span></a>). If <span class="math notranslate nohighlight">\(v_{\pi}(s)\)</span> is the value of a given state, then <span class="math notranslate nohighlight">\(q_{\pi}(s,a)\)</span> is the value of a given action of that state:</p>
<div class="math notranslate nohighlight" id="equation-state-action">
<span class="eqno">(5)<a class="headerlink" href="#equation-state-action" title="Permalink to this equation">¶</a></span>\[ q_{\pi}(s,a) = E_{\pi}[G_t | S_t = s, A_t = a] = E_{\pi}[\sum_{k=0}^T \gamma^kR_{t+k+1} | S_t=s, A_t = a] \]</div>
<p>This can be seen in the backup diagram as starting from the black dot and computing the subsequent value thereafter. <span class="math notranslate nohighlight">\(q_{\pi}(s,a)\)</span> is also called the action-value function as it describes each value of an action for each state.</p>
<div class="figure align-default" id="backup-diagram-fig">
<a class="reference internal image-reference" href="_images/backup_diagram.png"><img alt="_images/backup_diagram.png" src="_images/backup_diagram.png" style="width: 300px; height: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">General backup diagram</span><a class="headerlink" href="#backup-diagram-fig" title="Permalink to this image">¶</a></p>
</div>
<p>For the agent, it is important to find the optimal policy which maximizes the expected cumulative rewards. The optimal policy <span class="math notranslate nohighlight">\(\pi_*\)</span> is the policy for which <span class="math notranslate nohighlight">\(v_{\pi_*}(s) &gt; v_{\pi}(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span>. An optimal policy also has the same action-value function <span class="math notranslate nohighlight">\(q_*(s,a)\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span> and <span class="math notranslate nohighlight">\(a \in A\)</span>. The optimal policy does not depend solely on one policy and can encompass multiple policies. It is thus not policy dependent:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3db53222-deb1-47d7-9cf8-7dbab6dc0865">
<span class="eqno">(6)<a class="headerlink" href="#equation-3db53222-deb1-47d7-9cf8-7dbab6dc0865" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\begin{split}
v_*(s) &amp; = max_{a \in A(s)} q_{\pi_*}(s,a) 
\\
&amp; = max_{a} E_{\pi_*}[G_t | S_t=s, A_t=a] 
\\
 &amp; = max_{a} E_{\pi_*}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] 
\\ 
 &amp; = max_{a} E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a]
\end{split}
\end{equation}\]</div>
<p>Once <span class="math notranslate nohighlight">\(v_*(s)\)</span> is found, you need to apply a greedy algorithm as the optimal value function already takes into account the long-term consequences of choosing that action. Finding <span class="math notranslate nohighlight">\(q_*(s, a)\)</span>, makes things even easier, as the action-value function caches the result of all one-step-ahead searches.</p>
<p>Solving the Bellman equation such that we know all possibilities with their probabilities and rewards is in most practical cases impossible. Typically this is  due to three main factors <span id="id22">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>. The first problem is obtaining full knowledge of the dynamics of the environment. The second factor is the computational resources to complete the calculation. The last factor is that the states need to have the Markov property. To circumvent these obstacles, RL tries to approximate the Bellman optimality equation using various methods. In the next chapter, a brief layout of these methods is discussed, focussing on the methods applicable for financial planning.</p>
</div>
<div class="section" id="generalized-policy-iteration-model-based-rl-and-model-free-rl">
<h2>Generalized Policy Iteration, Model-based RL and Model-free RL<a class="headerlink" href="#generalized-policy-iteration-model-based-rl-and-model-free-rl" title="Permalink to this headline">¶</a></h2>
<p>A general theory in finding the optimal policy <span class="math notranslate nohighlight">\(\pi_*\)</span> is called Generalized Policy Iteration (GLI). This method is applied to almost all RL algorithms. The main idea behind GLI is that there is a process that evaluates the value function of the current policy <span class="math notranslate nohighlight">\(\pi\)</span> called policy evaluation and a process that improves the current value function called policy improvement <span id="id23">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>, <span id="id24">[<a class="reference internal" href="Appendix.html#id50">VOW12</a>]</span>. To find the optimal policy these two processes work in tandem with each other as seen in <a class="reference internal" href="#gpi-fig"><span class="std std-numref">Figure 4</span></a> <span id="id25">[<a class="reference internal" href="Appendix.html#id50">VOW12</a>]</span>. Counterintuitively, these processes also work in a conflicting manner as policy improvement makes the policy incorrect and it is thus no longer the same policy <span id="id26">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>,  <span id="id27">[<a class="reference internal" href="Appendix.html#id48">BHB+20</a>]</span>. While policy evaluations create a consistent policy and thus the policy no longer improves upon itself. This idea runs in parallel with the balance between exploration and exploitation in RL.  If the focus lies more on exploration, the agent frequently tries to find states which improve the value function. However, putting more emphasis on exploration is a costly setting as the agent will more frequently choose suboptimal policies to explore the state space. If exploitation is prioritized, the agent will take a long time to find the optimal policy as the agent is likely not to explore new states to improve the policy <span id="id28">[<a class="reference internal" href="Appendix.html#id50">VOW12</a>]</span>.  An <span class="math notranslate nohighlight">\(\varepsilon\)</span>-greedy algorithm is a good example of an algorithm where the balance between exploration and exploitation is important (see next subsection).</p>
<div class="figure align-default" id="gpi-fig">
<a class="reference internal image-reference" href="_images/GPI.png"><img alt="_images/GPI.png" src="_images/GPI.png" style="width: 500px; height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Generalized policy iteration</span><a class="headerlink" href="#gpi-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Reinforcement Learning can be subdivided between model-based RL and model-free RL. In model-free RL the dynamics of the environment are not known. <span class="math notranslate nohighlight">\(\pi_*\)</span> is found by purely interacting with the environment. Meaning that these algorithms do not use transition probability distribution and reward function related to MDP <span id="id29">[<a class="reference internal" href="Appendix.html#id79">FranccoisLHI+18</a>]</span>. Moreover, model-free RL has irreversible access to the environment. Meaning the algorithm has to move forward after an action is taken <span id="id30">[<a class="reference internal" href="Appendix.html#id78">MBJ20b</a>]</span>. Model-based RL on the other hand has reversible access to the environment because they can revert the model and make another trail from the same state <span id="id31">[<a class="reference internal" href="Appendix.html#id8">MBJ20a</a>]</span>. Good examples of model-free RL techniques are the Q-learning and Sarsa. They tend to be used on a variety of tasks, like playing video games and learning complicated locomotion skills <span id="id32">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>. Model-free RL lay at the foundation of RL and the first algorithms to be applied in RL were model-free RL techniques. On the other hand, model-based RL is developed independently and in parallel with planning methods like optimal control and the search community as they both solve the same problem but differ in the approach <span id="id33">[<a class="reference internal" href="Appendix.html#id46">WZZ19</a>]</span>. Most algorithms in model-based RL have a model which describes the dynamics of the environment. They improve the learned value or policy function by sampling from the model <span id="id34">[<a class="reference internal" href="Appendix.html#id8">MBJ20a</a>]</span> (see <a class="reference internal" href="#model-based-rl"><span class="std std-numref">Figure 5</span></a>). This enables the agent to think in advance and as it were plan for possible actions. Model-based reinforcement learning finds thus large similarities with the Planning literature and as a result, a lot of cross-breeding between the two is happening. For example, an extension of the POMP algorithm called Partially Observable Multi-Heuristic Dynamic Programming (POMHDP) is based on recent progress from the search community <span id="id35">[<a class="reference internal" href="Appendix.html#id11">KSL19</a>]</span>. A hybrid version of the two approaches in which the model is learned through interaction with the environment, has also been widely applied. The imagination-augmented agents (12A) for example combine model-based and model-free aspects by employing the predictions as an additional context in a deep policy network <span id="id36">[<a class="reference internal" href="Appendix.html#id78">MBJ20b</a>]</span>.  In the next subsection, three fundamental algorithms in RL are discussed which will enable us to better capture the dimensions and challenges of RL algorithms.</p>
<div class="figure align-default" id="model-based-rl">
<a class="reference internal image-reference" href="_images/model_based_RL.png"><img alt="_images/model_based_RL.png" src="_images/model_based_RL.png" style="width: 400px; height: 275px;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Model-based Reinforcement Learning</span><a class="headerlink" href="#model-based-rl" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="dynamic-programming-monte-carlo-methods-and-temporal-difference-learning">
<h3>Dynamic Programming, Monte Carlo Methods and Temporal-Difference Learning<a class="headerlink" href="#dynamic-programming-monte-carlo-methods-and-temporal-difference-learning" title="Permalink to this headline">¶</a></h3>
<p>The three most fundamental algorithms for RL are Dynamic Programming, Monte Carlo method ,and Temporal-difference Learning.  Dynamic Programming (DP) is known for two algorithms in RL: value iteration (VI) and policy iteration (PI). For both methods, the dynamics of the environment need to be completely known and therefore, they fall under model-based RL. The two algorithms also use a discrete-time, state ,and action MDP as they are iterative procedures. The PI can be subdivided into three steps: initialize, policy evaluation ,and policy improvement <span id="id37">[<a class="reference internal" href="Appendix.html#id50">VOW12</a>]</span>. The first step is to initialize the value function <span class="math notranslate nohighlight">\(v_{\pi}\)</span> by choosing an arbitrary policy <span class="math notranslate nohighlight">\(\pi\)</span>. The following step is to evaluate the function successively by updating the Bellman equation <a class="reference internal" href="#equation-bell">(4)</a>. Updating the Bellman equation is also called the expected update as the equation is updated using the whole state space instead of a sample of the state space. One update is also called a sweep, as the update sweeps through the state space. Once that the value function <span class="math notranslate nohighlight">\(v_{\pi}\)</span> is updated, we know how good it is to follow the current policy. The next step is to deviate from the policy trajectory and choose a different action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> to find a more optimal policy value. We compute the new <span class="math notranslate nohighlight">\(\pi '\)</span> and compare it to the old policy. The new policy is accepted if <span class="math notranslate nohighlight">\(\pi '(s) &gt; \pi(s)\)</span>. This process is repeated until a convergence criterion is met <span id="id38">[<a class="reference internal" href="Appendix.html#id49">PRD96</a>]</span>. The complete algorithm can be found in the appendix. VI combines the policy evaluation with the policy improvement by truncating the sweep with one update of each state. It effectively combines the policy evaluation and policy evaluation in one sweep (see appendix for the algorithm) <span id="id39">[<a class="reference internal" href="Appendix.html#id49">PRD96</a>]</span> . PI and VI are the foundation of DP and numerous adaptions have been made to these algorithms. Although these algorithms do not have a wide application in many fields, they are essential in describing what an RL algorithm effectively tries to approximate <span id="id40">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>.</p>
<p>The Monte Carlo (MC) methods do not assume full knowledge of the dynamics of the environment and are thus considered model-free RL techniques. They only require a sample sequence of states, actions ,and rewards from the interaction of an environment. Technically, a model is still required which generates sample transitions, but the complete probability distribution <span class="math notranslate nohighlight">\(p\)</span> of the dynamic system is not necessary. The idea behind almost all MC methods is that the agent learns the optimal policy by averaging the sample returns of a policy <span class="math notranslate nohighlight">\(\pi\)</span> <span id="id41">[<a class="reference internal" href="Appendix.html#id71">ADBB17</a>]</span>. They can therefore not learn on an online basis as after each episode they need to average their returns. Another difference between the two methods is that the MC method does not bootstrap like DP <span id="id42">[<a class="reference internal" href="Appendix.html#id60">MJ20</a>]</span>. Meaning, each state has an independent estimate. Note that Monte Carlo methods create a non-stationary problem as each action taken at a state depends on the previous states. MC methods can either estimate <span class="math notranslate nohighlight">\(a\)</span> state value <a class="reference internal" href="#equation-value">(3)</a> or estimate the value of a state-action pairs <a class="reference internal" href="#equation-state-action">(5)</a> (recall that the state-action values are the value of an action given a state). If state values are estimated, a model is required as it needs to be able to look one step ahead and choose the action which leads to the best reward and next state. With action value estimation you already estimated the value of the action and no model needs to be taken into account.  Monte Carlo methods also use a term called visits. A visit is when a state or state-action pair is in the sample path. Multiple visits to a state are possible in an episode. Two general Monte Carlo Methods can be deducted from visits. The every-visit MC methods and the first-visit MC methods. The every-visit MC methods estimate the value of a state as the average of the returns that have followed all visits to it. The first visit method only looks at the first visit of that state to estimate the average returns <span id="id43">[<a class="reference internal" href="Appendix.html#id50">VOW12</a>]</span>. The biggest hurdle in MC methods is that most state-action pairs might never be visited in the sample.</p>
<p>To overcome this problem multiple solutions were explored. The naïve solution to this problem is called the exploring starts. Here, the idea is to allocate to each action in each state a nonzero probability at the start of the process. Although this is not possible in a practical setting where we truly want to interact with an environment, it enables us to improve the policy by making it greedy with respect to the current value function. As each state has a certain probability to explore, it will eventually explore the complete state space. If then an infinite number of episodes are taken, the policy improvement theory states that the policy <span class="math notranslate nohighlight">\(\pi\)</span> will convergence to the optimal policy <span class="math notranslate nohighlight">\(\pi_*\)</span> given the exploring starts <span id="id44">[<a class="reference internal" href="Appendix.html#id69">Dol10</a>]</span>. Two other possibilities are applied in the field to solve this problem: on-policy methods and off-policy methods <span id="id45">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>. On-policy methods attempt to improve on the current policy. This is also called a soft policy as <span class="math notranslate nohighlight">\(\pi(a|s) &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span> and all <span class="math notranslate nohighlight">\( a \in A(s)\)</span>, but shifts eventually to the deterministic optimal policy. One of these on-policy methods is called an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-greedy policy. The <span class="math notranslate nohighlight">\(\varepsilon\)</span>-greedy policy uses with probability <span class="math notranslate nohighlight">\(\varepsilon\)</span> a random action instead of the greedy action. <span class="math notranslate nohighlight">\(\varepsilon\)</span> is a fine-tuning parameter as it sets the balance between exploration and exploitation. The <span class="math notranslate nohighlight">\(\varepsilon\)</span>-soft policy is thus also a compromised solution as one cannot exploit and explore at the same rate. This is re-elected by the fact that the <span class="math notranslate nohighlight">\(\varepsilon\)</span>-greedy policy is the best policy only among the <span class="math notranslate nohighlight">\(\varepsilon\)</span>-soft policies. The pseudocode of on-policy first visit MC for <span class="math notranslate nohighlight">\(\varepsilon\)</span>-soft policies algorithm can be found in the appendix. Lastly, the off-policy methods can be applied to overcome both the unrealistic exploring starts and the compromise needed in the on-policy methods. Off policy methods solve the exploration versus exploitation dilemma by considering two separate policies <span id="id46">[<a class="reference internal" href="Appendix.html#id50">VOW12</a>]</span>. one policy, called the target policy <span class="math notranslate nohighlight">\(\pi\)</span>, is being learned to become the optimal policy ,and another policy, called the behavior policy <span class="math notranslate nohighlight">\(b\)</span>, generates the behavior to explore the state space. In an off-policy method there needs to be coveraged between the behavior policy and the target policy to transfer the exploration done by behavior policy <span class="math notranslate nohighlight">\(b\)</span> to the target policy <span class="math notranslate nohighlight">\(\pi\)</span>. Meaning, every action taken under <span class="math notranslate nohighlight">\(\pi\)</span> also needs to be taken occasionally under <span class="math notranslate nohighlight">\(b\)</span>. Consequently, the behavior policy needs to be stochastic in states where it deviates from the target policy. Complete coverage would imply that the behavior policy and the target policy are the same. The off-policy method would then become an on-policy method. The on-policy method can thus be viewed as a special case of off-policy in which the two policies are the same. Most off-policy methods use importance sampling to estimate expected values under one distribution given samples from another. Importance sampling uses the ratio of returns according to the relative probability of the trajectories of the target and behavior policies to learn the optimal policy <span id="id47">[<a class="reference internal" href="Appendix.html#id70">MVHS14</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[ p_{t:T-1} = \frac{\prod^{T-1}_{k=t} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}\]</div>
<p>The formula is called the importance-sampling ratio. Note that the ratio only depends on the two policies and the sequence, not on the MDP. The importance-sampling ratio effectively transforms the expectations of <span class="math notranslate nohighlight">\(v_b(s)\)</span> to have the right expectation.  Now, we can effectively estimate <span class="math notranslate nohighlight">\(v_{\pi}(s)\)</span>:</p>
<div class="math notranslate nohighlight">
\[V_{\pi}(s) = \frac{\sum_{t\in J(s)} p_{t:T-1}G_t}{|J(s)|}\]</div>
<p>Where <span class="math notranslate nohighlight">\(J(s)\)</span> are all time steps in which state <span class="math notranslate nohighlight">\(s\)</span> is visited for an every-visit MC method and for a first-visit MC method <span class="math notranslate nohighlight">\(J(s)\)</span> are all time steps that were first visits to state <span class="math notranslate nohighlight">\(s\)</span>. An alternative to importance sampling is weighted importance sampling in which a weighted average is used:</p>
<div class="math notranslate nohighlight">
\[ V(s) = \frac{\sum_{t \in J(s)}p_{t:T-1}G_t}{\sum_{t \in J(s)}p_{t:T-1}} \]</div>
<p>The advantage of using a weighted importance sampling is a reduced variance as the variance is bounded when a weighting scheme is applied. The downside of this technique is that it increases the bias as the expectation deviates from the expectation of the target policy <span id="id48">[<a class="reference internal" href="Appendix.html#id70">MVHS14</a>]</span>.</p>
<p>The last general method to talk about is Temporal-difference Learning (TD). Temporal-difference Learning is a hybrid between Monte Carlo methods and Dynamic Programming. As DP, it updates estimates based on other learned estimates, not waiting on the final outcome, but it can learn directly from experience without a model of the environment like MC methods <span id="id49">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>. The simplest TD method is the one-step TD. It updates the prediction of <span class="math notranslate nohighlight">\(v_{\pi}\)</span> at each time step:</p>
<div class="math notranslate nohighlight">
\[V(S_t) \leftarrow V(S_t) + \alpha[R_{T+1} + \gamma V(S_{t+1}) - V(S_t)]\]</div>
<p>While MC method would update after each episode:</p>
<div class="math notranslate nohighlight">
\[ V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)] \]</div>
<p>One-step TD effectively bootstraps the update like DP, but it uses a sampling estimate like the MC method to estimate V <span id="id50">[<a class="reference internal" href="Appendix.html#id59">RMM18</a>]</span>. The sampling estimate differs from the expected estimate on the fact that they are based on a single sample successor rather than on the complete distribution of all possible successors <span id="id51">[<a class="reference internal" href="Appendix.html#id41">KC21</a>]</span>. In the updating rule of TD methods there is the TD error (see quantity in brackets) which is the difference between the previous estimate of <span class="math notranslate nohighlight">\(S_t\)</span> and the updated estimate  <span class="math notranslate nohighlight">\(R_{t+1} + \gamma V(S_{t+1})\)</span>. The TD error is the error in the estimate made at that time. The pseudocode of the one-step TD method can be found in the appendix. TD methods lend themselves quite easily to different methods in MC. For example, the Sarsa control algorithm is an on-policy TD in which the action values are updates using state-action pairs <span id="id52">[<a class="reference internal" href="Appendix.html#id41">KC21</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[ q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma q(s_{t+1}, a_{t+1}) - q(s_t, a_t)] \]</div>
<p>The same methodology is used here. <span class="math notranslate nohighlight">\(q_\pi\)</span> is continuously estimated for policy <span class="math notranslate nohighlight">\(\pi\)</span> while policy <span class="math notranslate nohighlight">\(\pi\)</span> changes toward the optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span> by a greedy approach. TD methods can also be applied to off-policy fashion. They are then called Q-learning which is widely applied in the literature. Q-learning is an off-policy method because they learn the action-value function <span class="math notranslate nohighlight">\(q\)</span> independent of the policy being followed. They select the maximal or minimal action-value pair in the current state <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-q-learning">
<span class="eqno">(7)<a class="headerlink" href="#equation-q-learning" title="Permalink to this equation">¶</a></span>\[q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma min_a q(s_{t+1}, a_{t+1}) - q(s_t, a_t)] \]</div>
<p>The policy still has an effect in that it determines which states-action pairs are being visited, but the learned action-value function <span class="math notranslate nohighlight">\(q\)</span> directly approximates <span class="math notranslate nohighlight">\(q_*\)</span>. This simplifies the analysis and enables early convergence. The last TD method is called the expected Sarsa and it uses the expected value instead of the minimum over the next state-action pairs to update the value function:</p>
<div class="math notranslate nohighlight">
\[  q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma \mathbb{E}_{\pi}[q(s_{t+1}, a_{t+1})|S_{t+1}] - q(s_t, a_t)] \]</div>
<div class="math notranslate nohighlight">
\[ q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma \sum_a \pi(a|s_{t+1}) q(s_{t+1}, a) - q(s_t, a_t)] \]</div>
<p>The main benefit of Expected Sarsa over Sarsa is that it eliminates the variance caused by the random selection of <span class="math notranslate nohighlight">\(a_{t+1}\)</span>. Another benefit of Expected Sarsa is that it can be used as an off-policy method when the target policy <span class="math notranslate nohighlight">\(\pi\)</span> is replaced with another policy <span id="id53">[<a class="reference internal" href="Appendix.html#id58">San21</a>]</span>.</p>
<p>These three methods lay at the foundation of RL and numerous adaptations have been made to fit the problem at hand. For example, Adaptive Dynamic Programming is implemented to approximate the original dynamic programming equations <span id="id54">[<a class="reference internal" href="Appendix.html#id3">WZL09</a>]</span>. In <a class="reference internal" href="#diff-meth-fig"><span class="std std-numref">Figure 6</span></a> the backup diagrams of the three different methods are represented. From the backup diagrams, it becomes clear that each algorithm approaches GPI from a different perspective. In the following subsection the different elements in a RL algorithm are explored.</p>
<div class="figure align-default" id="diff-meth-fig">
<a class="reference internal image-reference" href="_images/different_methods.png"><img alt="_images/different_methods.png" src="_images/different_methods.png" style="width: 500px; height: 350px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">The backup diagrams of Monte Carlo, Temporal-difference Learning and Dynammic Programming</span><a class="headerlink" href="#diff-meth-fig" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="dimensions-of-a-reinforcement-learning-algorithm">
<h2>Dimensions of a Reinforcement Learning Algorithm<a class="headerlink" href="#dimensions-of-a-reinforcement-learning-algorithm" title="Permalink to this headline">¶</a></h2>
<p>Moerland et al. <span id="id55">[<a class="reference internal" href="Appendix.html#id8">MBJ20a</a>]</span> addresses the six most critical dimensions of an RL algorithm: computational effort, action value selection, cumulative return estimation, policy evaluation, function representation ,and update method. These dimensions give a clear overview of the ability of RL algorithms to adapt to different situations. The first dimension has to do with the computational effort that is required to run the algorithm. The computational effort is primarily determined by the state set that is chosen (see <a class="reference internal" href="#state-space-fig"><span class="std std-numref">Figure 7</span></a>). The first option is to consider all states <span class="math notranslate nohighlight">\(S\)</span> of the dynamic environment. In practice, this often becomes impractical to consider due to the curse of dimensionality. The second and third possibilities are all reachable states and all relevant states. All reachable states are the states which are reachable from any start under any policy, while for the relevant states only those states under the optimal policy are considered. The last option is to use start states. These are all the states with a non-zero probability under <span class="math notranslate nohighlight">\(p(s_0)\)</span>. The curse of dimensionality is further discussed in the next subsection as it is one of the primary challenges in RL.</p>
<div class="figure align-default" id="state-space-fig">
<a class="reference internal image-reference" href="_images/state_space.png"><img alt="_images/state_space.png" src="_images/state_space.png" style="width: 500px; height: 350px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">state_space dimensions</span><a class="headerlink" href="#state-space-fig" title="Permalink to this image">¶</a></p>
</div>
<p>The second dimension is the action selection and has primarily to do with exploration process of the algorithm. The first consideration in action selection is the candidate set that is considered for the next action. Then the optimal action needs to be considered while still keeping exploration in mind. For selecting the candidate set two main approaches are considered: step-wise and frontier. Frontier methods only start exploration once they are on the frontier of the iteration, while step-wise methods have a new candidate set at each step of the trajectory. the MC method, DP ,and TD learning described above use step-wise exploration, while frontier methods are primarily used in robotics <span id="id56">[<a class="reference internal" href="Appendix.html#id56">NZKN19</a>]</span>. For the second consideration, selecting the action value,  different methods have been adopted. The first one is random explorations like <span class="math notranslate nohighlight">\(\varepsilon\)</span>-greedy exploration as explained in the section of Monte Carlo methods. These explorations techniques enable us to escape from a local minimum but can cause a jittering effect in which we undo an exploration step at random. The second approach is a value-based exploration that uses the value-based information to better direct the perturbation <span id="id57">[<a class="reference internal" href="Appendix.html#id39">YLL+19</a>]</span>. A good example of this are mean action values. They improve the random exploration by incorporating the mean estimates of all the available actions. Meaning, they explore actions with higher values more frequently than actions with lower values. The last option is state-based exploration. State-based exploration uses state-dependent properties to inject noise. Dynamic programming is a good example of this approach. DP is an ordered state-based exploration. Ordered state-based exploration sweeps through the state space in an orded like tree structure. Other state-based explorations are possible like novelty and priors.</p>
<p>The dimensions of the calculation of the cumulative return estimation (see <a class="reference internal" href="#equation-return">(1)</a>) can be expressed in an other formula to address the practical issues and limitations in RL:</p>
<div class="amsmath math notranslate nohighlight" id="equation-76c384be-1642-4b2b-a326-83e49e3e283c">
<span class="eqno">(8)<a class="headerlink" href="#equation-76c384be-1642-4b2b-a326-83e49e3e283c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \begin{split}
        &amp; G_t = \sum_{k=0}^T\gamma^kR_{t+k+1}
\\
        &amp; q(s,a) = E[G_t| S_t = d, A_t = a]
\\
        &amp;\hat{q}(s,a) = \sum_{k=0}^T \gamma^kR_{t+k+1} + \gamma^KB(s_{t+T}) 
    \end{split}
 \end{equation}\]</div>
<p>Where <span class="math notranslate nohighlight">\(T \in {1,2,3, ..., \infty}\)</span> denotes the sample depth and <span class="math notranslate nohighlight">\(B(.)\)</span> is a bootstrap function. For the sample depth three possible option are possible: <span class="math notranslate nohighlight">\(K = \infty\)</span>, <span class="math notranslate nohighlight">\(K = 1\)</span>,  <span class="math notranslate nohighlight">\(K = n\)</span> or reweighted. Monte Carlo methods for example use a sample depth to infinity as they do not bootstrap at all. Instead, DP uses bootstrapping at each iteration, so <span class="math notranslate nohighlight">\(K = 1\)</span>. An intermediate method between DP and Monte Carlo methods can also be devised in which <span class="math notranslate nohighlight">\( K = n\)</span>. The reweighted option is a special case of <span class="math notranslate nohighlight">\( K = n\)</span> in which targets of different depths are combined with a weighting scheme. The bootstrap function can be devised using a learned value function like the state value function or the state-action value function or following a heuristic approach. A good heuristic can be obtained by first solving a simplified version of the problem. An example of this is first solving the deterministic problem and then using the solution as a heuristic on its stochastic counterpart <span id="id58">[<a class="reference internal" href="Appendix.html#id8">MBJ20a</a>]</span>. The second dimension in cumulative return estimation is whether full knowledge of the dynamic system is in place (full backups) or a sample is taken from the environment (sample backups) <span id="id59">[<a class="reference internal" href="Appendix.html#id71">ADBB17</a>]</span>. In <a class="reference internal" href="#backups-bootstrap-fig"><span class="std std-numref">Figure 8</span></a> these two dimensions are represented.</p>
<div class="figure align-default" id="backups-bootstrap-fig">
<a class="reference internal image-reference" href="_images/backups.png"><img alt="_images/backups.png" src="_images/backups.png" style="width: 300px; height: 290px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">consideration in calculating the cumulative return estimation</span><a class="headerlink" href="#backups-bootstrap-fig" title="Permalink to this image">¶</a></p>
</div>
<p>The fourth dimension to consider is policy evaluation. Policy evaluation has to do with which policy to use: on-policy or off-policy method. We have already seen this dimension in the section of MC methods and it will not be further discussed. Another dimension is function representation. The first choice that needs to be made here is which function to represent. In theory, we have two essential functions: the value function and the policy function. The value function can be the state-action value function or just the state value function, but primarily represents the value of the current or optimal policy at all considered state-action pairs. The policy function on the other hand maps every state to a probability distribution over actions and is best used in continuous action spaces as we can directly act in the environment by sampling from the policy distribution <span id="id60">[<a class="reference internal" href="Appendix.html#id8">MBJ20a</a>]</span>. The second choice is how to represent this function. There are two possibilities here. The first option is using a tabular approach in which each state is a unique element for which we store an individual estimate.  This can be done on a global level or local level. At the global level, the entire state space is encapsulated by the table. Unfortunately, this method does not scale well and is only relevant in small exploratory problems. On the contrary, a local table does scale well as it is built temporarily until the next real step. The other method for function representation is function approximation. Function approximation builds on the concept of generalization. Generalization assumes that similar states to function will in general also have approximately similar output predictions (Generalization is further discussed in the next section).  Function approximation uses this to share information between near similar states and therefore store a global solution for a larger state space <span id="id61">[<a class="reference internal" href="Appendix.html#id50">VOW12</a>]</span>. There are two kinds of function approximations: parametric and non-parametric. A good example of a parametric function approximation is a neural network and for non-parametric a k-nearest neighbors can be thought of. The big challenge in function approximation is finding the balance between overfitting and underfitting the actual data.</p>
<p>The last dimension is the updating method. The updating method used should be in line with the function representation and the policy evaluation method as certain updating rules only work on a set of function representation and policy evaluation methods <span id="id62">[<a class="reference internal" href="Appendix.html#id8">MBJ20a</a>]</span>. For the updating method, there are quite a few choices to make. The first choice is choosing between gradient-based updates and gradient-free updates. In gradient-based updates we repeatedly update our parameters in the direction of the negative gradient loss with respect to the parameters:</p>
<div class="math notranslate nohighlight">
\[ \theta \leftarrow \theta - \alpha \cdot \frac{\partial L(\theta)}{\partial \theta} \]</div>
<p>Where <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}^+\)</span> is a learning rate. Before the updating rule can be applied a loss function <span class="math notranslate nohighlight">\(L(\theta)\)</span> should first be chosen. The loss function is usually a function of both the function representation and the policy evaluation method. As there are two kinds of function to represent in function representation, there are also two kinds of losses: value loss and policy loss. The most general value loss is the mean squared error loss. In policy loss, there are various methods to estimate the loss. For example the policy gradient specifies a relation between the value estimates <span class="math notranslate nohighlight">\(\hat{q}(s_t,a_t)\)</span> and the policy <span class="math notranslate nohighlight">\(\pi_{\theta}(a_t|s_t)\)</span> by ensuring that actions with high values also get high policy probabilities assigned:</p>
<div class="math notranslate nohighlight">
\[L(\theta|s_t, a_t) = -\hat{q}(s_t,a_t) \cdot ln (\pi_{\theta}(a_t|s_t))\]</div>
<p>Once the loss function is defined, the gradient-based updating rule can be applied. The updating again depends on the function representation for example the value update on a table for the mean squared loss function becomes:</p>
<div class="math notranslate nohighlight">
\[ q(s,a) \leftarrow q(s,a) - \alpha \cdot \frac{\partial L(q(s,a))}{\partial q(s,a)} \]</div>
<div class="math notranslate nohighlight">
\[ \frac{\partial L(q(s,a))}{\partial q(s,a)} = 2 \cdot \frac{1}{2}(q(s,a) - \hat{q}(s,a)) \]</div>
<div class="math notranslate nohighlight">
\[ q(s,a) \leftarrow q(s,a) - \alpha(q(s,a) - \hat{q}(s,a)) \]</div>
<div class="math notranslate nohighlight">
\[ q(s,a) \leftarrow (1- \alpha) \cdot q(s,a) + \alpha \cdot \hat{q}(s,a)  \]</div>
<p>Where <span class="math notranslate nohighlight">\(q(s,a)\)</span> is a table entry. The same can be done for function approximation where the derivative of the loss function then becomes:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial L(\theta)}{\partial \theta} = (q(s,a) - \hat{q}(s,a)) \cdot \frac{\partial q(s,a)}{\partial \theta} \]</div>
<p>Where <span class="math notranslate nohighlight">\(\frac{\partial q(s,a)}{\partial \theta}\)</span> can be for example the derivatives in a neural network.</p>
<p>Gradient-free updating rules use a parametrized policy function and then repeatedly perturb the parameters in policy space, evaluate the new solution by sampling traces and decide whether the perturbed solution should be retained. they only require an evaluation function and treat the problem as a black-box optimization setting. Gradient-free updating methods are thus not fit for model-based RL. An overview of the different dimensions can be viewed in <a class="reference internal" href="#dim-fig"><span class="std std-numref">Figure 9</span></a>.</p>
<div class="figure align-default" id="dim-fig">
<a class="reference internal image-reference" href="_images/dimensions.png"><img alt="_images/dimensions.png" src="_images/dimensions.png" style="width: 600px; height: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">The dimensions of a reinforcment learning algorithm</span><a class="headerlink" href="#dim-fig" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="curse-of-dimensionality-and-generalization">
<h2>Curse of Dimensionality and generalization<a class="headerlink" href="#curse-of-dimensionality-and-generalization" title="Permalink to this headline">¶</a></h2>
<p>If the state space is small, a tabular approach can be used to represent each state or state-action pair in a separate cell. This is very tractable because the states can be stored in a table. Unfortunately, This method becomes infeasible once the state space increases in size as the number of states grows exponentially larger with the number of state variables. Also, when the action and state space are continuous, we have an infinite amount of possible state-action pairs to explore, making the tabular approach impossible <span id="id63">[<a class="reference internal" href="Appendix.html#id61">SBLL19</a>]</span>. This is called the curse of dimensionality. To tackle the issue, RL borrows a technique from supervised learning called generalization. In RL generalization refers either to</p>
<ul class="simple">
<li><p>the capacity to achieve good performance in an environment where limited data has been gathered</p></li>
<li><p>the capacity to obtain good performance in a related environment</p></li>
</ul>
<p>In the first case, the agent needs to behave optimally in a similar environment as the trained one. Here, the notion of sample efficiency is important. In the latter case, there are patterns between the trained and test environments and the agent needs to transfer its learning from one environment to the other. This is also called transfer learning and meta-learning <span id="id64">[<a class="reference internal" href="Appendix.html#id79">FranccoisLHI+18</a>]</span>.</p>
<p>To better grasp the concept of generalization, consider an i.i.d. sampled dataset <span class="math notranslate nohighlight">\(D\)</span> as a set of four-tuples <span class="math notranslate nohighlight">\(\{s,a,r,s'\} \in S \times A \times R \times S \)</span> available to the agent. Where <span class="math notranslate nohighlight">\(D_{\infty}\)</span> is the data set where the number of tuples tends to infinity. Then a learning algorithm is a mapping of a dataset <span class="math notranslate nohighlight">\(D\)</span> into a policy <span class="math notranslate nohighlight">\(\pi_D\)</span>, where the suboptimality of the policy’s expected return can be decomposed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7f0308c9-c439-4fe8-8f70-1622baf6e302">
<span class="eqno">(9)<a class="headerlink" href="#equation-7f0308c9-c439-4fe8-8f70-1622baf6e302" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \begin{split}
        \mathbb{E}[V^{\pi^*}(s) - V^{\pi_d}(s)] &amp; = \mathbb{E}[V^{\pi^*}(s) - V^{\pi_{D_{\infty}}}(s) + V^{\pi_{D_{\infty}}}(s) -V^{\pi_{D}}(s) ]
\\
        &amp; = \underbrace{(V^{\pi^*}(s) - V^{\pi_{D_{\infty}}}(s) )}_{\text{asymptotic  bias}} + \underbrace{\mathbb{E}[V^{\pi_{D_{\infty}}}(s) - V^{\pi_{D}}(s)]}_{\text{variance/overfitting term }}
    \end{split}
\end{equation}\]</div>
<p>The decomposition consists of the asymptotic bias and the overfitting term. The asymptotic bias is independent of the quantity of the data and is caused by how the <span class="math notranslate nohighlight">\(\pi_{D_{\infty}}\)</span> is computed. In contrast, the overfitting term is directly related to the size of the dataset and measures the error caused by a limited amount of data. The goal of the policy <span class="math notranslate nohighlight">\(\pi_D\)</span> is to minimize the suboptimality as efficiently as possible.  Improving the suboptimality is challenging due to the trade-off between the asymptotic bias and the overfitting term. For example, if there is a significantly large state space, we cannot visit every state-action pair and therefore have a limited amount of data. Meaning, the error in our estimate is primarily caused by the overfitted term. The overfitting term can be reduced by introducing asymptotic bias. This will in turn significantly reduce the error caused by the small dataset, but also introduce a biased estimator <span id="id65">[<a class="reference internal" href="Appendix.html#id79">FranccoisLHI+18</a>]</span>. The most used technique in the literature to introduce bias is function approximations. These techniques will be discussed in the next subsection together with the challenges of using function approximations.</p>
<div class="section" id="function-approximation">
<h3>Function Approximation<a class="headerlink" href="#function-approximation" title="Permalink to this headline">¶</a></h3>
<p>Function approximations attempt to generalize by taking examples from the desired function and then constructing an approximation of the function using those examples. So instead of using a table in which the states are stored, a parametrized functional form with weight vector <span class="math notranslate nohighlight">\(w \in R^d\)</span> is used to represent the function. So for example, the value and state-value function can be approximated by</p>
<div class="math notranslate nohighlight">
\[ \hat{v}(s;w) \approx v_{\pi}(s) \]</div>
<div class="math notranslate nohighlight">
\[ \hat{q}(s,a;w) \approx q_{\pi}(s,a) \]</div>
<p><span class="math notranslate nohighlight">\(v\)</span> can be approximated by <span class="math notranslate nohighlight">\(\hat{v}\)</span> using for instance a linear function in features of state <span class="math notranslate nohighlight">\(s\)</span> given a weight vector <span class="math notranslate nohighlight">\(w\)</span> of the features weight. Another possibility is using a decision tree, where <span class="math notranslate nohighlight">\(w\)</span> is all the numbers defining the split points and leaf values of the tree. The most notable function approximations are neural networks. Especially deep neural networks (DNN) can significantly reduce the time and effort required to approximate the value function <span id="id66">[<a class="reference internal" href="Appendix.html#id61">SBLL19</a>]</span>. The approximation functions used in supervised learning are not always suitable in an RL environment. The reason for this, is that supervised learning methods like decision trees and multivariate regression are used in a static environment where the model is only learning from a prespecified dataset. Contrarily to supervised learning, RL methods learn while interacting with an environment and thus need to learn efficiently from incremental data <span id="id67">[<a class="reference internal" href="Appendix.html#id58">San21</a>]</span>. In addition, RL requires function approximation methods to be able to handle non-stationary target functions, because we often seek to learn <span class="math notranslate nohighlight">\(q_{\pi}\)</span> while <span class="math notranslate nohighlight">\(\pi\)</span> is changing. Even if the policy remains the same, the target values of training examples are non-stationary if they are generated by bootstrap methods (DP, TD learning) <span id="id68">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>.</p>
<p>As discussed in the previous section, function representation such as function approximation and table approach have a large impact on the updating rule applied. If a look-up table is used, the update is trivial as the table entry for <span class="math notranslate nohighlight">\(s\)</span>’s estimated value has simply been shifted a fraction to the update target <span class="math notranslate nohighlight">\(u\)</span>. All the values of the other states are in this case left unchanged. However, with function approximation we generalize the states by representing many states with weights (see <a class="reference internal" href="#function-fig"><span class="std std-numref">Figure 10</span></a>). Meaning, an update at one state affects many others, and it is thus not possible to get the values of all states correct. The problem of having more states than weights is resolved by a state distribution <span class="math notranslate nohighlight">\(\mu (s) \geq 0, \sum_s \mu(s) = 1\)</span>. The state distribution specifies which states are important by applying a weight to the error of the state  <span id="id69">[<a class="reference internal" href="Appendix.html#id58">San21</a>]</span>. once a weight is giving to each state’s error, a mean squared value error is applied which tells us how good the function approximation is representing the value function:</p>
<div class="math notranslate nohighlight">
\[ \overline{VE}(w) = \sum_{s \in S}  \mu (s)[v_{\pi}(s) - \hat{v}(s,w)]^2 \]</div>
<div class="figure align-default" id="function-fig">
<a class="reference internal image-reference" href="_images/function_approximation.png"><img alt="_images/function_approximation.png" src="_images/function_approximation.png" style="width: 500px; height: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">general approach of function approximation methods</span><a class="headerlink" href="#function-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Now we have discussed the general working of function approximation, a closer look is given to linear gradient-descent methods and artificial neural networks (ANN). Linear gradient descent methods and ANN are two popular function approximations used in RL. Although there are many other techniques like Coarse Coding and memory-based function approximation, it is impossible to cover all function approximations. We will therefore only look at linear-gradient descent methods. One of the linear gradient descent methods is the stochastic-gradient descent (SGD). In SGD the weight vector is a column vector with fixed number of real-valued components <span class="math notranslate nohighlight">\(w = (w_1, w_2,..., w_d)^{\top}\)</span> and the approximate value function <span class="math notranslate nohighlight">\(\hat{v}(s,w)\)</span> is a differentiable function of w for all <span class="math notranslate nohighlight">\( s \in S\)</span>. The weight vector is updated at each time step <span class="math notranslate nohighlight">\(t=0,1,2,3,...\)</span>, giving us <span class="math notranslate nohighlight">\(w_t\)</span>. By interacting with the environment we get a new example <span class="math notranslate nohighlight">\(S_t \rightarrow v_{\pi}(S_t)\)</span> giving us a state <span class="math notranslate nohighlight">\(S_t\)</span> and the value under the policy. The goal of the function approximation is to approximate the value of the state with limited resources. Meaning from the example, we generalize to all other states while upholding an accurate representation of the seen state. SGD methods do exactly that they minimize the error observed on the examples by adjusting the weight vector after each example by a small amount in the direction that would reduce the error in that example the most:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3cce6d7e-3e19-4292-9032-3a39500ee4dc">
<span class="eqno">(10)<a class="headerlink" href="#equation-3cce6d7e-3e19-4292-9032-3a39500ee4dc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \begin{split}
        W_{t+1} &amp; = w_t - \frac{1}{2}\alpha \Delta [v_{\pi}(S_t) - \hat{v}(S_t,w_t)]^2 
\\
        &amp; = w_T + \alpha [v_{\pi}(S_t) - \hat{v}(S_t,w_t)] \Delta \hat{v}(S_t, w_t)
    \end{split}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a positive step-size parameter, and <span class="math notranslate nohighlight">\(\Delta \hat{v}(S_t, w_t)\)</span> is the column vector of partial derivatives of the expression with respect to the components of the vector. This can be written for any scale expression <span class="math notranslate nohighlight">\(f(w)\)</span> that is a function of a vector <span class="math notranslate nohighlight">\(w\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \Delta f(w) = (\frac{\partial f(w)}{ \partial w_1}, \frac{\partial f(w)}{ \partial w_2}, ..., \frac{\partial f(w)}{ \partial w_d})^{\top}  \]</div>
<p>The gradient of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(w\)</span> determines the optimal directions in which the error falls most rapidly and <span class="math notranslate nohighlight">\(\alpha\)</span> determines the step size in the direction of the gradient. The step size is an important parameter to set as it sets up the balance between accuracy on the example and the generalization property. More specifically, we do not want the error of the example to be zero as that would decrease the quality of the generalization to other states. Instead, the step size should incorporate the generalization by decreasing <span class="math notranslate nohighlight">\(\alpha\)</span> to zero over time. Then, by combining small updates over many examples, the SGD method creates the ability to minimize an average performance measure such as the <span class="math notranslate nohighlight">\(\overline{VE}\)</span>. following this property, the SGD method is guaranteed to converge to a local optimum. The SGD method is called stochastic because we implement the update on a single example <span id="id70">[<a class="reference internal" href="Appendix.html#id55">BI99</a>]</span>.</p>
<p>In most cases, the true value, <span class="math notranslate nohighlight">\(v_{\pi}(S_t)\)</span>, is not the target output. Rather, some approximation to it like a noise corrupted version of <span class="math notranslate nohighlight">\(v_{\pi}(S_t)\)</span> or a bootstrapping target using <span class="math notranslate nohighlight">\(\hat{v}\)</span>. If the target value is not the exact update because <span class="math notranslate nohighlight">\(v_{\pi}(S_t)\)</span> is unknown, we can approximate it by substituting <span class="math notranslate nohighlight">\(U_t\)</span> in place of <span class="math notranslate nohighlight">\(v_{\pi}S_t\)</span>. <span class="math notranslate nohighlight">\(U_t\)</span> needs to be an unbiased estimate , <span class="math notranslate nohighlight">\(\mathbb{E}[U_t|S_t=s] = v_{\pi}(s)\)</span>. Unfortunately, the bootstrapping method of estimating <span class="math notranslate nohighlight">\(v_{\pi}(S_t)\)</span> is biased because it is estimated on the current value of the weight vector <span class="math notranslate nohighlight">\(w_t\)</span>. as a result, they only include a part of the gradient. Namely, they take into account the effect of changing the weight vector <span class="math notranslate nohighlight">\(w_t\)</span> on the estimate but ignore its effect on the target. Therefore, they are called semi-gradient methods. In the next paragraph, the SGD update is combined with a linear approximate function <span id="id71">[<a class="reference internal" href="Appendix.html#id57">GP13</a>]</span>.</p>
<p>The linear method approximate the state-value function by the inner product between <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(x(s)\)</span></p>
<div class="math notranslate nohighlight">
\[ \hat{v}(s, w) = w^{\top}x(s) = \sum_{i=1}^d w_ix_i\]</div>
<p>Where the vector <span class="math notranslate nohighlight">\(x(s)\)</span> is called a feature vector representing state <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(w\)</span> is the weight vector. For linear function approximations, it is natural to use the SGD updates. The gradient then becomes</p>
<div class="math notranslate nohighlight">
\[ \Delta \hat{v}(s,w) = x(s) \]</div>
<p>and the SGD update reduces to</p>
<div class="math notranslate nohighlight">
\[ w_{t+1} = w_t +\alpha[U_t - \hat{v}(S_t, w_t)x(S_t)] \]</div>
<p>By specifying the feature vector, it is possible to add domain knowledge to a reinforcement learning system. The features should therefore correspond to the aspects of the state space.</p>
<p>A limitation of the linear function approximation is that it cannot incorperate interaction between features. A possible solution to the problem are polynomial features as higher-order polynomial bases allow for more accurate approximations of more complicated functions.  Also, nonlinear function approximations like an artificial neural network can be used.</p>
<p>Most of the function approximations that are discussed above relate to on-policy RL. For off-policy RL function approximation methods are more challenging as the distribution of the update becomes a difficult concept in which two policies need to factor in the generalization aspect. The main intuition behind the problem is that the behavior policy might select actions which the target policy would never select. This can cause divergence because no update is made on these actions (the importance sampling ratio is zero). consequently, the off-policy method expects higher rewards but does not actually know whether there are higher rewards after the action as there was no update to check this creating divergence. A lot of research is still needed in this area to solve the divergence problems of off-policy methods. For instance, the Baird’s counterexample is one of the big hurdles of finding a general concept of convergence for off-policy learning methods. Also, the combination of function approximation, bootstrapping ,and off-policy training is called the deadly triad as combining these three elements can give rise to the danger of instability and divergence. Unfortunately, all these three elements give rise to significant advantages ,and giving up on one of these elements is sometimes not preferred <span id="id72">[<a class="reference internal" href="Appendix.html#id77">SB18</a>]</span>.</p>
<p>Function approximation is a subfield in RL which is one of the main drivers behind the recent advances in the field. With state-of-the-art function approximation techniques, more complex environments can be tackled. This can give rise to more advanced applications of RL in domains like optimal control, robotics ,and financial planning.  In the next subsection, an adaptation of Q-learning is discussed, called G-learning.</p>
</div>
</div>
<div class="section" id="g-learning-a-stochastic-adaptation-on-q-learning">
<h2>G-learning, a stochastic adaptation on Q-learning<a class="headerlink" href="#g-learning-a-stochastic-adaptation-on-q-learning" title="Permalink to this headline">¶</a></h2>
<p>Q-learning learns extremely slow in noisy environments due to the minimization bias. In <a class="reference internal" href="#equation-q-learning">(7)</a> the minimum over the estimated values is used implicitly as an estimate of the minimum value, which can lead to a significant positive bias in noisy environments. Consider, for example, a single state <span class="math notranslate nohighlight">\(s\)</span> where there are many actions <span class="math notranslate nohighlight">\(a\)</span> whose true values are all zero but whose estimated values are uncertain and thus distributed some above and some below zero. The minimum of the true values is zero, but the minimum of the estimates is negative. Consequently, introducing minimization bias {cite}´sutton2018reinforcement´. This can also be illustrated by the Jensen’s inequality for the concave min operator. Assume that <span class="math notranslate nohighlight">\(Q(s,a)\)</span> is an unbiased but noisy estimate of the optimal <span class="math notranslate nohighlight">\(Q^*(s,a)\)</span>. Then it applies that</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}[min_aQ(s,a)] \leq min_aQ^*(s,a)\]</div>
<p>This creates an optimistic bias, causing the cost-to-go to appear lower than it is. The minimization bias has an impact on the learning rate of the Q-learning policy. The impact depends on the gap <span class="math notranslate nohighlight">\(Q^*(s,a') - V^*(s)\)</span>  between the value of a non-optimal action <span class="math notranslate nohighlight">\(a'\)</span> and that of the optimal action. If the gap is large, <span class="math notranslate nohighlight">\(a'\)</span> seems suboptimal as desired. If the gap is small, confusing <span class="math notranslate nohighlight">\(a'\)</span> for the optimal action does not affect the learning process. However, when the gap is in the order of the noise term, the minimization bias has a significant impact, because <span class="math notranslate nohighlight">\(a'\)</span> does not appear to be suboptimal and is thus still accepted as optimal. The optimistic bias is further enhanced by propagating the bias between states and can lead to regions of the state space that are highly biased creating large-gap suboptimal actions. Although this problem hampers the learning rate, Q-learning can still learn in a stochastic environment since the bias draws exploration towards the given state, leading to a decrease in variance, which in turn reduces the bias <span id="id73">[<a class="reference internal" href="Appendix.html#id51">FPT15</a>]</span>.</p>
<p>G-learning is an adaptation of Q-learning constructed by Fox et all <span id="id74">[<a class="reference internal" href="Appendix.html#id51">FPT15</a>]</span>. It is designed to handle noisy environments. It is also an off-policy approach in a model-free setting, but it regularizes the state-action value function learned by an agent. It regularizes the state-action value function by penalizing deterministic policies early in the optimization process. Penalization is done early in the process because there is still a small sample size and therefore a more randomized policy is preferred. When the sample size grows, one should expect to shifts to a more deterministic and exploiting policy. This is what G-learning effectively does. It adds a cost-to-go term to the value function that penalizes the early deterministic policies which diverge from a simple stochastic prior policy <span class="math notranslate nohighlight">\(\pi_0(a|s)\)</span>. The prior stochastic policy sets up an information cost of a learned policy <span class="math notranslate nohighlight">\(\pi(a|s)\)</span>, effectively penalizing deviations from the prior policy. G-learning is therefore called an entropy regularization of Q-learning.</p>
<div class="math notranslate nohighlight">
\[ g^{\pi}(s,a) = log(\frac{\pi(a,s)}{\pi_0(a,s)}) \]</div>
<p>taken the expectation of the policy <span class="math notranslate nohighlight">\(\pi\)</span> gives us the Kullback-Leibler divergence of the two policies.</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}_{\pi}[g^{\pi}(s,a)|s] = D_{KL}[\pi_s || \pi_{0_s}] \]</div>
<p>Now consider the total discounted expected information cost</p>
<div class="math notranslate nohighlight" id="equation-information-cost">
<span class="eqno">(11)<a class="headerlink" href="#equation-information-cost" title="Permalink to this equation">¶</a></span>\[I^{\pi}(s) = \sum_{t\geq0} \gamma^t \mathbb{E}[g^{\pi}(s_t,a_t)|s_0=s]\]</div>
<p>Adding <a class="reference internal" href="#equation-information-cost">(11)</a> to the value function <a class="reference internal" href="#equation-value">(3)</a> gives the free-energey function</p>
<div class="math notranslate nohighlight" id="equation-free-energy">
<span class="eqno">(12)<a class="headerlink" href="#equation-free-energy" title="Permalink to this equation">¶</a></span>\[\begin{split}    F^{\pi}(s) &amp; = V^{\pi}(s) + \frac{1}{\beta} I^{\pi}(s)
\\
    &amp; = \sum_{t\geq 0 } \gamma^t \mathbb{E}[\frac{1}{\beta} g^{\pi}(s_t,a_t) + R_t|s_0=s] \end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\beta\)</span> is the parameter that sets the weight of the information cost in the value function. If <span class="math notranslate nohighlight">\(\beta\)</span> is small, <span class="math notranslate nohighlight">\(\pi\)</span> will act similar to <span class="math notranslate nohighlight">\(\pi_0\)</span>. When <span class="math notranslate nohighlight">\(\beta\)</span> is large, <span class="math notranslate nohighlight">\(\pi\)</span> will diverge from the prior and will therefore approach the greedy policy of Q-learning. A smooth transition between small and large values for \beta will allow the algorithm to avoid early deterministic policies and still be able to exploit the optimal values. The same approach can be done for the state-action value function <span class="math notranslate nohighlight">\(q(s,a)\)</span></p>
<div class="math notranslate nohighlight" id="equation-free-q">
<span class="eqno">(13)<a class="headerlink" href="#equation-free-q" title="Permalink to this equation">¶</a></span>\[\begin{split}    H^{\pi}(s,a) &amp; = \mathbb{E}[R|s,a] + \gamma \mathbb{E}[F^{\pi}(s')|s,a] 
\\ 
   &amp; = \sum_{t\geq 0 } \gamma^t \mathbb{E}[R_t + \frac{\gamma}{\beta} g^{\pi}(s_{t+1}, a_{t+1})|s_0=s, a_0=a] \end{split}\]</div>
<p>Notice that the information term at time <span class="math notranslate nohighlight">\(t=0\)</span> is not needed as the action <span class="math notranslate nohighlight">\(a_0 =a\)</span> is already known. Given <a class="reference internal" href="#equation-free-energy">(12)</a> and <a class="reference internal" href="#equation-free-q">(13)</a> it follows that</p>
<div class="math notranslate nohighlight" id="equation-free-energy2">
<span class="eqno">(14)<a class="headerlink" href="#equation-free-energy2" title="Permalink to this equation">¶</a></span>\[F^{\pi}(s) = \sum_a \pi(a|s)[\frac{1}{\beta}log\frac{\pi(a|s)}{\pi_0(a|s)} + H^{\pi}(s,a)]\]</div>
<p>The gradient of <span class="math notranslate nohighlight">\(F^{\pi}\)</span> at zero is</p>
<div class="math notranslate nohighlight" id="equation-soft-min">
<span class="eqno">(15)<a class="headerlink" href="#equation-soft-min" title="Permalink to this equation">¶</a></span>\[\pi(a|s) = \frac{\pi_0(a|s)e^{-\beta H(s,a)}}{\sum_{a'} \pi_0(a'|s)e^{-\beta H(s,a')}}\]</div>
<p>Equation <a class="reference internal" href="#equation-soft-min">(15)</a> is the soft-min operator applied to H. Now evaluate  <a class="reference internal" href="#equation-free-energy2">(14)</a> at <a class="reference internal" href="#equation-soft-min">(15)</a></p>
<div class="math notranslate nohighlight" id="equation-soft-minf">
<span class="eqno">(16)<a class="headerlink" href="#equation-soft-minf" title="Permalink to this equation">¶</a></span>\[ F^{\pi}(s) = \frac{-1}{\beta} log\sum_a \pi_0(a,s) e^{-\beta H^{\pi}(s,a)} \]</div>
<p>This expression can get pluged in <a class="reference internal" href="#equation-free-q">(13)</a> and as a result the optimal <span class="math notranslate nohighlight">\(H^*\)</span> is achieved.</p>
<div class="amsmath math notranslate nohighlight" id="equation-3333b02b-6853-4e93-b586-83dcfd87ff83">
<span class="eqno">(17)<a class="headerlink" href="#equation-3333b02b-6853-4e93-b586-83dcfd87ff83" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \begin{split}
        H^*(s,a) &amp; = \mathbb{E}[R|s,a] + \gamma\mathbb{E}[F^{\pi}(s)] 
\\
        &amp;  = \mathbb{E}[R|s,a] -\frac{\gamma}{\beta} \mathbb{E}[log \sum_{a'} \pi_0(a'|s')e^{-\beta H^*(s',a')}]
    \end{split}
\end{equation}\]</div>
<p>The G-learner is used in the first application for a retirement plan optimization problem. In the next subsection, we will describe the general approach of the Deep BSDE method and link it to the RL theory. The Deep BSD method is applied to solve high-dimensional terminal Partial Differential Equation (PDE), which a lot of optimal control problems in finance struggle to solve.</p>
</div>
<div class="section" id="the-deep-backward-stochastic-differential-equation-method">
<h2>The Deep Backward Stochastic Differential Equation Method<a class="headerlink" href="#the-deep-backward-stochastic-differential-equation-method" title="Permalink to this headline">¶</a></h2>
<p>There are many financial control problems that need to solve a partial differential equation to obtain the optimal solution and some financial planning applications are no different. The Deep BSDE method is an RL inspired approach to solve terminal PDEs in higher dimensions. The general PDEs that the Deep BSDE method solves can be written as:</p>
<div class="math notranslate nohighlight" id="equation-gen-form">
<span class="eqno">(18)<a class="headerlink" href="#equation-gen-form" title="Permalink to this equation">¶</a></span>\[\frac{\partial u}{\partial t} + \frac{1}{2} Tr(\sigma \sigma^T (Hess_xu) + \Delta u(t,x)  \mu(t,x) + f(t,x,u, \sigma^T(\Delta_x u)) = 0 \]</div>
<p>with some terminal condition <span class="math notranslate nohighlight">\(u(T,x) = g(x)\)</span>.</p>
<p>With <span class="math notranslate nohighlight">\(u(T,x) = L(x)\)</span>. The key idea is to reformulate the PDE as an appropriate stochastic problem <span id="id75">[<a class="reference internal" href="Appendix.html#id19">HJ+20</a>]</span> and <span id="id76">[<a class="reference internal" href="Appendix.html#id35">WHJ17</a>]</span>. Here,  the probability space (<span class="math notranslate nohighlight">\(\Omega,\mathcal{F}, \mathbb{P}\)</span>) is adapted to the high dimensional problem. So <span class="math notranslate nohighlight">\(W: [0, T] \times \Omega \rightarrow \mathbb{R}^d\)</span> becomes a d-dimensional standard Brownian motion on (<span class="math notranslate nohighlight">\(\Omega,\mathcal{F}, \mathbb{P}\)</span>) and let <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> be the set of all <span class="math notranslate nohighlight">\(\mathbb{F}\)</span>-adapted <span class="math notranslate nohighlight">\(\mathcal{R^d}\)</span>-values stochastic processes with continuous sample paths. Let <span class="math notranslate nohighlight">\(\{X_T\}_{0 \leq t \leq T}\)</span> be a d-dimensional stochastic process which satisfies</p>
<div class="math notranslate nohighlight">
\[ X_t = \varepsilon + \int_0^t \mu(s,X_s)ds + \int_0^t \sigma(s,X_s)dW_s \]</div>
<p>Using Itô’s lemma, we obtain that</p>
<div class="math notranslate nohighlight">
\[ y(t, X_t) - u(0,X_0) = - \int_0^t f(s,X_s,u(s,X_s), [\sigma(s,X_s)]^T(\Delta_x u)(s,X_s)) ds + \int_0^t[\Delta u(s,X_s)]^T\sigma(s,X_s)dW_s\]</div>
<p>A forward-backward stochastic differential equation can be written as</p>
<div class="math notranslate nohighlight" id="equation-stoch-con">
<span class="eqno">(19)<a class="headerlink" href="#equation-stoch-con" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{cases} X_t = \varepsilon + \int_0^t \mu(s,X_s) ds + \int_0^t\sigma(s,X_S)dW_S \\ 
Y_t = g(X_T) + \int_t^T f(s, X_s, Y_s, Z_s)ds - \int_t^T(Z_s)^T dW_s
\end{cases} \end{split}\]</div>
<p>In the literature it was found that the solution of PDE and its spatial derivative are now the solution of the stochastic control problem <a class="reference internal" href="#equation-stoch-con">(19)</a> <span id="id77">[<a class="reference internal" href="Appendix.html#id19">HJ+20</a>]</span>. The relationship between the PDE <a class="reference internal" href="#equation-gen-form">(18)</a> and the BSDe <a class="reference internal" href="#equation-stoch-con">(19)</a> is based on the nonlinear Feynman-Kac formula <span id="id78">[<a class="reference internal" href="Appendix.html#id32">Blo18</a>]</span> and <span id="id79">[<a class="reference internal" href="Appendix.html#id53">GulerLP19</a>]</span>. Under suitable additional regularity assumption on the nonlinearity <span class="math notranslate nohighlight">\(f\)</span> in the sense that for all <span class="math notranslate nohighlight">\(t \in[0,T]\)</span> it holds <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>-a.s. that</p>
<div class="math notranslate nohighlight" id="equation-identity">
<span class="eqno">(20)<a class="headerlink" href="#equation-identity" title="Permalink to this equation">¶</a></span>\[Y_t = u(t, \epsilon + W_t) \in \mathbb{R}  \hspace{0.2cm}\text{and}\hspace{0.2cm} Z_t = (\Delta_x u) (t, \epsilon + W_t) \in \mathbb{R}^d\]</div>
<p>The first identity in <a class="reference internal" href="#equation-identity">(20)</a> is referred to as nonlinear Feynman-Kac formula <span id="id80">[<a class="reference internal" href="Appendix.html#id35">WHJ17</a>]</span>. The pair <span class="math notranslate nohighlight">\((Y_t, Z_t), t \in [0,T]\)</span> is a solution for the BSDE and with <a class="reference internal" href="#equation-identity">(20)</a> in mind, the PDE problem can be formulated as the following variational problem:</p>
<div class="math notranslate nohighlight">
\[ inf_{Y_0,\{Z_T\}_{0\leq t\leq T}} \mathbb{E}[|g(X_T) - Y_T|^2] \]</div>
<div class="math notranslate nohighlight">
\[ s.t. \hspace{0.2cm}X_T = \varepsilon + \int_0^t \mu(s,X_s)ds + \int_0^t\sum(s,X_s)dW_s \]</div>
<div class="math notranslate nohighlight">
\[  \hspace{1.2cm}Y_t = Y_0 - \int_0^th(s,X_s,Y_s,Z_s)ds + \int_0^t(Z_s)^TdW_s\]</div>
<p>The minimizer of this variational problem is the solution to the PDE <span id="id81">[<a class="reference internal" href="Appendix.html#id33">Rai18</a>]</span>. The main idea behind Deep BSDE method is to approximate the unknown function <span class="math notranslate nohighlight">\(X_0 \rightarrow u(, X_0)\)</span> and <span class="math notranslate nohighlight">\(X_t \rightarrow [\sigma(t,X_t)]^T((\Delta_x u)(t,X_t)\)</span> by two feedforward neural networks <span class="math notranslate nohighlight">\(\psi\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> <span id="id82">[<a class="reference internal" href="Appendix.html#id36">HJW18</a>]</span>. To achieve this we discretize time using Euler scheme on a grid <span class="math notranslate nohighlight">\( 0 = t_0&lt;t_1&lt;...&lt;T_N =T \)</span></p>
<div class="math notranslate nohighlight">
\[ inf_{\psi_0,\{\phi_n\}^{N-1}_{n=0}} \mathbb{E}[|g(X_T) - Y_T|^2] \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-bd6132f6-ce5f-4103-9224-e99228130362">
<span class="eqno">(21)<a class="headerlink" href="#equation-bd6132f6-ce5f-4103-9224-e99228130362" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\begin{split}
s.t. \hspace{0.2cm} &amp; X_0 = \varepsilon, \hspace{0.2cm} Y_0 = \psi_0(\varepsilon)
\\
&amp; X_{t_{n+1}} = X_{t_i} \mu(t_n,X_{t_n}) \Delta t + \sigma(t_n,X_{t_n}) \Delta W_n
\\
 &amp; Z_{t_n} = \psi_(X_{t_n})
\\ 
 &amp; Y_{t_{n+1}} = Y_{t_n} - f(t_n,X_{t_n},Y_{t_n},Z_{t_n})\Delta t  + (Z_{t_n})^T \Delta W_n
\end{split}
\end{equation}\]</div>
<p>At each time slide <span class="math notranslate nohighlight">\(t_n\)</span>, a subnetwork is associated. These subnetworks are then stacked together to form a deep composite neural network <span id="id83">[<a class="reference internal" href="Appendix.html#id34">HJW17</a>]</span>. The network takes the paths  <span class="math notranslate nohighlight">\(\{X_{t_n}\}_{0\leq n \leq N}\)</span> and <span class="math notranslate nohighlight">\(\{W_{t_n}\}_{0\leq n \leq N}\)</span> as the input data and gives as final output, denoted by <span class="math notranslate nohighlight">\(\hat{u}(\{ X_{t_n}\}_{0 \leq n \leq N}, \{W_{t_n}\}_{0 \leq n \leq N}\)</span>,  as an approximation to <span class="math notranslate nohighlight">\(u(t_N, X_{t_N})\)</span> (see <a class="reference internal" href="#bsdn-fig"><span class="std std-numref">Figure 11</span></a>) <span id="id84">[<a class="reference internal" href="Appendix.html#id36">HJW18</a>]</span>. Thereby it is only solved a time step <span class="math notranslate nohighlight">\(t=0\)</span>. The difference in the matching of a given terminal condition can be used to define the expected loss function <span id="id85">[<a class="reference internal" href="Appendix.html#id19">HJ+20</a>]</span> <span id="id86">[<a class="reference internal" href="Appendix.html#id34">HJW17</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[ m(\theta) = \mathbb{E}[|g(X_{t_N}) - \hat{u}(\{ X_{t_n}\}_{0 \leq n \leq N}, \{W_{t_n}\}_{0 \leq n \leq N} |^2]\]</div>
<div class="figure align-default" id="bsdn-fig">
<a class="reference internal image-reference" href="_images/BSDE_NN.png"><img alt="_images/BSDE_NN.png" src="_images/BSDE_NN.png" style="width: 700px; height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">neural network for Deep BSDE method</span><a class="headerlink" href="#bsdn-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Another way to look at it is that the stochastic control problem is a model-based reinforcement learning problem <span id="id87">[<a class="reference internal" href="Appendix.html#id36">HJW18</a>]</span>. In this setting the gradient <span class="math notranslate nohighlight">\(Z\)</span> is viewed as the policy we try to approximate using a feedforward neural network. The process <span class="math notranslate nohighlight">\(u(t, \varepsilon + W_t), t \in [0, T]\)</span>, corresponds to the value function associated with the stochastic control problem and can be approximately employed by the policy Z <span id="id88">[<a class="reference internal" href="Appendix.html#id35">WHJ17</a>]</span>. A benefit of using the Deep BSDE method is it does not require us to generate training data beforehand. The paths play the role of the data and they are generated on the spot <span id="id89">[<a class="reference internal" href="Appendix.html#id19">HJ+20</a>]</span>.</p>
<p>The Deep BSDE method solves the PDE for <span class="math notranslate nohighlight">\(Y_0= u(0, X_0) = u(0, \varepsilon)\)</span>. This means that in order to obtain an approximate of <span class="math notranslate nohighlight">\(Y_t = u(t,X_t)\)</span> at a later time <span class="math notranslate nohighlight">\(t&gt;0\)</span>, we will have to retain our algorithm. Although this method is originally designed to only solve the equation at time step <span class="math notranslate nohighlight">\(t=0\)</span>, future research might be able to solve the PDE at each time point <span class="math notranslate nohighlight">\(t\)</span>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="abstract.html" title="previous page">Abstract</a>
    <a class='right-next' id="next-link" href="Financial_application.html" title="next page">Financial Applications of Reinforcement Learning</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ignace Decocq<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>