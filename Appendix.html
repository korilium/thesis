
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Appendix &#8212; Future Financial Planning Tools for Consumers</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e2363ea40746bee74734a24ffefccd78.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="discussion" href="Discussion.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Future Financial Planning Tools for Consumers</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="abstract.html">
   Abstract
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Reinforcement_learning.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Financial_application.html">
   Financial Applications of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Discussion.html">
   discussion
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Appendix.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/korilium/thesis"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/korilium/thesis/issues/new?title=Issue%20on%20page%20%2FAppendix.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/korilium/thesis/edit/master/Appendix.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/korilium/thesis/master?urlpath=tree/Appendix.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pseudocode-algorithms">
   pseudocode algorithms
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h1>
<div class="section" id="pseudocode-algorithms">
<h2>pseudocode algorithms<a class="headerlink" href="#pseudocode-algorithms" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="policy-iteration-fig">
<a class="reference internal image-reference" href="_images/policy_iteration.png"><img alt="_images/policy_iteration.png" src="_images/policy_iteration.png" style="width: 400px; height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">algorithm for the policy iteration</span><a class="headerlink" href="#policy-iteration-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="value-iteration-fig">
<a class="reference internal image-reference" href="_images/value_iteration.png"><img alt="_images/value_iteration.png" src="_images/value_iteration.png" style="width: 400px; height: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">algorithm for the value iteration</span><a class="headerlink" href="#value-iteration-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="mc-iteration-fig">
<a class="reference internal image-reference" href="_images/MC_method_on-policy.png"><img alt="_images/MC_method_on-policy.png" src="_images/MC_method_on-policy.png" style="width: 400px; height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">algorithm for MC method on-policy</span><a class="headerlink" href="#mc-iteration-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="td-fig">
<a class="reference internal image-reference" href="_images/TD.png"><img alt="_images/TD.png" src="_images/TD.png" style="width: 500px; height: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">algorithm for TD(0)</span><a class="headerlink" href="#td-fig" title="Permalink to this image">¶</a></p>
</div>
<p id="id1"><dl class="citation">
<dt class="label" id="id71"><span class="brackets">ADBB17</span></dt>
<dd><p>Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief survey of deep reinforcement learning. <em>arXiv preprint arXiv:1708.05866</em>, 2017.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">AKAM+21</span></dt>
<dd><p>Ahmad Taher Azar, Anis Koubaa, Nada Ali Mohamed, Habiba A Ibrahim, Zahra Fathy Ibrahim, Muhammad Kazim, Adel Ammar, Bilel Benjdira, Alaa M Khamis, Ibrahim A Hameed, and others. Drone deep reinforcement learning: a review. <em>Electronics</em>, 10(9):999, 2021.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">BI99</span></dt>
<dd><p>Leemon C Baird III. Reinforcement learning through gradient descent. Technical Report, CARNEGIE-MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE, 1999.</p>
</dd>
<dt class="label" id="id48"><span class="brackets">BHB+20</span></dt>
<dd><p>André Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement learning with generalized policy updates. <em>Proceedings of the National Academy of Sciences</em>, 117(48):30079–30087, 2020.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">BFH17</span></dt>
<dd><p>Qianwen Bi, Michael Finke, and Sandra J Huston. Financial software use and retirement savings. <em>Journal of Financial Counseling and Planning</em>, 28(1):107–128, 2017.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">BDTS20</span></dt>
<dd><p>Rachel Qianwen Bi, Lukas R Dean, Jingpeng Tang, and Hyrum L Smith. Limitations of retirement planning software: examining variance between inputs and outputs. <em>Journal of Financial Service Professionals</em>, 2020.</p>
</dd>
<dt class="label" id="id32"><span class="brackets">Blo18</span></dt>
<dd><p>Daniel Alexandre Bloch. Machine learning: models and algorithms. <em>Machine Learning: Models And Algorithms, Quantitative Analytics</em>, 2018.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">BS11</span></dt>
<dd><p>Kenneth Bruhn and Mogens Steffensen. Household consumption, investment and life insurance. <em>Insurance: Mathematics and Economics</em>, 48(3):315–325, 2011.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">CL20</span></dt>
<dd><p>Shou Chen and Guangbing Li. Time-inconsistent preferences, consumption, investment and life insurance decisions. <em>Applied Economics Letters</em>, 27(5):392–399, 2020.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">DPMSNR14</span></dt>
<dd><p>Albert De-Paz, Jesus Marin-Solano, Jorge Navas, and Oriol Roch. Consumption, investment and life insurance strategies with heterogeneous discounting. <em>Insurance: Mathematics and Economics</em>, 54:66–75, 2014.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">DH20</span></dt>
<dd><p>Matthew Dixon and Igor Halperin. G-learner and girl: goal based wealth management with reinforcement learning. <em>arXiv preprint arXiv:2002.10990</em>, 2020.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">DHB20</span></dt>
<dd><p>Matthew F Dixon, Igor Halperin, and Paul Bilokon. <em>Machine Learning in Finance: From Theory to Practice</em>. Springer International Publishing AG, Cham, 2020. ISBN 9783030410674.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">Dol10</span></dt>
<dd><p>Victor Dolk. Survey reinforcement learning. <em>Eindhoven University of Technology</em>, 2010.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">DMBE18</span></dt>
<dd><p>Taft Dorman, Barry S Mulholland, Qianwen Bi, and Harold Evensky. The efficacy of publicly-available retirement planning tools. <em>Available at SSRN 2732927</em>, 2018.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">EWC21</span></dt>
<dd><p>Maria K Eckstein, Linda Wilbrecht, and Anne GE Collins. What do reinforcement learning models measure? interpreting model parameters in cognition and neuroscience. <em>Current Opinion in Behavioral Sciences</em>, 41:128–137, 2021.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">FPT15</span></dt>
<dd><p>Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. <em>arXiv preprint arXiv:1512.08562</em>, 2015.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">FranccoisLHI+18</span></dt>
<dd><p>Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An introduction to deep reinforcement learning. <em>arXiv preprint arXiv:1811.12560</em>, 2018.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">GP13</span></dt>
<dd><p>Matthieu Geist and Olivier Pietquin. Algorithmic survey of parametric value function approximation. <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 24(6):845–867, 2013.</p>
</dd>
<dt class="label" id="id53"><span class="brackets">GulerLP19</span></dt>
<dd><p>Batuhan Güler, Alexis Laignelet, and Panos Parpas. Towards robust and stable deep learning algorithms for forward backward stochastic differential equations. <em>arXiv preprint arXiv:1910.11623</em>, 2019.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">Ham18</span></dt>
<dd><p>Ahmad Hammoudeh. A concise introduction to reinforcement learning. 2018.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">HJ+20</span></dt>
<dd><p>Jiequn Han, Arnulf Jentzen, and others. Algorithms for solving high dimensional pdes: from nonlinear monte carlo to machine learning. <em>arXiv preprint arXiv:2008.13333</em>, 2020.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">HJW17</span></dt>
<dd><p>Jiequn Han, Arnulf Jentzen, and E Weinan. Overcoming the curse of dimensionality: solving high-dimensional partial differential equations using deep learning. <em>arXiv preprint arXiv:1707.02568</em>, pages 1–13, 2017.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">HJW18</span></dt>
<dd><p>Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations using deep learning. <em>Proceedings of the National Academy of Sciences</em>, 115(34):8505–8510, 2018.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">Her11</span></dt>
<dd><p>Hal E Hershfield. Future self-continuity: how conceptions of the future self transform intertemporal choice. <em>Annals of the New York Academy of Sciences</em>, 1235:30, 2011.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">KSL19</span></dt>
<dd><p>Sung-Kyun Kim, Oren Salzman, and Maxim Likhachev. Pomhdp: search-based belief space planning using multiple heuristics. In <em>Proceedings of the International Conference on Automated Planning and Scheduling</em>, volume 29, 734–744. 2019.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">KC21</span></dt>
<dd><p>Yeo Jin Kim and Min Chi. Time-aware q-networks: resolving temporal irregularity for deep reinforcement learning. <em>arXiv preprint arXiv:2105.02580</em>, 2021.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">KS15</span></dt>
<dd><p>Morten Tolver Kronborg and Mogens Steffensen. Optimal consumption, investment and life insurance with surrender option guarantee. <em>Scandinavian Actuarial Journal</em>, 2015(1):59–87, 2015.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">Leu94</span></dt>
<dd><p>Siu Fai Leung. Uncertain lifetime, the theory of the consumer, and the life cycle hypothesis. 1994.</p>
</dd>
<dt class="label" id="id17"><span class="brackets">Lev18</span></dt>
<dd><p>Sergey Levine. Reinforcement learning and control as probabilistic inference: tutorial and review. <em>arXiv preprint arXiv:1805.00909</em>, 2018.</p>
</dd>
<dt class="label" id="id70"><span class="brackets">MVHS14</span></dt>
<dd><p>Ashique Rupam Mahmood, Hado Van Hasselt, and Richard S Sutton. Weighted importance sampling for off-policy learning with linear function approximation. In <em>NIPS</em>, 3014–3022. 2014.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">Mer69</span></dt>
<dd><p>Robert C Merton. Lifetime portfolio selection under uncertainty: the continuous-time case. <em>The review of Economics and Statistics</em>, pages 247–257, 1969.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">Mer75</span></dt>
<dd><p>Robert C Merton. Optimum consumption and portfolio rules in a continuous-time model. In <em>Stochastic Optimization Models in Finance</em>, pages 621–661. Elsevier, 1975.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">MBJ20a</span></dt>
<dd><p>Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. A framework for reinforcement learning and planning. <em>arXiv preprint arXiv:2006.15009</em>, 2020.</p>
</dd>
<dt class="label" id="id78"><span class="brackets">MBJ20b</span></dt>
<dd><p>Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement learning: a survey. <em>arXiv preprint arXiv:2006.16712</em>, 2020.</p>
</dd>
<dt class="label" id="id60"><span class="brackets">MJ20</span></dt>
<dd><p>Amit Kumar Mondal and N Jamali. A survey of reinforcement learning techniques: strategies, recent development, and future directions. <em>arXiv preprint arXiv:2001.06921</em>, 2020.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">NRC20</span></dt>
<dd><p>Muddasar Naeem, S Tahir H Rizvi, and Antonio Coronato. A gentle introduction to reinforcement learning and its application in different fields. <em>IEEE Access</em>, 2020.</p>
</dd>
<dt class="label" id="id56"><span class="brackets">NZKN19</span></dt>
<dd><p>Farzad Niroui, Kaicheng Zhang, Zendai Kashino, and Goldie Nejat. Deep reinforcement learning robot for search and rescue applications: exploration in unknown cluttered environments. <em>IEEE Robotics and Automation Letters</em>, 4(2):610–617, 2019. <a class="reference external" href="https://doi.org/10.1109/LRA.2019.2891991">doi:10.1109/LRA.2019.2891991</a>.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">PRD96</span></dt>
<dd><p>Elena Pashenkova, Irina Rish, and Rina Dechter. Value iteration and policy iteration algorithms for markov decision problem. In <em>AAAI’96: Workshop on Structural Issues in Planning and Temporal Reasoning</em>. Citeseer, 1996.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">PVW11</span></dt>
<dd><p>James M Poterba, Steven F Venti, and David A Wise. Were they prepared for retirement? financial status at advanced ages in the hrs and ahead cohorts. In <em>Investigations in the Economics of Aging</em>, pages 21–69. University of Chicago Press, 2011.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">Rai18</span></dt>
<dd><p>Maziar Raissi. Forward-backward stochastic neural networks: deep learning of high-dimensional partial differential equations. <em>arXiv preprint arXiv:1804.07010</em>, 2018.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">Ric75</span></dt>
<dd><p>Scott F Richard. Optimal consumption, portfolio and life insurance rules for an uncertain lived individual in a continuous time model. <em>Journal of Financial Economics</em>, 2(2):187–203, 1975.</p>
</dd>
<dt class="label" id="id59"><span class="brackets">RMM18</span></dt>
<dd><p>Lev Rozonoer, Boris Mirkin, and Ilya Muchnik. Braverman readings in machine learning. In <em>Key Ideas from Inception to Current State: International Conference Commemorating the 40th Anniversary of Emmanuil Braverman's Decease, Boston, MA Invited Talks. Cham: Springer International Publishing</em>. Springer, 2018.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">San21</span></dt>
<dd><p>Nimish Sanghi. <em>Deep Reinforcement Learning with Python: With Pytorch, TensorFlow and OpenAI Gym</em>. Apress L. P, Berkeley, CA, 2021. ISBN 1484268083.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">SW16</span></dt>
<dd><p>Yang Shen and Jiaqin Wei. Optimal investment-consumption-insurance with random parameters. <em>Scandinavian Actuarial Journal</em>, 2016(1):37–62, 2016.</p>
</dd>
<dt class="label" id="id61"><span class="brackets">SBLL19</span></dt>
<dd><p>Joohyun Shin, Thomas A Badgwell, Kuang-Hung Liu, and Jay H Lee. Reinforcement learning–overview of recent progress and implications for process control. <em>Computers &amp; Chemical Engineering</em>, 127:282–294, 2019.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">SB18</span></dt>
<dd><p>Richard S Sutton and Andrew G Barto. <em>Reinforcement learning: An introduction</em>. MIT press, 2018.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">VOW12</span></dt>
<dd><p>Martijn Van Otterlo and Marco Wiering. Reinforcement learning and markov decision processes. In <em>Reinforcement learning</em>, pages 3–42. Springer, 2012.</p>
</dd>
<dt class="label" id="id3"><span class="brackets">WZL09</span></dt>
<dd><p>Fei-Yue Wang, Huaguang Zhang, and Derong Liu. Adaptive dynamic programming: an introduction. <em>IEEE computational intelligence magazine</em>, 4(2):39–47, 2009.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">WZZ19</span></dt>
<dd><p>Haoran Wang, Thaleia Zariphopoulou, and Xun Yu Zhou. Exploration versus exploitation in reinforcement learning: a stochastic control approach. <em>Available at SSRN 3316387</em>, 2019.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">WCJW20</span></dt>
<dd><p>Jiaqin Wei, Xiang Cheng, Zhuo Jin, and Hao Wang. Optimal consumption–investment and life-insurance purchase strategy for couples with correlated lifetimes. <em>Insurance: Mathematics and Economics</em>, 91:244–256, 2020.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">WHJ17</span></dt>
<dd><p>E Weinan, Jiequn Han, and Arnulf Jentzen. Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations. <em>Communications in Mathematics and Statistics</em>, 5(4):349–380, 2017.</p>
</dd>
<dt class="label" id="id2"><span class="brackets">Yaa65</span></dt>
<dd><p>Menahem E Yaari. Uncertain lifetime, life insurance, and the theory of the consumer. <em>The Review of Economic Studies</em>, 32(2):137–150, 1965.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">YLL+19</span></dt>
<dd><p>Niko Yasui, Sungsu Lim, Cam Linke, Adam White, and Martha White. An empirical and conceptual categorization of value-based exploration methods. <em>ICML Exploration in Reinforcement Learning Workshop</em>, 2019.</p>
</dd>
<dt class="label" id="id9"><span class="brackets">Ye06</span></dt>
<dd><p>Jinchun Ye. <em>Optimal life insurance purchase, consumption and portfolio under an uncertain life</em>. University of Illinois at Chicago, 2006.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Discussion.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">discussion</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Ignace Decocq<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>