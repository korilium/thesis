
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Financial Applications of Reinforcement Learning &#8212; Future Financial Planning Tools for Consumers</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e2363ea40746bee74734a24ffefccd78.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="discussion" href="Discussion.html" />
    <link rel="prev" title="Reinforcement Learning" href="Reinforcement_learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Future Financial Planning Tools for Consumers</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="abstract.html">
   Abstract
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Reinforcement_learning.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Financial Applications of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Discussion.html">
   discussion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Financial_application.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/korilium/thesis"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/korilium/thesis/issues/new?title=Issue%20on%20page%20%2FFinancial_application.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/korilium/thesis/edit/master/Financial_application.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/korilium/thesis/master?urlpath=tree/Financial_application.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#g-learner-for-goal-based-retirment-plan-optimization">
   G-learner for goal-based retirment plan optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-target-of-the-investment-profolio">
     The target of the investment profolio
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applying-the-g-learning-to-the-investment-portfolio">
     Applying the G-learning to the investment portfolio
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reinforcement-learning-in-optimal-control-applying-the-deep-bsde-method">
   Reinforcement learning in optimal control: applying the Deep BSDE method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-optimal-consumption-investment-and-life-insurance-model">
     an Optimal consumption, investment and life insurance model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-model-specifications">
     The model specifications
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dynamic-programming-principle">
     dynamic programming principle
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="financial-applications-of-reinforcement-learning">
<h1>Financial Applications of Reinforcement Learning<a class="headerlink" href="#financial-applications-of-reinforcement-learning" title="Permalink to this headline">¶</a></h1>
<p>Advancements in Reinforcement Learning can be applied to a financial setting. In this subsection, two financial applications are introduced. The first application applies the G-learning algorithm, described in the previous section, in a goal-based wealth management environment. The second application presents a model proposed for optimal consumption, life insurance and investment with a terminal PDE as the solution. This model can be extended in multiple ways ,and can be applied in a broad range of financial scenarios, making it an attractive model for consumers. The only problem of the model is the curse of dimensionality which makes the Hamilton-Jacobi-Bellman (HJB) equation of the model in higher dimensions impossible to solve. The deep BSDE method, presented in the previous section, can be applied to the problem, thereby solving the HJB equation in higher dimensions. Although the method originates from optimal control literature, it resembles many features of the reinforcement algorithms.</p>
<div class="section" id="g-learner-for-goal-based-retirment-plan-optimization">
<h2>G-learner for goal-based retirment plan optimization<a class="headerlink" href="#g-learner-for-goal-based-retirment-plan-optimization" title="Permalink to this headline">¶</a></h2>
<p>Before the algorithm is applied in the retirement plan optimization setting, the G-learner needs to be described to the terminal setting. Recall from the G-learner section that the action free-energy function is</p>
<div class="math notranslate nohighlight" id="equation-h-rpo">
<span class="eqno">(22)<a class="headerlink" href="#equation-h-rpo" title="Permalink to this equation">¶</a></span>\[H^{\pi}(s,a) = \mathbb{E}[R|s,a] + \gamma \mathbb{E}[F^{\pi}(s')|s,a] \]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-f-rpo">
<span class="eqno">(23)<a class="headerlink" href="#equation-f-rpo" title="Permalink to this equation">¶</a></span>\[F^{\pi}(s) = \sum_a \pi(a|s)[\frac{1}{\beta}log\frac{\pi(a|s)}{\pi_0(a|s)} + H^{\pi}(s,a)]\]</div>
<p>The optimal action policy could be derived from <span class="math notranslate nohighlight">\(F^{\pi}(s)\)</span> by maximizing the funcion by <a class="reference internal" href="Reinforcement_learning.html#equation-soft-min">(15)</a> and then evaluating <span class="math notranslate nohighlight">\(F^{\pi}(s)\)</span> at that point <a class="reference internal" href="Reinforcement_learning.html#equation-soft-minf">(16)</a>. Using <a class="reference internal" href="Reinforcement_learning.html#equation-soft-minf">(16)</a> and applying it in <a class="reference internal" href="Reinforcement_learning.html#equation-soft-min">(15)</a> gives us the optimal action policy</p>
<div class="math notranslate nohighlight" id="equation-pi-rpo">
<span class="eqno">(24)<a class="headerlink" href="#equation-pi-rpo" title="Permalink to this equation">¶</a></span>\[\pi(a_t|s_t) = \pi_0(a_t|s_t)e^{\beta (H^{\pi}_t(s_t,a_t) - F_t^{\pi}(s_t) )} \]</div>
<p>These three equation <a class="reference internal" href="#equation-h-rpo">(22)</a>, <a class="reference internal" href="#equation-f-rpo">(23)</a> and <a class="reference internal" href="#equation-pi-rpo">(24)</a> create a system of equations for G-learning that should be solved self-consistently for <span class="math notranslate nohighlight">\(\pi(a_t|s_t)\)</span>, <span class="math notranslate nohighlight">\(H^{\pi}_t(s_t,a_t)\)</span> and <span class="math notranslate nohighlight">\(F^{\pi}_t(s_t)\)</span> by backward recursion for <span class="math notranslate nohighlight">\(t = T-1, ..., 0\)</span> with terminal conditions</p>
<div class="math notranslate nohighlight">
\[ H^{\pi}_T(s_t, a_T^*) = \hat{R}T(s_t,a_T^*)\]</div>
<div class="math notranslate nohighlight">
\[ F_t^{\pi} = G_T^{\pi}(s_t,a_T^*) = \hat{R}_T(s_t,a_T^*) \]</div>
<p>The system of equations can be reduced to a non-linear equation when the rewards are observed.</p>
<div class="math notranslate nohighlight">
\[ G^{\pi}_t(s,a) = \hat{R}(s_t,a_t) + \mathbb{E}_{t,a}\left[\frac{\gamma}{\beta} log \sum_{a_{t+1}} \pi_0(a_{t+1}|x_{t+1})e^{\beta H^{\pi}_{t+1}(x_{t+1}, a_{t+1})}\right] \]</div>
<p>This is the soft relaxation of the Bellman optimality equation for the action-value Q-function, described in the G-learner section.</p>
<p>Before the G-learning algorithm can be applied, the signal process of the environment should be presented. Therefore, we build in the first subsection a pre-specified target for our portfolio optimization together with the return specifications. The algorithm can then optimize its behavior by adapting to the target.</p>
<div class="section" id="the-target-of-the-investment-profolio">
<h3>The target of the investment profolio<a class="headerlink" href="#the-target-of-the-investment-profolio" title="Permalink to this headline">¶</a></h3>
<p>Following the assumptions and notation in the model of <span id="id1">[<a class="reference internal" href="Appendix.html#id14">DH20</a>]</span> for the investment portfolio. The model can be described as follows. the position in the assets <span class="math notranslate nohighlight">\(i = 1, 2, ..., n\)</span> of the portfolio model are set in dollar values denoted as a vector <span class="math notranslate nohighlight">\(s_t\)</span>, with components <span class="math notranslate nohighlight">\((s_t)_i\)</span> for a dollar value of asset <span class="math notranslate nohighlight">\(i\)</span> at the beginning of period <span class="math notranslate nohighlight">\(t\)</span>. The first asset <span class="math notranslate nohighlight">\(n=1\)</span> of the portfolio is a risk-free bank account with risk-free interest rate <span class="math notranslate nohighlight">\(r_f\)</span>, while the other assets are risky with uncertain returns <span class="math notranslate nohighlight">\(r_t\)</span>, whose expected values are <span class="math notranslate nohighlight">\(\overline{r}_t\)</span>. The covariance matrix of return is <span class="math notranslate nohighlight">\(\Sigma_r\)</span> of size <span class="math notranslate nohighlight">\((N-1) \times  (N-1)\)</span>. <span class="math notranslate nohighlight">\(u_t\)</span> are the trades of the investement portfolio at timestep <span class="math notranslate nohighlight">\(t\)</span>, while <span class="math notranslate nohighlight">\(c_t\)</span> is the cash installment in the plan at time <span class="math notranslate nohighlight">\(t\)</span>. Both <span class="math notranslate nohighlight">\(u_t\)</span> and <span class="math notranslate nohighlight">\(c_t\)</span> need to be optimized and this pair <span class="math notranslate nohighlight">\((c_t, u_t)\)</span> can thus be considered the action variables of the dynamic optimization problem corresponding to the retirement plan.</p>
<p>A pre-specified target value <span class="math notranslate nohighlight">\(\hat{P}_{t+1}\)</span> is set at time <span class="math notranslate nohighlight">\(t\)</span> for the next time step <span class="math notranslate nohighlight">\(t+1\)</span>. The target value <span class="math notranslate nohighlight">\(\hat{P}_{t+1}\)</span> is set such that at step <span class="math notranslate nohighlight">\(t\)</span> it will exceed the next step value <span class="math notranslate nohighlight">\(V_{t+1} = (1+ r_t)(s_t +u_t)\)</span> of the portfolio. The reward system of the environment is determined such that a penalty is created when the portfolio performance is under-cutting the target value.</p>
<div class="math notranslate nohighlight" id="equation-target">
<span class="eqno">(25)<a class="headerlink" href="#equation-target" title="Permalink to this equation">¶</a></span>\[R_t(s_t, u_t, c_t) = - c_t - \lambda \mathbb{E}_t \left[\left(\hat{P}_{t+1} - (1+r_t)(s_t + u_t)\right)_+\right] - u_t^{\top} \Omega u_t\]</div>
<p>The first term is the installment amount <span class="math notranslate nohighlight">\(c_t\)</span> at the beginning of time period <span class="math notranslate nohighlight">\(t\)</span>, the second is the penalty if the portfolio underperformance the target value ,and the last term is an approximation for transaction costs using a convex function with parameter matrix <span class="math notranslate nohighlight">\(\Omega\)</span> and serves as a <span class="math notranslate nohighlight">\(L_2\)</span> regularization (see next section).</p>
<p>We modify <a class="reference internal" href="#equation-target">(25)</a> in two ways for two reasons. First, the two decision vriabales <span class="math notranslate nohighlight">\(c_t\)</span> and <span class="math notranslate nohighlight">\(u_t\)</span> are not independent and have the following constraint:</p>
<div class="math notranslate nohighlight">
\[ \sum_{n=1}^N = c_t \]</div>
<p>Thereby, setting the total change in all positions equal to the cash installments at time <span class="math notranslate nohighlight">\(t\)</span>.  The second reason is that the <span class="math notranslate nohighlight">\(max(\centerdot,0)\)</span> operator is difficult to work with under the expectation and is therefore approximated with a quadratic function. This gives us</p>
<div class="math notranslate nohighlight" id="equation-adapted-target">
<span class="eqno">(26)<a class="headerlink" href="#equation-adapted-target" title="Permalink to this equation">¶</a></span>\[ R_t(s_t, u_t) = - \sum_{n=1}^N u_{tn} - \lambda \mathbb{E}_t \left[\left(\hat{P}_{t+1} - (1+r_t)(s_t+u_t)\right)^2\right] -u_t^{\top} \Omega u_t \]</div>
<p>The benefits of the adapted value function are twofold. First, it is able to solve the constraint between the cash injection <span class="math notranslate nohighlight">\(c_t\)</span> and the trades <span class="math notranslate nohighlight">\(u_t\)</span>, reducing the dimensionality of the optimization problem. The second benefit is making the reward function highly tractable by transforming it into a quadratic function in actions <span class="math notranslate nohighlight">\(u_t\)</span>. The only drawback is that the penalization is symmetric, penalizing both <span class="math notranslate nohighlight">\(V_{t+1} \gg \hat{P}_{t+1}\)</span> and <span class="math notranslate nohighlight">\(V_{t+1} \ll \hat{P}_{t+1}\)</span>. To mitigate the drawback, we only consider target values considerably higher than the expectation of the next-period portfolio value. A good choice for the target value <span class="math notranslate nohighlight">\(\hat{P}_{t+1}\)</span> can for example be a linear combination of a portfolio-independent benchmark <span class="math notranslate nohighlight">\(B_t\)</span> and a portfolio fixed-rate growth <span class="math notranslate nohighlight">\(\eta\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \hat{P}_{t+1} = (1-\rho)B_t + \rho \eta \mathbb{1}^{\top}s_t \]</div>
<p>Where <span class="math notranslate nohighlight">\(0 \leq \rho \leq 1\)</span> is the relative weight of the two terms and <span class="math notranslate nohighlight">\(\eta &gt;1 \)</span> defines the desired growth rate of the current portfolio. Note, that <span class="math notranslate nohighlight">\(B_t\)</span> and <span class="math notranslate nohighlight">\(\eta\)</span> need to be sufficiently large such that <a class="reference internal" href="#equation-adapted-target">(26)</a> is a reasonable proxy of <a class="reference internal" href="#equation-target">(25)</a>. The advantage of the target portfolio is that both parameters can be learned from an observed behavior of a financial agent using Inverse Reinforcement Learning.</p>
<p>Equation <a class="reference internal" href="#equation-adapted-target">(26)</a> can be written in a quadratic form, once we denote the return as <span class="math notranslate nohighlight">\(r_t =\overline{r}_t + \tilde{\epsilon}_t\)</span> where the first component is the risk-free rate and  where <span class="math notranslate nohighlight">\(\tilde{\epsilon}_t\)</span> is an idiosyncratic noise with covariance <span class="math notranslate nohighlight">\(\Sigma_r\)</span> of size <span class="math notranslate nohighlight">\((N-1) \times (N-1)\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-quadreturn">
<span class="eqno">(27)<a class="headerlink" href="#equation-quadreturn" title="Permalink to this equation">¶</a></span>\[\begin{split}    R_t(s_t, u_t) &amp; = - \lambda \hat{P}^2_{t+1} - u^{\top}_t \mathbb{1} + 2\lambda \hat{P}_{t+1}(s_t + u_t)^{\top}(1+\overline{r}_t) - \lambda(s_t + u_t)^{\top} \hat{\Sigma}_t(s_t + u_t) - u_t^{\top} \Omega u_t 
\\
    &amp; = s^{\top}_t R_t^{(ss)}s_t + u_t^{\top}R_t^{(us)}s_t + u_t^{\top}R_t^{(uu)}u_y + s_t^{\top} R_t^{(s)} + u_t^{\top}R_t^{(u)} + R_t^{(0)} \end{split}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight" id="equation-a1d6bba5-2e8d-42c7-9a35-67cc3ede940c">
<span class="eqno">(28)<a class="headerlink" href="#equation-a1d6bba5-2e8d-42c7-9a35-67cc3ede940c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \begin{split}
         &amp; \hat{\Sigma}_t = \begin{bmatrix} 
                    0 &amp; 0\\
                    0 &amp; \Sigma 
                    \end{bmatrix}  + ( 1+ \overline{r}_t)(1+\overline{r}_t)^{\top}
\\
         &amp; R_t^{(ss)} = - \lambda \eta^2 \rho^2 \mathbb{1} \mathbb{1}^{\top} + 2\lambda \eta \rho(\mathbb{1} + \overline{r}_t) \mathbb{1}^{\top} - \lambda \hat{\Sigma}_t 
\\ 
        &amp; R_t^{(us)} = 2\lambda \eta \rho (1+\overline{r}_t)\mathbb{1}^{\top} - 2\lambda \hat{\Sigma}_t 
\\ 
        &amp; R_t^{(uu)} = -\lambda \hat{\Sigma}_t - \Omega
\\ 
        &amp; R_t^{(s)} = -2\lambda \eta \rho (1-\rho)B_t\mathbb{1} + 2\lambda(1- \rho)B_t(1+\overline{r}_t) 
\\ 
        &amp; R_t^{u} = -\mathbb{1} + 2 \lambda(1-\rho)B_t(1-\overline{r}_t)
\\ 
        &amp; R_t^{(0)} = -(1-\rho)^2\lambda B^2_t
    \end{split}
\end{equation}\]</div>
<p>The free parameters defining the reward function are thus <span class="math notranslate nohighlight">\(\lambda\)</span>, <span class="math notranslate nohighlight">\(\eta\)</span>, <span class="math notranslate nohighlight">\(\rho\)</span> and <span class="math notranslate nohighlight">\(\Omega\)</span></p>
</div>
<div class="section" id="applying-the-g-learning-to-the-investment-portfolio">
<h3>Applying the G-learning to the investment portfolio<a class="headerlink" href="#applying-the-g-learning-to-the-investment-portfolio" title="Permalink to this headline">¶</a></h3>
<p>A semi-analytical formulation of the G-learning is applied following Dixen et al. <span id="id2">[<a class="reference internal" href="Appendix.html#id14">DH20</a>]</span>. First, a functional form of the value function as a quadratic form if <span class="math notranslate nohighlight">\(s_t\)</span> is applied:</p>
<div class="math notranslate nohighlight" id="equation-quadvalue">
<span class="eqno">(29)<a class="headerlink" href="#equation-quadvalue" title="Permalink to this equation">¶</a></span>\[F_t^{\pi}(s_t) = s_t^{\top} F_t^{(ss)}s_t + s_t^{\top}F_t^{(s)} + F_t^{(0)}\]</div>
<p>The dynamic equation is written as follows</p>
<div class="math notranslate nohighlight" id="equation-dynamsyst">
<span class="eqno">(30)<a class="headerlink" href="#equation-dynamsyst" title="Permalink to this equation">¶</a></span>\[s_{t+1} = A_t(s_t+u_t) + (s_t + u_t) \circ \tilde{\epsilon}_t \text{,} \hspace{0.4cm} A_t = diag(1+\overline{r}_T) \text{,} \hspace{0.4cm} \tilde{\epsilon}_t = (0, \epsilon_t)\]</div>
<p>Where the expected return <span class="math notranslate nohighlight">\(\overline{r}_t\)</span> are available as an output of a separate statistical model like a factor model framework. The coefficients of <a class="reference internal" href="#equation-quadvalue">(29)</a> are computed backward in time starting from the last maturity <span class="math notranslate nohighlight">\(t = T-1\)</span>. At time-step <span class="math notranslate nohighlight">\(t= T-1\)</span>, the reward function <a class="reference internal" href="#equation-quadvalue">(29)</a> can be optimized using an analytical approach by the following action:</p>
<div class="math notranslate nohighlight" id="equation-optimized">
<span class="eqno">(31)<a class="headerlink" href="#equation-optimized" title="Permalink to this equation">¶</a></span>\[u_{T-1} = \tilde{\Sigma}_{T-1}^{-1}\left(\frac{1}{2\lambda}R_t^{(u)} + \frac{1}{2\lambda}R_t^{(us)}x_{T-1}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{\Sigma}_{T-1}\)</span> is defined as</p>
<div class="math notranslate nohighlight" id="equation-optimizedcov">
<span class="eqno">(32)<a class="headerlink" href="#equation-optimizedcov" title="Permalink to this equation">¶</a></span>\[ \tilde{\Sigma}_{T-1} = \hat{\Sigma}_{T-1} + \frac{1}{\lambda}\Omega \]</div>
<p>Notice that the last term <span class="math notranslate nohighlight">\(\Omega\)</span> which represents the convex costs in <a class="reference internal" href="#equation-target">(25)</a>, creates an <span class="math notranslate nohighlight">\(L_2\)</span> regularization of matrix inversion in <a class="reference internal" href="#equation-optimizedcov">(32)</a>.</p>
<p>Now that the reward function is solved for time-step <span class="math notranslate nohighlight">\(t= T-1 \)</span>, the coeffiecients <span class="math notranslate nohighlight">\(F_{T-1}^{(ss)}\)</span>, <span class="math notranslate nohighlight">\(F_{T-1}^{(s)}\)</span> and <span class="math notranslate nohighlight">\(F_{T-1}^{(0)}\)</span> of the value function  can be calculated. We know that at time-step <span class="math notranslate nohighlight">\(t = T-1\)</span>, the value function needs to equal the reward function <span class="math notranslate nohighlight">\( F_{T-1}^{\pi}(s_{T-1})= \hat{R}_{T-1}\)</span>. Indeed,  by plugging <a class="reference internal" href="#equation-optimized">(31)</a> back into <a class="reference internal" href="#equation-quadreturn">(27)</a> and comparing the result with <a class="reference internal" href="#equation-quadvalue">(29)</a>, we get the following terminal conditions for the parameters of <a class="reference internal" href="#equation-quadvalue">(29)</a>:</p>
<div class="math notranslate nohighlight">
\[ F_{T-1}^{(ss)} = R_{T-1}^{(ss)} + \frac{1}{2\lambda}[R_{T-1}^{(us)}]^{\top} [\tilde{\Sigma}_{T-1}^{-1}]^{\top}R_{T-1}^{(us)} + \frac{1}{4\lambda^2}[R_{T-1}^{(us)}]^{\top}[\tilde{\Sigma}_{T-1}^{-1}]^{\top} R_{T-1}^{(uu)}\tilde{\Sigma}_{T-1}^{-1}R_{T-1}^{(us)} \]</div>
<div class="math notranslate nohighlight">
\[ F_{T-1}^{(s)} = R_{T-1}^{(s)} + \frac{1}{\lambda}[R_{T-1}^{(us)}]^{\top} [\tilde{\Sigma}_{T-1}^{-1}]^{\top}R_{T-1}^{(u)} + \frac{1}{2\lambda^2}[R_{T-1}^{(us)}]^{\top}[\tilde{\Sigma}_{T-1}^{-1}]^{\top} R_{T-1}^{(uu)}\tilde{\Sigma}_{T-1}^{-1}R_{T-1}^{(u)} \]</div>
<div class="math notranslate nohighlight">
\[ F_{T-1}^{(0)} = R_{T-1}^{(0)} + \frac{1}{2\lambda}[R_{T-1}^{(u)}]^{\top} [\tilde{\Sigma}_{T-1}^{-1}]^{\top}R_{T-1}^{(u)} + \frac{1}{4\lambda^2}[R_{T-1}^{(u)}]^{\top}[\tilde{\Sigma}_{T-1}^{-1}]^{\top} R_{T-1}^{(uu)}\tilde{\Sigma}_{T-1}^{-1}R_{T-1}^{(u)}\]</div>
<p>Any other time-step <span class="math notranslate nohighlight">\(t =T-2,..., 0\)</span> is computed using backward recursion following the Bellman equation but first, the conditional expectation of the next period F-function is computed using <a class="reference internal" href="#equation-dynamsyst">(30)</a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split} \mathbb{E}_{t,a}[F^{\pi}_{t+1}(s_{t+1})] = (s_t + u_t)^{\top} \left( A^{\top}_t \overline{F}_{t+1}^{(ss)}A_t + \tilde{\Sigma}_r \circ \overline{F}^{(ss)}_{t+1} \right) (s_t + u_t) \\ 
+ (s_t + u_t)^{\top} A^{\top}_t \overline{F}_{t+1}^{(s)} + \overline{F}_{t+1}^{(0)}, \hspace{0.4cm} \tilde{\Sigma}_r = 
\begin{bmatrix} 
0 &amp; 0 \\ 
0 &amp; \Sigma_r 
\end{bmatrix}  \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{F}_{t+1}^{ss} = \mathbb{E}[F_{t+1}^{(ss)}]\)</span> similarly for <span class="math notranslate nohighlight">\(\overline{F}_{t+1}^{(x)}\)</span> and  <span class="math notranslate nohighlight">\(\overline{F}_{t+1}^{(0)}\)</span>. Now, we plug in the conditional expectation of the F-function and the reward function in the Bellman equation</p>
<div class="math notranslate nohighlight">
\[ H_t^{\pi}(s_t, u_t) = \hat{R}_t(s_t, u_t) + \gamma \mathbb{E}_{t,u}[F_{t+1}^{\pi}(s_{t+1}|s_t,u_t)] \]</div>
<p>Notice that both the reward function <a class="reference internal" href="#equation-quadreturn">(27)</a> and the conditional expectation of the F-function are quadratic functions of <span class="math notranslate nohighlight">\(x_t\)</span> and <span class="math notranslate nohighlight">\(u_t\)</span> in the Bellman equation. This means that the action-value function is also a quadratic function  of <span class="math notranslate nohighlight">\(x_t\)</span> and <span class="math notranslate nohighlight">\(u_t\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-action-valueg">
<span class="eqno">(33)<a class="headerlink" href="#equation-action-valueg" title="Permalink to this equation">¶</a></span>\[H_t^{\pi}(s_t, u_t) = s_t^{\top} Q^{(ss)}_t s_t + u_t^{\top} Q_t^{(us)}s_t + u^{\top}_t Q_t^{(uu)}u_t + s_t^{\top}Q_t^{(s)} + u^{\top}_t Q^{(u)}_t + Q_t^{(0)} \]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight" id="equation-4e25f650-6e97-4abb-b85d-09c1ced67e1d">
<span class="eqno">(34)<a class="headerlink" href="#equation-4e25f650-6e97-4abb-b85d-09c1ced67e1d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \begin{split}
        &amp; Q^{(ss)}_t = R_t^{(ss)} + \gamma(A_t^{\top}\overline{F}_{t+1}^{(ss)} A_t + \tilde{\Sigma}_r \circ \overline{F}_{t+1}^{ss})
\\ 
        &amp; Q^{(us)}_t = R_t^{(us)} + 2\gamma(A_t^{\top}\overline{F}_{t+1}^{(ss)} A_t + \tilde{\Sigma}_r \circ \overline{F}_{t+1}^{ss}) 
\\ 
        &amp; Q^{(uu)}_t = R_t^{(uu)} + \gamma(A_t^{\top}\overline{F}_{t+1}^{(ss)} A_t + \tilde{\Sigma}_r \circ \overline{F}_{t+1}^{ss}) - \Omega 
\\ 
        &amp; Q^{(s)}_t = R_t^{(s)} + \gamma A_t^{\top}\overline{F}_{t+1}^{(s)}
\\
        &amp; Q^{(u)}_t = R_t^{(u)} + \gamma A_t^{\top}\overline{F}_{t+1}^{(s)}
\\
        &amp; Q^{(0)}_t = R_t^{(0)} + \gamma F_{t+1}^{(0)}
    \end{split}
\end{equation}\]</div>
<p>Now that the action-value function is computed, we only need to calculate the F-function for the current step. Remember from <a class="reference internal" href="Reinforcement_learning.html#equation-soft-minf">(16)</a> that the F-function can be described by the prior policy distribution <span class="math notranslate nohighlight">\(\pi_0(u_t|x_t)\)</span>  and the action value function <span class="math notranslate nohighlight">\(H_t^{\pi}(s_t, u_t)\)</span> .</p>
<div class="math notranslate nohighlight">
\[ F_t^{\pi}(s_t) = \frac{1}{\beta}log \int \pi_0(u_t|s_t)e^{\beta H_t^{\pi}(s_t, u_t)}du_t \]</div>
<p>Let the prior policy be a Gaussian:</p>
<div class="math notranslate nohighlight">
\[ \pi_0(u_t|s_t) = \frac{1}{\sqrt{(2\pi)^n|\Sigma_p|}}e^{-\frac{1}{2}(u_t - \hat{u}_t)^{\top}\Sigma_p^{-1}(u_t - \hat{u}_t)} \]</div>
<p>Where the mean value is a linear function of the state <span class="math notranslate nohighlight">\(x_t\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \hat{u}_t = \overline{u}_t + \overline{v}_t s_t\]</div>
<p>By applying the n-dimesnional Gaussian integration formula, the integration over <span class="math notranslate nohighlight">\(u_t\)</span> in {eq}`` can be performed analytically</p>
<div class="math notranslate nohighlight">
\[ \int e^{\frac{1}{2}u^{\top}Au+u^{\top}B} d^{n}u = \sqrt{\frac{(2\pi)}{|A|}} e^{\frac{1}{2}B^{\top}A^{-1}B} \]</div>
<p>where <span class="math notranslate nohighlight">\(|A|\)</span> denotes the determinant of matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>Once the Gaussian integration is performed, we can again compare the resulting expression with <a class="reference internal" href="#equation-quadvalue">(29)</a> and obtain the following coefficients:</p>
<div class="math notranslate nohighlight" id="equation-entropyvalue">
<span class="eqno">(35)<a class="headerlink" href="#equation-entropyvalue" title="Permalink to this equation">¶</a></span>\[\begin{split}    F^{\pi}_t(s_t) &amp; = s_t^{\top}F_t^{(ss)}s_t + s_t^{\top}F_t^{s} + F^{0}_t 
\\ 
    F_t^{(ss)} &amp; = Q_t^{(ss)} + \frac{1}{2\beta}(U^{\top}_t\overline{\Sigma}_p^{-1} U_t - \overline{v}_t^{\top}\Sigma_p^{-1} \overline{v}_t)
\\
    F_t^{(s)} &amp; = Q_t^{(s)} + \frac{1}{\beta}(U^{\top}_t\overline{\Sigma}_p^{-1} W_t - \overline{v}_t^{\top}\Sigma_p^{-1} \overline{u}_t) 
\\
    F_t^{(0)} &amp; = Q_t^{(0)} + \frac{1}{2\beta}(W^{\top}_t\overline{\Sigma}_p^{-1} W_t - \overline{u}_t^{\top}\Sigma_p^{-1} \overline{u}_t) - \frac{1}{2\beta}(log|\Sigma_p| + log|\overline{\Sigma_p}|) \end{split}\]</div>
<p>With auxiliary parameters</p>
<div class="math notranslate nohighlight">
\[ U_t = \beta Q^{(us)} + \Sigma^{-1}_p\overline{v}_t \]</div>
<div class="math notranslate nohighlight">
\[ W_t = \beta Q_t^{(u)} + \Sigma^{-1}_p \overline{u}_t \]</div>
<div class="math notranslate nohighlight">
\[ \overline{\Sigma}_p = \Sigma_p^{-1} - 2 \beta Q_t^{(uu)} \]</div>
<p>Now from <a class="reference internal" href="#equation-pi-rpo">(24)</a> we know that the optimal policy is</p>
<div class="math notranslate nohighlight">
\[ \pi(u_t|s_t) = \pi_0(u_t|s_t)e^{\beta(H^{\pi}_t(s_t,u_t) - F_t^{\pi}(s_t))} \]</div>
<p>By applying the action-value function <span class="math notranslate nohighlight">\(G^{\pi}_t(x_t,u_T)\)</span> and the entropy regularized value function <a class="reference internal" href="#equation-entropyvalue">(35)</a> to the optimal policy, we obtain the update of the policy parameters for time-step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-updateg">
<span class="eqno">(36)<a class="headerlink" href="#equation-updateg" title="Permalink to this equation">¶</a></span>\[ \pi(u_t|s_t) = \frac{1}{\sqrt{(2 \pi)^n |\tilde{\Sigma}_p|}} e^{-\frac{1}{2}(u_t - \tilde{u}_t - \tilde{v}_t s_t)^{\top} \Sigma_p^{-1}(u_t - \hat{u}_t - \tilde{v}_t s_t)}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\tilde{\Sigma}^{-1}_p = \Sigma_p^{-1} -2\beta Q^{(uu)} \]</div>
<div class="math notranslate nohighlight">
\[ \tilde{u}_t = \tilde{\Sigma}_p (\Sigma^{-1}_p \overline{u}_t + \beta Q^{(u)}_t) \]</div>
<div class="math notranslate nohighlight">
\[ \tilde{v}_t = \tilde{\Sigma}_p (\Sigma_p^{-1}\overline{v}_t + \beta Q_t^{(ux)}) \]</div>
<p>the G-learning algorithm is like any other RL algorithm and follows the GLI. The algorithm has thus a policy evaluation and a policy improvement process. Namely, equation  <a class="reference internal" href="#equation-action-valueg">(33)</a>and <a class="reference internal" href="#equation-entropyvalue">(35)</a> evaluate the current policy, while <a class="reference internal" href="#equation-updateg">(36)</a>  improves the policy. These processes work in tandem at each time step <span class="math notranslate nohighlight">\(t\)</span>, until the convergence criterium is met. Note, that there is an additional step to calculate the optimal contribution at each time step, because of the budget constraint developed in the subsection of building the target. By simply adding each optimal action <span class="math notranslate nohighlight">\(u_t\)</span> at time step <span class="math notranslate nohighlight">\(t\)</span> we can obtain the optimal cash contribution. A numerical example of the G-learner algorithm for a goal-based retirement plan can be found in <span id="id3">[<a class="reference internal" href="Appendix.html#id18">DHB20</a>]</span>.</p>
</div>
</div>
<div class="section" id="reinforcement-learning-in-optimal-control-applying-the-deep-bsde-method">
<h2>Reinforcement learning in optimal control: applying the Deep BSDE method<a class="headerlink" href="#reinforcement-learning-in-optimal-control-applying-the-deep-bsde-method" title="Permalink to this headline">¶</a></h2>
<p>For the Deep BSDE method to be useful in financial scenarios, a financial model for which the solution is a terminal PDE should first be derived. In this chapter, we introduce such a model called the optimal consumption, investment ,and life insurance model. Before applying the model following ye <span id="id4">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span>, an introduction to the model is given together with various extensions of Ye formulation.</p>
<div class="section" id="an-optimal-consumption-investment-and-life-insurance-model">
<h3>an Optimal consumption, investment and life insurance model<a class="headerlink" href="#an-optimal-consumption-investment-and-life-insurance-model" title="Permalink to this headline">¶</a></h3>
<p>The first person to include uncertain lifetime and life insurance decisions in a discrete life-cycle model was Yaari <span id="id5">[<a class="reference internal" href="Appendix.html#id2">Yaa65</a>]</span>. He explored the model using a utility function without bequest (Fisher Utility function) and a utility function with bequest (Marshall Utility function) in a bounded lifetime. In both cases, he looked at the implications of including life insurance. Although Yaari’s model was revolutionary in the sense that now the uncertainty of life could be modeled, Leung <span id="id6">[<a class="reference internal" href="Appendix.html#id7">Leu94</a>]</span> found that the constraints laid upon the Fisher utility function were not adequate and lead to terminal wealth depletion. Richard <span id="id7">[<a class="reference internal" href="Appendix.html#id6">Ric75</a>]</span> applied the methodology of Merton <span id="id8">[<a class="reference internal" href="Appendix.html#id4">Mer69</a>, <a class="reference internal" href="Appendix.html#id5">Mer75</a>]</span> to the problem setting of Yaari in a continuous time frame. Unfortunately, Richard’s model had one deficiency: The bounded lifetime is incompatible with the dynamic programming approach used in Merton’s model. As an individual approaches his maximal possible lifetime T, he will be inclined to buy an infinite amount of life insurance. To circumvent this Richard used an artificial condition on the terminal value. But due to the recursive nature of dynamic programming, modifying the last value would imply modifying the whole result. Ye <span id="id9">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span>  found a solution to the problem by abandoning the bounded random lifetime and replacing it with a random variable taking values in <span class="math notranslate nohighlight">\([0,\infty)\)</span>. The models that replaced the bounded lifetime, are thereafter called intertemporal models as the models did not consider the whole lifetime of an individual but rather looked at the planning horizon of the consumer.  Note that the general setting of Ye <span id="id10">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span> has a wide range of theoretical variables, while still upholding a flexible approach to different financial settings. On this account, it is a good baseline to confront the issues concerning the current models of financial planning.</p>
<p>After Ye <span id="id11">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span> various models have been proposed which all have given rise to unique solutions to the consumption, investment, and insurance problem. The first unique setting is a model with multiple agents involved. For example,  Bruhn and Steffensen <span id="id12">[<a class="reference internal" href="Appendix.html#id23">BS11</a>]</span> analyzed the optimization problem for couples with correlated lifetimes with their partner nominated as their beneficiary using a copula and common-shock model, while Wei et al.<span id="id13">[<a class="reference internal" href="Appendix.html#id26">WCJW20</a>]</span> studied optimization strategies for a household with economically and probabilistically dependent persons. Another setting is where certain constraints are used to better describe the financial situation of consumers. Namely, Kronborg and Steffensen <span id="id14">[<a class="reference internal" href="Appendix.html#id25">KS15</a>]</span> discussed two constraints. One constraint is a capital constraint on the savings in which savings cannot drop below zero. The other constrain involves a minimum return in savings. A third setting describes models that analyze the financial market and insurance market in a pragmatic environment. A good illustration is the study of Shen and Wei <span id="id15">[<a class="reference internal" href="Appendix.html#id27">SW16</a>]</span>. They incorporate all stochastic processes involved in the investment and insurance market where all randomness is described by a Brownian motion filtration. An interesting body of models is involved in time-inconsistent preferences. In this framework, consumers do not have a time-consistent rate of preference as assumed in the economic literature. There exists rather a divergence between earlier intentions and later choices De-Paz et al. <span id="id16">[<a class="reference internal" href="Appendix.html#id37">DPMSNR14</a>]</span>. This concept is predominantly described in psychology. Specifically, rewards presented closer to the present are discounted proportionally less than rewards further into the future. An application of time-inconsistent preferences in the consumption, investment, and insurance optimization can be found in Chen and Li <span id="id17">[<a class="reference internal" href="Appendix.html#id31">CL20</a>]</span> and De-Paz et al. <span id="id18">[<a class="reference internal" href="Appendix.html#id37">DPMSNR14</a>]</span>.</p>
</div>
<div class="section" id="the-model-specifications">
<h3>The model specifications<a class="headerlink" href="#the-model-specifications" title="Permalink to this headline">¶</a></h3>
<p>In this section, I will set the dynamics for the baseline model in place. The dynamics follow primarily from the paper of Ye <span id="id19">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span>.</p>
<p>Let the state of the economy be represented by a standard Brownian motion <span class="math notranslate nohighlight">\(w(t)\)</span>, the state of the consumer’s wealth be characterized by a finite state multi-dimensional continuous-time Markov chain <span class="math notranslate nohighlight">\(X(t)\)</span> and let the time of death be defined by a non-negative random variable <span class="math notranslate nohighlight">\(\tau\)</span>. All are defined on a given probability space (<span class="math notranslate nohighlight">\(\Omega, \mathcal{F}, \mathbb{P} \)</span>) and <span class="math notranslate nohighlight">\(W(t)\)</span> is independent of <span class="math notranslate nohighlight">\(\tau\)</span>. Let <span class="math notranslate nohighlight">\(T&lt; \infty\)</span> be a fixed planning horizon. This can be seen as the end of the working life for the consumer. <span class="math notranslate nohighlight">\(\mathbb{F} = \{\mathcal{F}_t, t \in [0,T]\}\)</span>, be the P-augmentation of the filtration <span class="math notranslate nohighlight">\(\sigma\)</span>{<span class="math notranslate nohighlight">\(W(s), s&lt;t \}, \forall t \in [0,T]\)</span> , so <span class="math notranslate nohighlight">\(\mathcal{F}_t\)</span> represents the information at time t. The economy consist of a financial market and an insurance market. In the following section I will construct these markets seperately following Ye <span id="id20">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span>.</p>
<p>The financial market consist of a risk-free security <span class="math notranslate nohighlight">\(B(t)\)</span> and a risky security <span class="math notranslate nohighlight">\(S(t)\)</span>, who evolve according to</p>
<div class="math notranslate nohighlight">
\[ \frac{dB(t)}{B(t)}=r(t)dt \]</div>
<div class="math notranslate nohighlight">
\[ \frac{dS(t)}{S(t)}=\mu(t)dt+\sigma(t)dW(t)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\mu, \sigma, r &gt; 0\)</span> are constants and <span class="math notranslate nohighlight">\(\mu(t), r(t), \sigma(t): [0,T] \to R\)</span> are continuous. With <span class="math notranslate nohighlight">\(\sigma(t)\)</span> satisfying <span class="math notranslate nohighlight">\(\sigma^2(t) \ge k, \forall t \in [0,T]\)</span></p>
<p>The random variable <span class="math notranslate nohighlight">\(\tau_d\)</span> needs to be first modeled for the insurance  market. Assume that <span class="math notranslate nohighlight">\(\tau\)</span> has a probability density function <span class="math notranslate nohighlight">\(f(t)\)</span> and probability distribution function given by</p>
<div class="math notranslate nohighlight">
\[ F(t) \triangleq P(\tau &lt; t) = \int_0^t f(u) du \]</div>
<p>Assuming <span class="math notranslate nohighlight">\(\tau\)</span> is independent of the filtration <span class="math notranslate nohighlight">\(\mathbb{F}\)</span></p>
<p>Following on the probability distribution function we can define the survival function as follows</p>
<div class="math notranslate nohighlight">
\[ \bar{F}(t)\triangleq P(\tau \ge t) = 1 -F(t) \]</div>
<p>The hazard function is the  instantaneous death rate for the consumer at time t and is defined by</p>
<div class="math notranslate nohighlight">
\[ \lambda(t) = \lim_{\Delta t\to 0} = \frac{P(t\le\tau &lt; t+\Delta t| \tau \ge t)}{\Delta t} \]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda(t): [0,\infty[ \to R^+\)</span> is a continuous, deterministic function with <span class="math notranslate nohighlight">\(\int_0^\infty \lambda(t) dt = \infty\)</span>.</p>
<p>Subsequently, the survival and probability density function can be characterized by</p>
<div class="math notranslate nohighlight">
\[ \bar{F}(t)= {}_tp_0= e^{-\int_0^t \lambda(u)du} \]</div>
<div class="math notranslate nohighlight">
\[ f(t)=\lambda(t) e^{-\int_0^t\lambda(u)du} \]</div>
<p>With conditional probability described as</p>
<div class="math notranslate nohighlight">
\[ f(s,t) \triangleq \frac{f(s)}{\bar{F}(t)}=\lambda(s) e^{-\int_t^s\lambda(u)dy} \]</div>
<div class="math notranslate nohighlight">
\[ \bar{F}(s,t) = {}_sp_t \triangleq \frac{\bar{F}(s)}{\bar{F}(t)} = e^{-\int_t^s \lambda(u)du} \]</div>
<p>Now that <span class="math notranslate nohighlight">\(\tau\)</span> has been modeled, the life insurance market can be constructed. Let’s assume that life insurance is continuously offered and that it provides coverage for an infinitesimally small period of time. In return, the consumer pays a premium rate p when he enters into a life insurance contract so that he might insure his future income. In compensation, he will receive a total benefit of <span class="math notranslate nohighlight">\(\frac{p}{\eta(t)}\)</span> when he dies at time t. Where <span class="math notranslate nohighlight">\(\eta : [0,T] \to R^+ \)</span> is a continuous, deterministic function.</p>
<p>Both markets are now described and the wealth process <span class="math notranslate nohighlight">\(X(t)\)</span> of the consumer can now be constructed. Given an initial wealth <span class="math notranslate nohighlight">\(x_0\)</span>, the consumer receives a certain amount of income <span class="math notranslate nohighlight">\(i(t)\)</span> <span class="math notranslate nohighlight">\(\forall t \in [0,\tau \wedge T]\)</span> and satisfying <span class="math notranslate nohighlight">\(\int_0^{\tau \wedge T} i(u)du &lt; \infty\)</span>. He needs to choose at time t a certain premium rate <span class="math notranslate nohighlight">\(p(t)\)</span>, a certain consumption rate <span class="math notranslate nohighlight">\(c(t)\)</span> and a certain amount of his wealth <span class="math notranslate nohighlight">\(\theta (t)\)</span> that he invest into the risky asset <span class="math notranslate nohighlight">\(S(t)\)</span>. So given the processes <span class="math notranslate nohighlight">\(\theta\)</span>, c, p and i, there is a wealth process <span class="math notranslate nohighlight">\(X(t)\)</span>  <span class="math notranslate nohighlight">\(\forall t \in [0, \tau \wedge T] \)</span> determined by</p>
<div class="math notranslate nohighlight">
\[ dX(t) = r(t)X(t) + \theta(t)[( \mu(t) - r(t))dt +\sigma(t)dW(t)] -c(t)dt -p(t)dt + i(t)dt,   \quad t \in [0,\tau \wedge T] \]</div>
<p>If <span class="math notranslate nohighlight">\(t=\tau\)</span> then the consumer will receive the insured amount <span class="math notranslate nohighlight">\(\frac{p(t)}{\eta(t)}\)</span>. Given is wealth X(t) at time t his total legacy will be</p>
<div class="math notranslate nohighlight">
\[ Z(t) = X(t) + \frac{p(t)}{\eta(t)} \]</div>
<p>The predicament for the consumer is that he needs to choose the optimal rates for c, p , <span class="math notranslate nohighlight">\(\theta\)</span> from the set <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> , called the set of admissible strategies, defined by</p>
<div class="math notranslate nohighlight">
\[ \mathcal{A}(x) \triangleq  \textrm{set of all possible triplets (c,p,}\theta) \]</div>
<p>such that his expected utility from consumption, from legacy when <span class="math notranslate nohighlight">\(\tau &gt; T\)</span> and from terminal wealth when <span class="math notranslate nohighlight">\(\tau \leq T \)</span>  is maximized.</p>
<div class="math notranslate nohighlight">
\[ V(x) \triangleq \sup_{(c,p,\theta) \in \mathcal{A}(x)} E\left[\int_0^{T \wedge \tau} U(c(s),s)ds + B(Z(\tau),\tau)1_{\{\tau \ge T\}} + L(X(T))1_{\{\tau&gt;T\}}\right] \]</div>
<p>Where <span class="math notranslate nohighlight">\(U(c,t)\)</span> is the utility function of consumption, <span class="math notranslate nohighlight">\(B(Z,t)\)</span> is the utility function of legacy ,and <span class="math notranslate nohighlight">\(L(X)\)</span> is the utility function for the terminal wealth. <span class="math notranslate nohighlight">\(V(x)\)</span> is called the value function and the consumers wants to maximize his value function by choosing the optimal set <span class="math notranslate nohighlight">\(\mathcal{A} = (c,p,\theta)\)</span>. The optimal set <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is found by using the dynamic programming technique described in the following section.</p>
</div>
<div class="section" id="dynamic-programming-principle">
<h3>dynamic programming principle<a class="headerlink" href="#dynamic-programming-principle" title="Permalink to this headline">¶</a></h3>
<p>To solve the consumer’s problem the value function needs to be restated in a dynamic programming form.</p>
<div class="math notranslate nohighlight">
\[J(t, x; c, p, \theta) \triangleq E \left[\int_0^{T \wedge \tau} U(c(s),s)ds + B(Z(\tau),\tau)1_{\{\tau \ge T\}} + L(X(T))1_{\{\tau&gt;T\}}| \tau&gt; t, \mathcal{F}_t \right] \]</div>
<p>The value function becomes</p>
<div class="math notranslate nohighlight">
\[ V(t,x) \triangleq \sup_{\{c,p,\theta\} \in \mathcal{A}(t,x)} J(t, x; c, p, \theta)  \]</div>
<p>Because <span class="math notranslate nohighlight">\(\tau\)</span> is independent of the filtration, the value function can be rewritten as</p>
<div class="math notranslate nohighlight">
\[ E \left[\int_0^T  \bar{F}(s,t)U(c(s),s) + f(s,t)B(Z(\tau),\tau) ds  + \bar{F}(T,t)L(X(T))| \mathcal{F}_t \right]\]</div>
<p>The optimization problem is now converted from a random closing time point to a fixed closing time point. The mortality rate can also be seen as a discounting function for the consumer as he would value the utility on the probability of survival.</p>
<p>Following the dynamic programming principle, we can rewrite this equation as the value function at time s plus the value created from time step t to time step s. This enables us to view the optimization problem into a time step setting, giving us the incremental value gained at each point in time.</p>
<div class="math notranslate nohighlight">
\[ V(t,x) = \sup_{\{c,p,\theta\} \in \mathcal{A}(t,x)} E\left[e^{-\int_t^s\lambda(v)dv}V(s,X(s)) + \int_t^s f(s,t)B(Z(s),s) + \bar{F}(s,t)U(c(s),s)ds|\mathcal{F}_t\right] \]</div>
<p>The Hamiltonian-Jacobi-bellman (HJB) equation can be derived from the dynamic programming principle and is as follows</p>
<div class="math notranslate nohighlight" id="equation-bell2">
<span class="eqno">(37)<a class="headerlink" href="#equation-bell2" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{cases} 
V_t(t,x) -\lambda V(t,x) + \sup_{(c,p,\theta)} \Psi(t,x;c,p,\theta)  = 0 \\ V(T,x) = L(x)  
\end{cases}\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split} \Psi(t,x; c,p,\theta) \triangleq r(t)x + \theta(\mu(t) -r(t)) + i(t) -c -p)V_x(t,x) + \\ \frac{1}{2}\sigma^2(t)\theta^2V_{xx}(t,x) + \lambda(t)B(x+ p/\eta(t),t) + U(c,t) \end{split}\]</div>
<p>Proofs for deriving the HJB equation, dynamic programming principle ,and converting from a random closing time point to a fixed closing time point can be found in Ye <span id="id21">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span></p>
<p>A strategy is optimal if</p>
<div class="amsmath math notranslate nohighlight" id="equation-e2fa0e77-6bc8-4303-8136-15183d8a53df">
<span class="eqno">(38)<a class="headerlink" href="#equation-e2fa0e77-6bc8-4303-8136-15183d8a53df" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \begin{split}
        0 &amp; =V_t(t,x) -\lambda(t)V(t,x) + \sup_{c,p,\theta}(t,x;c,p,\theta)  
\\
        0 &amp; = V_t(t,x) -\lambda(t)V(t,x) + (r(t)x+ i(t))V_x + \sup_c\{U(c,t)-cV_x\} 
\\ 
        &amp; + \sup_p\{\lambda(t)B(x + p/\eta(t),t) - pV_x\} + \sup_\theta \{ \frac{1}{2}\sigma^2(t)V_{xx}(t,x)\theta^2 +(\mu(t) - r(t))V_x(t,x)\theta\} 
    \end{split}
\end{equation}\]</div>
<p>The first order conditions for regular interior maximum are</p>
<div class="math notranslate nohighlight" id="equation-cons-cond">
<span class="eqno">(39)<a class="headerlink" href="#equation-cons-cond" title="Permalink to this equation">¶</a></span>\[\begin{split}    \sup_c  \{ U(c,t) - cV_x\} &amp; = \Psi_c(t,x;c^*,p^*,\theta^*)  
\\ 
    \rightarrow  0 &amp; = -V_x(t,x) + U_c(c*,t) 
\\
    \sup_p\{\lambda(t)B(x + p/\eta(t),t) - pV_x\} &amp; = \Psi_p(t,x;c^*,p^*,\theta^*)
\\ 
    \rightarrow 0 &amp; = -V_x(t,x) + \frac{\lambda(t)}{\eta{t}}B_Z(x + p^*/\eta(t),t)
\\
    \sup_\theta \{ \frac{1}{2}\sigma^2(t)V_{xx}(t,x)\theta^2 +(\mu(t) - r(t))V_x(t,x)\theta\} &amp; = \Psi_\theta(t,x;c^*,p^*,\theta^*)
\\
     \rightarrow 0 &amp; = (\mu(t) -r(t))V_x(t,x) + \sigma^2(t)\theta^*V_{xx}(t,x)\end{split}\]</div>
<p>The second order conditions are</p>
<div class="math notranslate nohighlight">
\[ \Psi_{cc}, \Psi_{pp}, \Psi_{\theta \theta} &lt; 0 \]</div>
<p>This optimal control problem has been solved analytically by Ye <span id="id22">[<a class="reference internal" href="Appendix.html#id9">Ye06</a>]</span> for the Constant Relative Risk Aversion utility function. To solve <a class="reference internal" href="#equation-bell2">(37)</a> the  BSDE method can be used. The Deep BSDE method was the first deep learning-based numerical algorithm to solve general nonlinear parabolic PDEs in high dimensions.</p>
<p>remember the general form of PDEs which the Deep BSDE method solves:</p>
<div class="math notranslate nohighlight" id="equation-gen-form2">
<span class="eqno">(40)<a class="headerlink" href="#equation-gen-form2" title="Permalink to this equation">¶</a></span>\[\frac{\partial u}{\partial t} + \frac{1}{2} Tr(\sigma \sigma^T (Hess_xu) + \Delta u(t,x)  \mu(t,x) + f(t,x,u, \sigma^T(\Delta_x u)) = 0 \]</div>
<p>With some terminal condition <span class="math notranslate nohighlight">\(u(T,x) = g(x)\)</span>. <a class="reference internal" href="#equation-bell2">(37)</a> can thus be reformulated in the general form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\underbrace{V_t(t,x)}_{\frac{\partial u}{\partial t}} + \underbrace{\frac{1}{2}\sigma(t)^2\theta^2V_{xx}(t,x)}_{\frac{1}{2}Tr(\sigma \sigma^T(Hess_xu(t,x)))} + \underbrace{(r(t) x + \theta(\mu(t) -r(t)) + i(t) -c -p)V_x(t,x)}_{\Delta u(t,x)\mu (t,x)} \\ + \underbrace{\lambda(t)B(x+\frac{p}{\eta(t)},t) + U(t,x) - \lambda(t)V(t,x)}_{f(t,x,u(t,x), \sigma^T(t,x)\Delta u(t,x))}\end{split}\]</div>
<p>The BSDE method can thus be applied.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Reinforcement_learning.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Reinforcement Learning</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Discussion.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">discussion</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Ignace Decocq<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>