%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Future Financial Planning Tools for Consumers}
\date{Aug 08, 2021}
\release{}
\author{Ignace Decocq}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{abstract::doc}}


\sphinxAtStartPar
Financial planning tools can be a double edge sword for consumers as most tools of today lack sufficient theoretical background or are used for commercial purposes. To tackle the shortcomings of today’s financial planning tools, we delve into a Machine Learning subfield called Reinforcement Learning. Reinforcement Learning has seen great advancement in the past decade and is becoming a comprehensive field in which financial tools can be better tailored for the consumer. More specifically, the recent coöperation between reinforcement learning and planning domains like optimal control enable algorithms to not just learn the environment, but also to be able to plan ahead. Given the fact that these algorithms can be widely generic and can thus be employed in a tailored financial environment for the consumer, makes the Reinforcement literature an interesting field for future financial planning tools. Inspired by this idea, the general theory of Reinforcement learning is introduced together with the most fundamental algorithms. Furthermore, a closer look will be given at the dimensions of a model\sphinxhyphen{}based Reinforcement Learning algorithm. Thereafter, The current challenges in Reinforcement Learning are discussed together with Deep Reinforcement Learning, which tackles the biggest hurdle in Reinforcement Learning called the Curse of Dimensionality. Next, the possible implications of Reinforcement Learning for financial planning are considered. Finally, two financial applications are introduced.


\chapter{Introduction}
\label{\detokenize{Introduction:introduction}}\label{\detokenize{Introduction::doc}}
\sphinxAtStartPar
The financial decisions that consumers need to make in their present lifetime, become increasingly more complex. A good example of this phenomenon is the shift from defined benefits to defined contributions in which consumers take on greater individual responsibility and risks. The evolution in the abstruseness of financial products has become challenging for consumers who possess low financial knowledge and limiting numeracy skills {[}\hyperlink{cite.Discussion:id14}{BFH17}{]}. Combined with uncertainty about the future, the consumer is necessitated to be more aware of his financial well\sphinxhyphen{}being than ever before. Looking back into the past, Porteba et al, {[}\hyperlink{cite.Discussion:id16}{PVW11}{]} conducted an examination of preparedness in retirement for Children of Depression, War Baby, and the Early Baby Boomer in the Health and Retirement Study and Asset and Health Dynamics Among the Oldest Old cohorts. They found that 46.1 percent die with less than 10 000 dollars. With this amount of assets, they would not have the capacity to pay for unexpected events and one might wonder if it is adequate asset levels for retirement. Furthermore, saving behavior has not kept pace with increasing life expectation and the expected prolonged lifespan of the coming generations are unprecedented {[}\hyperlink{cite.Discussion:id17}{Her11}{]}. All these elements give a painstakingly clear picture that having a vital understanding of one’s financial situation has become one of the greatest challenges in life.

\sphinxAtStartPar
To combat these difficulties, consumers require additional undertakings in planning for their future prosperity. One of the approaches to tackle this issue, is by using financial planning tools. These tools give the consumer the capability to estimate complex intertemporal calculations {[}\hyperlink{cite.Discussion:id13}{BDTS20}{]}. They also enhance financial behavior, increase household wealth accumulation and they are a complement to other planning aid like a financial advisor {[}\hyperlink{cite.Discussion:id14}{BFH17}{]}. Although financial planning tools can greatly benefit consumers, it can also be a double\sphinxhyphen{}edged sword. More specifically, when consumers are misinformed about the capabilities of the tool, or when the design of the tool is inadequate, the consumer can be given sub\sphinxhyphen{}optimal advice or even misleading advice {[}\hyperlink{cite.Discussion:id15}{DMBE18}{]}. Insufficiencies in design can arise when not all essential input variables are included, not all risks are considered, and when accuracy is sacrificed for the ease of use {[}\hyperlink{cite.Discussion:id13}{BDTS20}{]}. On top of that, there are wide variations in results because of the various methodology and assumptions used in the models {[}\hyperlink{cite.Discussion:id15}{DMBE18}{]}. For example, assumptions based on inflation and the use of different financial products have a large impact on the results. On the side of the consumer, the possibility of misunderstanding the implications of the results due to a lack of financial knowledge, is a matter of great concern in the eyes of financial educators {[}\hyperlink{cite.Discussion:id13}{BDTS20}{]}. Clarifying the results is therefore an essential part of making models operational. To improve upon these deficiencies, Dorman et al., {[}\hyperlink{cite.Discussion:id15}{DMBE18}{]} found that when the models handle additional theoretical variables, the accuracy will improve. Besides, they found that the consumer requires unique solutions that better capture their financial situation. Meaning planning tools need to be more flexible. They should be able to operate in different financial settings and have the ability to look at the impact of changes in input variables. To address the variability in results and the adaptability of models to different settings, this paper will look at reinforcement learning techniques in an intertemporal setting. Reinforcement Learning enables an increase in the flexibility of the model while keeping fundamental theoritical aspects like Optimal Control Theory at its core.

\sphinxAtStartPar
For the remainder of the paper, the general theory of Reinforcement Learning (RL) will first be introduced. Then, some challenges are discussed together with Deep Reinforcement Learning. Next, The possible implications of RL for financial planning are considered. Finally, two financial applications are reviewed.


\chapter{Reinforcement Learning}
\label{\detokenize{Reinforcement_learning:reinforcement-learning}}\label{\detokenize{Reinforcement_learning::doc}}
\sphinxAtStartPar
Supervised and unsupervised learning are the two most widely studied and researched branches of Machine Learning (ML). Besides these two, there is also a third subcategory in ML called Reinforcement Learning (RL). The three branches have fundamental differences between each other. Supervised learning for example is designed to learn from a training set of labeled data, where each element of the training set describes a certain situation and is linked to a label/action the supervisor has provided {[}\hyperlink{cite.Discussion:id60}{Ham18}{]}. On the other hand, RL is a method in which the machine tries to map situations to actions by maximizing a reward signal {[}\hyperlink{cite.Discussion:id63}{ADBB17}{]}. The two methods are fundamentally different from each other in the fact that in RL there is no supervisor which provides the label/action the machine needs to take, rather there is a reward system set up from which the machine can learn the correct action/label {[}\hyperlink{cite.Discussion:id60}{Ham18}{]}. contrarily to supervised learning, unsupervised learning tries to find hidden structures within an unlabeled dataset. This might seem similar to RL as both methods work with unlabeled datasets, but RL tries to maximize a reward signal instead of finding only hidden structures in the data {[}\hyperlink{cite.Discussion:id63}{ADBB17}{]}. Unsupervised learning on the other hand learns about how the data is distributed {[}\hyperlink{cite.Discussion:id53}{SBLL19}{]}.

\sphinxAtStartPar
RL finds its roots in multiple research fields. Each of these fields contributes to the RL in its own unique way (see \hyperref[\detokenize{Reinforcement_learning:tree-fig}]{Figure \ref{\detokenize{Reinforcement_learning:tree-fig}}}) {[}\hyperlink{cite.Discussion:id60}{Ham18}{]}. For example,  RL is similar to natural learning processes where the learning method is by experiencing many failures and successes. Therefore psychologists have used RL to mimic psychological processes when an organism makes choices based on experienced rewards/punishments {[}\hyperlink{cite.Discussion:id69}{EWC21}{]}. While psychologists are mimicking psychological processes, neuroscientists are using RL to focus on a well\sphinxhyphen{}defined network of regions of the brain that implement value learning {[}\hyperlink{cite.Discussion:id69}{EWC21}{]}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen,height=250\sphinxpxdimen]{{tree}.png}
\caption{research fields involved in reinforcement learning}\label{\detokenize{Reinforcement_learning:tree-fig}}\end{figure}


\section{Finite Markov Decision Processes}
\label{\detokenize{Reinforcement_learning:finite-markov-decision-processes}}
\sphinxAtStartPar
RL can be represented in finite Markov decision processes (MDPs), which are classical formalizations of sequential decision making. More specifically, MPDs give rise to a structure in which delayed rewards can be balanced with immediate rewards {[}\hyperlink{cite.Discussion:id70}{SB18}{]}. It also enables a straightforward framing of learning from interaction to achieve a goal {[}{]}. In its simplest form, RL works with an Agent\sphinxhyphen{}Environment Interface. The agent is exposed to some representation of the environment’s state \(S_t \in \mathrm{S}\). From this representation the agent needs to chose an action \( A_t \in \mathcal{A}(s)\), which will result in a numerical reward \(R_{t+1} \in 	\mathbb{R} \) and a new state \(S_{t+1}\) (see \hyperref[\detokenize{Reinforcement_learning:standard-model-fig}]{Figure \ref{\detokenize{Reinforcement_learning:standard-model-fig}}})  {[}\hyperlink{cite.Discussion:id70}{SB18}{]}. The goal for the agent is to learn a mapping from states to action called a policy \(\pi\) that maximizes the expected rewards:
\begin{equation*}
\begin{split} \pi^* = argmax_{\pi} E[R|\pi] \end{split}
\end{equation*}
\sphinxAtStartPar
If the MPDs are finite and discrete, the sets of states, actions ,and rewards (\(S\), \(A\) , and \(R\)) all have a finite number of elements. The agent\sphinxhyphen{}environment interaction can then be subdivided into episode {[}\hyperlink{cite.Discussion:id63}{ADBB17}{]}.  The agent’s goal is to maximize the expected discounted cumulative return in the episode {[}\hyperlink{cite.Discussion:id72}{FranccoisLHI+18}{]}:
\begin{equation}\label{equation:Reinforcement_learning:return}
\begin{split}G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... + \gamma^{T-t-1}R_T = \sum_{k=0}^T \gamma^k R_{t+k+1}\end{split}
\end{equation}
\sphinxAtStartPar
Where T indicates the terminal state and \(\gamma\) is the discount rate. The terminal state \(S_T\) is often followed by a reset to a starting state or sample from a starting distribution of states {[}\hyperlink{cite.Discussion:id72}{FranccoisLHI+18}{]}. An episode ends once the reset has occurred. The discount rate represents the present value of future rewards. If \(\gamma = 0\), the agent is myopic and is only concerned with maximizing the immediate rewards. The agent can consequently be considered greedy {[}\hyperlink{cite.Discussion:id53}{SBLL19}{]}.

\sphinxAtStartPar
The returns can be rewritten in a dynamic programming approach:
\begin{equation*}
\begin{split} G_t = R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ... + \gamma^{T-t-2}R_T) \end{split}
\end{equation*}\begin{equation*}
\begin{split} G_t = R_{t+1} + \gamma G_{t+1}\end{split}
\end{equation*}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen,height=300\sphinxpxdimen]{{standard_model}.png}
\caption{standard model reinforcement learning}\label{\detokenize{Reinforcement_learning:standard-model-fig}}\end{figure}

\sphinxAtStartPar
A key concept of MPDs is the Markov property: Only the current state affects the next state {[}\hyperlink{cite.Discussion:id72}{FranccoisLHI+18}{]}. The random variables (RV) \(R_t\) and \(S_t\) have then well defined discrete transition probability distributions dependent only on the previous state and action:
\begin{equation*}
\begin{split} p(s', t| s, a) = Pr(S_t = s', R_t = r | S_{t-1} = s, A_{t-1}=a) \end{split}
\end{equation*}
\sphinxAtStartPar
For all \(s', s \in \mathrm{S} , r \in 	\mathbb{R}, a \in \mathrm{A}(s) \). The probability of each element in the sets \(S\) and \(R\) completely characterizes the environment {[}\hyperlink{cite.Discussion:id70}{SB18}{]}. This is an unrealistic assumption to make, and several algorithms relax the Markov property. The Partial Observable Markov Decision Process (POMDP) algorithm, for example, maintains a belief over the current state given the previous belief state, the action taken, and the current observation {[}\hyperlink{cite.Discussion:id63}{ADBB17}{]}.  Once \(p\) is known, the environment is fully described and functions like a transition function \(T : D \times A \to p(S)\) and a reward function \(R: S \times A \times S \to \mathbb{R}\) can be deducted {[}\hyperlink{cite.Discussion:id70}{SB18}{]}.

\sphinxAtStartPar
Most algorithms in RL use a value function to estimate the value of a given state for the agent. Value functions are defined by the policy \(\pi\) the agent has decided to take. As mentioned previously, \(\pi\) is the mapping of states to probabilities of selecting an action. The value function \(v_{\pi}(s)\) in a state \(s\) following a policy \(\pi\) is as follows:
\begin{equation}\label{equation:Reinforcement_learning:value}
\begin{split}v_{\pi}(s) = E_{\pi}[G_t | S_t = s] = E_{\pi}[\sum_{k=0}^T \gamma^kR_{t+k+1} | S_t=s]\end{split}
\end{equation}
\sphinxAtStartPar
This can also be rewritten in a dynamic programming approach:
\begin{equation*}
\begin{split}v_{\pi}(s) = E_{\pi}[G_t | S_t = s] \end{split}
\end{equation*}\begin{equation*}
\begin{split} \hspace{2.4cm}= E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] \end{split}
\end{equation*}\begin{equation*}
\begin{split}  \hspace{6cm} = \sum_a \pi(a|s) \sum_{s'} \sum_r p(s', r|s,a)[r + \gamma E_{pi}[G_{t+1} | S_{t+1} = s'] \end{split}
\end{equation*}\begin{equation}\label{equation:Reinforcement_learning:BELL}
\begin{split}\hspace{5.8cm}= \sum_a \pi(a|s) \sum_{s', r}p(s', r|s,a)[r + \gamma v_{\pi}(s')| S_{t+1} = s'] \end{split}
\end{equation}
\sphinxAtStartPar
The formula is called the Bellman equation of \(v_{\pi}\). It describes the relationship between the value of a state and the values of its successor states given a certain policy \(\pi\). The relation can also be represented by a backup diagram (see \hyperref[\detokenize{Reinforcement_learning:backup-diagram-fig}]{Figure \ref{\detokenize{Reinforcement_learning:backup-diagram-fig}}}). If \(v_{\pi}(s)\) is the value of a given state, then \(q_{\pi}(s,a)\) is the value of a given action of that state:
\begin{equation}\label{equation:Reinforcement_learning:state-action}
\begin{split} q_{\pi}(s,a) = E_{\pi}[G_t | S_t = s, A_t = a] = E_{\pi}[\sum_{k=0}^T \gamma^kR_{t+k+1} | S_t=s, A_t = a] \end{split}
\end{equation}
\sphinxAtStartPar
This can be seen in the backup diagram as starting from the black dot and computing the subsequential value thereafter. \(q_{\pi}(s,a)\) is also called the action\sphinxhyphen{}value function as it describes each value of an action for each state.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen,height=250\sphinxpxdimen]{{backup_diagram}.png}
\caption{General backup diagram}\label{\detokenize{Reinforcement_learning:backup-diagram-fig}}\end{figure}

\sphinxAtStartPar
For the agent, it is important to find the optimal policy which maximizes the expected cumulative rewards. The optimal policy \(\pi_*\) is the policy for which \(v_{\pi_*}(s) > v_{\pi}(s)\) for all \(s \in S\). An optimal policy also has the same action\sphinxhyphen{}value function \(q_*(s,a)\) for all \(s \in S\) and \(a \in A\). The optimal policy does not depend solely on one policy and can encompass multiple policies. It is thus not policy dependent:
\begin{equation*}
\begin{split} v_*(s) = max_{a \in A(s)} q_{\pi_*}(s,a) \end{split}
\end{equation*}\begin{equation*}
\begin{split} = max_{a} E_{\pi_*}[G_t | S_t=s, A_t=a] \end{split}
\end{equation*}\begin{equation*}
\begin{split} = max_{a} E_{\pi_*}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \end{split}
\end{equation*}\begin{equation*}
\begin{split} = max_{a} E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a] \end{split}
\end{equation*}
\sphinxAtStartPar
Once \(v_*(s)\) is found, you  need to apply a greedy algorithm as the optimal value function already takes into account the long\sphinxhyphen{}term consequences of choosing that action. Finding \(q_*(s,a)\), makes things even easier, as the action\sphinxhyphen{}value function caches the result of all one\sphinxhyphen{}step\sphinxhyphen{}ahead searches.

\sphinxAtStartPar
Solving the Bellman equation of the value function or the action\sphinxhyphen{}value function such that we know all possibilities with their probabilities and rewards is in most practical cases impossible. Typical due to three main factors {[}\hyperlink{cite.Discussion:id70}{SB18}{]}. The first problem is obtaining full knowledge of the dynamics of the environment. The second factor is the computational resources to complete the calculation. The last factor is that the states need to have the Markov property. To circumvent these obstacles, RL tries to approximate the Bellman optimality equation using various methods. In the next chapter, a brief layout of these methods is discussed, focussing on the methods applicable for financial planning.


\section{Generelized Policy Iteration, Model\sphinxhyphen{}based RL and Model\sphinxhyphen{}free RL}
\label{\detokenize{Reinforcement_learning:generelized-policy-iteration-model-based-rl-and-model-free-rl}}
\sphinxAtStartPar
A general theory in finding the optimal policy \(\pi_*\) is called Generalized Policy Iteration (GLI). This method is applied to almost all RL algorithms. The main idea behind GLI is that there is a process that evaluates the value function of the current policy \(\pi\) called policy evaluation and a process that improves the current value function called policy improvement {[}\hyperlink{cite.Discussion:id70}{SB18}{]}, {[}\hyperlink{cite.Discussion:id42}{VOW12}{]}. To find the optimal policy these two processes work in tandem with each other as seen in \hyperref[\detokenize{Reinforcement_learning:gpi-fig}]{Figure \ref{\detokenize{Reinforcement_learning:gpi-fig}}} {[}\hyperlink{cite.Discussion:id42}{VOW12}{]}. Counterintuitively, these processes also work in a conflicting manner as policy improvement makes the policy incorrect and it is thus no longer the same policy {[}\hyperlink{cite.Discussion:id70}{SB18}{]},  {[}\hyperlink{cite.Discussion:id40}{BHB+20}{]}. While policy evaluations create a consistent policy and thus the policy no longer improves upon itself. This idea runs in parallel with the balance between exploration and exploitation in RL.  If the focus lies more on exploration, the agent frequently tries to find states which improve the value function. However, putting more emphasis on exploration is a costly setting as the agent will more frequently choose suboptimal policies to explore the state space. If exploitation is prioritized, the agent will take a long time to find the optimal policy as the agent is likely not to explore new states to improve the policy {[}\hyperlink{cite.Discussion:id42}{VOW12}{]}.  An \(\varepsilon\)\sphinxhyphen{}greedy algorithm is a good example of an algorithm where the balance between exploration and exploitation is important.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=300\sphinxpxdimen]{{GPI}.png}
\caption{Generalized policy iteration}\label{\detokenize{Reinforcement_learning:gpi-fig}}\end{figure}

\sphinxAtStartPar
Reinforcement Learning can be subdivided between model\sphinxhyphen{}based RL and model\sphinxhyphen{}free RL. In model\sphinxhyphen{}free RL the dynamics of the environment are not known. \(\pi_*\) is found by purely interacting with the environment. Meaning that these algorithms do not use transition probability distribution and reward function related to MDP {[}\hyperlink{cite.Discussion:id72}{FranccoisLHI+18}{]}. Moreover, model\sphinxhyphen{}free RL has irreversible access to the environment. Meaning the algorithm has to move forward after an action is taken {[}\hyperlink{cite.Discussion:id71}{MBJ20b}{]}. Model\sphinxhyphen{}based RL on the other hand has reversible access to the environment because they can revert the model and make another trail from the same state {[}\hyperlink{cite.Discussion:id7}{MBJ20a}{]}. Good examples of model\sphinxhyphen{}free RL techniques are the Q\sphinxhyphen{}learning and Sarsa. They tend to be used on a variety of tasks, like playing video games to learning complicated locomotion skills {[}\hyperlink{cite.Discussion:id70}{SB18}{]}. Model\sphinxhyphen{}free RL lay at the foundation of RL and are the first algorithms to be applied in RL were model\sphinxhyphen{}free RL techniques. On the other hand, model\sphinxhyphen{}based RL is developed independently and in parallel with planning methods like optimal control and the search community as they both solve the same problem but differ in the approach {[}\hyperlink{cite.Discussion:id38}{WZZ19}{]}. Most algorithms in model\sphinxhyphen{}based RL have a model which describes the dynamics of the environment. They sample from that model to then improve a learned value or policy function {[}\hyperlink{cite.Discussion:id7}{MBJ20a}{]} (see \hyperref[\detokenize{Reinforcement_learning:model-based-rl}]{Figure \ref{\detokenize{Reinforcement_learning:model-based-rl}}}). This enables the agent to think in advance and as it were plan for possible actions. Model\sphinxhyphen{}based reinforcement learning finds thus large similarities with the Planning literature and as a result, a lot of cross\sphinxhyphen{}breeding between the two is happening. For example, an extension of the POMP algorithm called Partially Observable Multi\sphinxhyphen{}Heuristic Dynamic Programming (POMHDP) is based on recent progress from the search community {[}\hyperlink{cite.Discussion:id10}{KSL19}{]}. A hybrid version of the two approaches in which the model is learned through interaction with the environment, has also been widely applied. The imagination\sphinxhyphen{}augmented agents (12A) for example combines model\sphinxhyphen{}based and model\sphinxhyphen{}free aspects by employing the predictions as an additional context in a deep policy network \{cite\}``moerland2020model.  In the next subsection, three fundamental algorithms in RL are discussed which will enable us to better capture the dimensions and challenges of RL algorithms.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen,height=200\sphinxpxdimen]{{model_based_RL}.png}
\caption{Model\sphinxhyphen{}based Reinforcement Learning}\label{\detokenize{Reinforcement_learning:model-based-rl}}\end{figure}


\subsection{Dynamic Programming, Monte Carlo Methods and Temporal\sphinxhyphen{}Difference Learning}
\label{\detokenize{Reinforcement_learning:dynamic-programming-monte-carlo-methods-and-temporal-difference-learning}}
\sphinxAtStartPar
Dynamic Programming (DP) is known for two algorithms in RL: value iteration (VI) and policy iteration (PI). For both methods, the dynamics of the environment need to be completely known and they ,therefore, fall under model\sphinxhyphen{}based RL. The two algorithms also use a discrete time, state and action MDP as they are iterative procedures. The PI can be subdivided into three steps: initialize, policy evaluation and policy improvement {[}\hyperlink{cite.Discussion:id42}{VOW12}{]}. The first step is to initialize the value function \(v_{\pi}\) by choosing an arbitrary policy \(\pi\). The following step is to evaluate the function successively by updating the Bellman equation \eqref{equation:Reinforcement_learning:BELL}. Updating on the Bellman equation is also called the expected update as the equation is updated using the whole state space instead of a sample of the state space. One update is also called a sweep as the update sweeps through the state space. Once that the value function \(v_{\pi}\) is updated, we know how good it is to follow the current policy. The next step is to deviate from the policy trajectory and choose a different action \(a\) in state \(s\) to find a more optimal policy value. We compute the new \(\pi '\) and compare it to the old policy. The new policy is accepted if \(\pi '(s) > \pi(s)\). This process is repeated until a convergence criterion is met {[}\hyperlink{cite.Discussion:id41}{PRD96}{]}. The complete algorithm can be found in the appendix. VI combines the policy evaluation with the policy improvement by truncating the sweep with one update of each state. It effectively combines the policy evaluation and policy evaluation in one sweep (see appendix for the algorithm) {[}\hyperlink{cite.Discussion:id41}{PRD96}{]} . PI and VI are the foundation of DP and numerous adaptions have been made to these algorithms. Although these algorithms do not have a wide application in many fields, their essential in describing what an RL algorithm effectively tries to approximate {[}\hyperlink{cite.Discussion:id70}{SB18}{]}.

\sphinxAtStartPar
The Monte Carlo (MC) methods do not assume full knowledge of the dynamics of the environment and are thus considered model\sphinxhyphen{}free RL techniques. They only require a sample sequence of states, actions and rewards from the interaction of an environment. Technically, a model is still required which generates sample transitions, but the complete probability distribution \(p\) of the dynamic system is not necessary. The idea behind almost all MC methods is that the agent learns the optimal policy by averaging the sample returns of a policy \(\pi\) {[}\hyperlink{cite.Discussion:id63}{ADBB17}{]}. They can therefore not learn on an online basis as after each episode they need to average their returns. Another difference between the two methods is that the MC method does not bootstrap like DP {[}\hyperlink{cite.Discussion:id52}{MJ20}{]} . Meaning, each state has an independent estimate. Note that Monte Carlo methods create a nonstationary problem as each action taken at a state depends on the previous states. MC methods can either estimate \(a\) state value \sphinxcode{\sphinxupquote{state\sphinxhyphen{}value}} or estimate the value of a state\sphinxhyphen{}action pairs \eqref{equation:Reinforcement_learning:value} (recall that the state\sphinxhyphen{}action values are the value of an action given a state). If state values are estimated, a model is required as it needs to be able to look ahead one step and choose the action which leads to the best reward and next state. With action value estimation you already estimated the value of the action and no model needs to be taken into account.  Monte Carlo methods also use a term called visits. A visit is when a state or state\sphinxhyphen{}action pair is in the sample path. Multiple visits to a state are possible in an episode. Two general Monte Carlo Methods can be deducted from visits. The every\sphinxhyphen{}visit MC methods and the first\sphinxhyphen{}visit MC methods. The every\sphinxhyphen{}visit MC methods estimate the value of a state as the average of the returns that have followed all visits to it. The first visit method only looks at the first visit of that state to estimate the average returns {[}\hyperlink{cite.Discussion:id42}{VOW12}{]}. The biggest hurdle in MC methods is that most state\sphinxhyphen{}action pairs might never be visited in the sample.

\sphinxAtStartPar
To overcome this problem multiple solutions have been explored. The naïve solution to this problem is called the exploring starts. Here, the idea is to allocate to each action in each state a nonzero probability at the start of the process. Although this is not possible in a practical setting where we truly want to interact with an environment, it enables us to improve the policy by making it greedy with respect to the current value function. As each state has a certain probability to explore, it will eventually explore the complete state space. If then an infinite number of episodes are taken, the policy improvement theory states that the policy \(\pi\) will convergence to the optimal policy \(\pi_*\) given the exploring starts {[}\hyperlink{cite.Discussion:id61}{Dol10}{]}. Two other possibilities are applied in the field to solve this problem: on\sphinxhyphen{}policy methods and off\sphinxhyphen{}policy methods {[}\hyperlink{cite.Discussion:id70}{SB18}{]}. On\sphinxhyphen{}policy methods attempt to improve on the current policy. This is also called a soft policy as \(\pi(a|s) > 0\) for all \(s \in S\) and all \( a \in A(s)\), but shifts eventually to the deterministic optimal policy. One of these on\sphinxhyphen{}policy methods is called an \(\varepsilon\)\sphinxhyphen{}greedy policy. The \(\varepsilon\)\sphinxhyphen{}greedy policy uses with probability \(\varepsilon\) a random action instead of the greedy action. \(\varepsilon\) is a fine\sphinxhyphen{}tuning parameter as it sets the balance between exploration and exploitation. The \(\varepsilon\)\sphinxhyphen{}soft policy is thus also a compromised solution as one cannot exploit and explore at the same rate. This is reelected by the fact that the \(\varepsilon\)\sphinxhyphen{}greedy policy is the best policy only among the \(\varepsilon\)\sphinxhyphen{}soft policies. The pseudocode of on\sphinxhyphen{}policy first visit MC for \(\varepsilon\)\sphinxhyphen{}soft policies algorithm can be found in the appendix. Lastly, the off\sphinxhyphen{}policy methods can be applied to overcome both the unrealistic exploring starts and the compromise needed in the on\sphinxhyphen{}policy methods. Off policy methods solve the exploration versus exploitation dilemma by considering two separate policies {[}\hyperlink{cite.Discussion:id42}{VOW12}{]}. one policy, called the target policy \(\pi\), is being learned to become the optimal policy and another policy, called the behavior policy \(b\), generates the behavior to explore the state space. In an off\sphinxhyphen{}policy method there needs to be coverage between the behavior policy and the target policy to transfer the exploration done by behavior policy \(b\) to the target policy \(\pi\). Meaning, every action taken under \(\pi\) also needs to be taken occasionally under \(b\). Consequently, the behavior policy needs to be stochastic in states where it deviates from the target policy. Complete coverage would imply that the behavior policy and the target policy are the same. The off\sphinxhyphen{}policy method would then become an on\sphinxhyphen{}policy method. The on\sphinxhyphen{}policy method can thus be viewed as a special case of off\sphinxhyphen{}policy in which the two policies are the same. Most off\sphinxhyphen{}policy methods use importance sampling to estimate expected values under one distribution given samples from another. Importance sampling uses the ratio of returns according to the relative probability of the trajectories of the target and behavior policies to learn the optimal policy {[}\hyperlink{cite.Discussion:id62}{MVHS14}{]}:
\begin{equation*}
\begin{split} p_{t:T-1} = \frac{\prod^{T-1}_{k=t} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}\end{split}
\end{equation*}
\sphinxAtStartPar
The formula is called the importance\sphinxhyphen{}sampling ratio. Note that the ratio only depends on the two policies and the sequence, not on the MDP. The importance\sphinxhyphen{}sampling ratio effectively transforms the expectations of \(v_b(s)\) to have the right expectation.  Now, we can effectively estimate \(v_{\pi}(s)\):
\begin{equation*}
\begin{split}V_{\pi}(s) = \frac{\sum_{t\in J(s)} p_{t:T-1}G_t}{|J(s)|}\end{split}
\end{equation*}
\sphinxAtStartPar
Where \(J(s)\) are all timesteps in which state \(s\) is visited for an every\sphinxhyphen{}visit MC method and for a first\sphinxhyphen{}visit MC method \(J(s)\) are all timesteps that were first visits to state \(s\). An alternative to importance sampling is weighted importance sampling in which a weighted average is used:
\begin{equation*}
\begin{split} V(s) = \frac{\sum_{t \in J(s)}p_{t:T-1}G_t}{\sum_{t \in J(s)}p_{t:T-1}} \end{split}
\end{equation*}
\sphinxAtStartPar
The advantage of using a weighted importance sampling is a reduced variance as the variance is bounded when a weighting scheme is applied. The downside of this technique is that it increases the bias as the expectation deviates from the expectation of the target policy {[}\hyperlink{cite.Discussion:id62}{MVHS14}{]}.

\sphinxAtStartPar
The last general method to talk about is temporal\sphinxhyphen{}difference learning (TD). Temporal difference learning is a hybrid between Monte Carlo methods and Dynamic Programming. As DP, it updates estimates based on other learned estimates, not waiting on the final outcome, but it can learn directly from experience without a model of the environment like MC methods {[}\hyperlink{cite.Discussion:id70}{SB18}{]}. The simplest TD method is the one\sphinxhyphen{}step TD. It updates the prediction of \(v_{\pi}\) at each time step:
\begin{equation*}
\begin{split}V(S_t) \leftarrow V(S_t) + \alpha[R_{T+1} + \gamma V(S_{t+1}) - V(S_t)]\end{split}
\end{equation*}
\sphinxAtStartPar
While MC method would update after each episode:
\begin{equation*}
\begin{split} V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)] \end{split}
\end{equation*}
\sphinxAtStartPar
One\sphinxhyphen{}step TD effectively bootstraps the update like DP, but it uses a sampling estimate like the MC method to estimate V {[}\hyperlink{cite.Discussion:id51}{RMM18}{]}. The sampling estimate differs from the expected estimate on the fact that they are based on a single sample successor rather than on the complete distribution of all possible successors {[}\hyperlink{cite.Discussion:id47}{Li16}{]}. In the updating rule of TD methods there is the TD error (see quantity in brackets) which is the difference between the previous estimate of \(S_t\) and the updated estimate  \(R_{t+1} + \gamma V(S_{t+1} - V(S_t)\). The TD error is the error in the estimate made at that time. The psuedocode of the one\sphinxhyphen{}step TD method can be found in the appendix. TD methods lend themself quite easily to different methods in MC. For example, the Sarsa control algorithm is an on\sphinxhyphen{}policy TD in which the action values are updates using state\sphinxhyphen{}action pairs {[}\hyperlink{cite.Discussion:id47}{Li16}{]}:
\begin{equation*}
\begin{split} q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma q(s_{t+1}, a_{t+1}) - q(s_t, a_t)] \end{split}
\end{equation*}
\sphinxAtStartPar
The same methodology is used here. \(q_\pi\) is continuously estimated for policy \(\pi\) while policy \(\pi\) changes toward the optimal policy \(\pi^*\) by a greedy approach. TD methods can also be applied to off\sphinxhyphen{}policy fashion. They are then called Q\sphinxhyphen{}learning which is widely applied in the literature. Q\sphinxhyphen{}learning is an off\sphinxhyphen{}policy method because they learn the action\sphinxhyphen{}value function \(q\) independent of the policy being followed. They select the maximal or minimal action\sphinxhyphen{}value pair in the current state \(s\):
\begin{equation}\label{equation:Reinforcement_learning:Q-learning}
\begin{split}q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma min_a q(s_{t+1}, a_{t+1}) - q(s_t, a_t)] \end{split}
\end{equation}
\sphinxAtStartPar
The policy still has an effect in that it determines which states\sphinxhyphen{}action pairs are being visited, but the learned action\sphinxhyphen{}value function \(q\) directly approximates \(q_*\). This simplifies the analysis and enables early convergence. The last TD method is called the expected Sarsa and it uses the expected value instead of the minimum over the next state\sphinxhyphen{}action pairs to update the value function:
\begin{equation*}
\begin{split}  q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma \mathbb{E}_{\pi}[q(s_{t+1}, a_{t+1})|S_{t+1}] - q(s_t, a_t)] \end{split}
\end{equation*}\begin{equation*}
\begin{split} q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[R_{t+1} + \gamma \sum_a \pi(a|s_{t+1}) q(s_{t+1}, a) - q(s_t, a_t)] \end{split}
\end{equation*}
\sphinxAtStartPar
The main benefit of Expected Sarsa over Sarsa is that it eliminates the variance caused by the random selection of \(a_{t+1}\). Another benefit of Expected Sarsa is that it can be used as an off\sphinxhyphen{}policy method when the target policy \(\pi\) is replaced with another policy {[}\hyperlink{cite.Discussion:id50}{San21}{]}.

\sphinxAtStartPar
These three methods lay at the foundation of RL and numerous adaptations have been made to fit the problem at hand. For example

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=350\sphinxpxdimen]{{different_methods}.png}
\caption{monte carlo temporal difference and dynammic programming}\label{\detokenize{Reinforcement_learning:diff-meth-fig}}\end{figure}


\subsection{Dimensions of a model\sphinxhyphen{}based reinforcement algorithm}
\label{\detokenize{Reinforcement_learning:dimensions-of-a-model-based-reinforcement-algorithm}}
\sphinxAtStartPar
{[}\hyperlink{cite.Discussion:id7}{MBJ20a}{]} addresses the six most critical dimensions of an RL algorthim: computational effort, action value selection, cumulative return estimation, policy evaluation, function representation and update method. The first dimension has to do with the computational effort that is required to run the algorithm. The computational effort is primarily determined by the state set that is chosen (see \hyperref[\detokenize{Reinforcement_learning:state-space-fig}]{Figure \ref{\detokenize{Reinforcement_learning:state-space-fig}}}). The first option is to consider all states \(S\) of the dynamic environment. In practice, this often becomes impractical to consider due to the curse of dimensionality. The second and third possibilities are all reachable states and all relevant states. All reachable states are the states which are reachable from any start under any policy, while for the relevant states only those states under the optimal policy are considered. The last option is to use start states. These are all the states with a non\sphinxhyphen{}zero probability under \(p(s_0)\)

\sphinxAtStartPar
(need examples and further explanaition curse of dimensionality)

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=350\sphinxpxdimen]{{state_space}.png}
\caption{state\_space dimensions}\label{\detokenize{Reinforcement_learning:state-space-fig}}\end{figure}

\sphinxAtStartPar
The second dimension is the action selection and has primarily to due to with exploration process of the algorithm. The first consideration in action selection is the candidate set that is considered for the next action. Then the optimal action needs to be considered while still keeping exploration in mind. For selecting the candidate set two main approaches are considered: step\sphinxhyphen{}wise and frontier. Frontier methods only start exploration once they are on the frontier, while step\sphinxhyphen{}wise methods have a new candidate set at each step of the trajectory. the MC method, DP and TD learning described above use step\sphinxhyphen{}wise exploration, while frontier methods are primarly used in robotics {[}\hyperlink{cite.Discussion:id49}{NZKN19}{]}. For the second consideration, selecting the action value,  different methods have been adopted. The first one is random explorations like \(\varepsilon\)\sphinxhyphen{}greedy exploration as explained in the section of Monte Carlo methods. These explorations techniques enable us to escape from a local minimum but can cause a jittering effect in which we undo an exploration step at random. The second approach is a value\sphinxhyphen{}based exploration that uses the value\sphinxhyphen{}based information to better direct the perturbation {[}\hyperlink{cite.Discussion:id48}{YLL+}{]}. A good example of this are mean action values. They improve the random exploration by incorporating the mean estimates of all the available actions. Meaning, they explore actions with higher values more frequently than actions with lower values. The last option is state\sphinxhyphen{}based exploration. State\sphinxhyphen{}based exploration uses state\sphinxhyphen{}dependent properties to inject noise. Dynamic programming is a good example of this approach. DP is an ordered state\sphinxhyphen{}based exploration. Ordered state\sphinxhyphen{}based exploration sweeps through the state space in orded like tree structure. Other state\sphinxhyphen{}based explorations are possible like novelty and priors.

\sphinxAtStartPar
The dimensions of the calculation of the cumulative return estimation (see \eqref{equation:Reinforcement_learning:return}) can be expressed in the formula to address the practical issues and limitations in RL:
\begin{equation*}
\begin{split} G_t = \sum_{k=0}^T\gamma^kR_{t+k+1} \end{split}
\end{equation*}\begin{equation*}
\begin{split} q(s,a) = E[G_t| S_t = d, A_t = a]\end{split}
\end{equation*}\begin{equation*}
\begin{split} \hat{q}(s,a) = \sum_{k=0}^T \gamma^kR_{t+k+1} + \gamma^KB(s_{t+T}) \end{split}
\end{equation*}
\sphinxAtStartPar
Where \(T \in {1,2,3, ..., \infty}\) denotes the sample depth and \(B(.)\) is a bootstrap function. For the sample depth three possible option are possible: \(K = \infty\), \(K = 1\),  \(K = n\) or reweighted. Monte Carlo methods for example use a sample depth to infinity as they do not bootstrap at all. Instead, DP uses bootstrapping at each iteration, so \(K = 1\). An intermediate method between DP and Monte Carlo methods can also be devised in which \( K = n\). The reweighted option is a special case of \( K = n\) in which targets of different depths are combined with a weighting scheme. The bootstrap function can be devised using a learned value function like the state value function or the state\sphinxhyphen{}action value function or following a heuristic approach. A good heuristic can be obtained by first solving a simplified version of the problem. An example of this is first solving the deterministic problem and then using the solution as a heuristic on its stochastic counterpart {[}\hyperlink{cite.Discussion:id7}{MBJ20a}{]}. The second dimension in cumulative return estimation is whether full knowledge of the dynamic system is in place (full backups) or a sample is taken from the environment (sample backups) {[}\hyperlink{cite.Discussion:id63}{ADBB17}{]}. In \hyperref[\detokenize{Reinforcement_learning:backups-bootstrap-fig}]{Figure \ref{\detokenize{Reinforcement_learning:backups-bootstrap-fig}}} these two dimensions are represented.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen,height=250\sphinxpxdimen]{{backups}.png}
\caption{consideration in calculating the cumulative return estimation}\label{\detokenize{Reinforcement_learning:backups-bootstrap-fig}}\end{figure}

\sphinxAtStartPar
The fourth dimension to consider is policy evaluation. Policy evaluation has two dimensions. One is on which policy to use: on\sphinxhyphen{}policy or off\sphinxhyphen{}policy method. We have already seen this dimension in the section of MC methods and it will not be further discussed. Another dimension is function representation. The first choice that needs to be made here is which function to represent. In theory, we have two essential functions: the value function and the policy function. The value function can be the state\sphinxhyphen{}action value function or just the state value function, but primarily represents the value of the current or optimal policy at all considered state\sphinxhyphen{}action pairs. The policy function on the other hand maps every state to a probability distribution over actions and is best used in continuous action spaces as we can directly act in the environment by sampling from the policy distribution {[}\hyperlink{cite.Discussion:id7}{MBJ20a}{]}. The second choice is how to represent this function. There are two possibilities here. The first option is using a tabular approach in which each state is a unique element for which we store an individual estimate.  This can be done on a global level or local level. At the global level, the entire state space is encapsulated by the table. Unfortunately, this method does not scale well and is only appliable in small exploratory problems. On the contrary, a local table does scale well as it is built temporarily until the next real step. The other method for function representation is function approximation. Function approximation builds on the concept of generalization. Generalization assumes that similar states to function will in general also have approximately similar output predictions (Generalization is further discussed in next section).  Function approximation uses this to share information between near similar states and therefore store a global solution for a larger state space \{cite\}van2012reinforcement. There are two kinds of function approximations: parametric and non\sphinxhyphen{}parametric. A good example of a parametric function approximation is a neural network and for non\sphinxhyphen{}parametric a k\sphinxhyphen{}nearest neighbors can be thought of. The big challenge in function approximation is finding the balance between overfitting and underfitting the actual data.

\sphinxAtStartPar
The last dimension is the updating method. The updating method used should be in line with the function representation and the policy evaluation method as certain updating rules only work on a set of function representation and policy evaluation methods {[}\hyperlink{cite.Discussion:id7}{MBJ20a}{]}. For the updating method, there are quite a few choices to make. The first choice is choosing between gradient\sphinxhyphen{}based updates and gradient\sphinxhyphen{}free updates. In gradient\sphinxhyphen{}based updates we repeatedly update our parameters in the direction of the negative gradient loss with respect to the parameters:
\begin{equation*}
\begin{split} \theta \leftarrow \theta - \alpha \cdot \frac{\partial L(\theta)}{\partial \theta} \end{split}
\end{equation*}
\sphinxAtStartPar
Where \(\alpha \in \mathbb{R}^+\) is a learning rate. Before the updating rule can be applied a loss function \(L(\theta)\) should first be chosen. The loss function is usually a function of both the function representation and the policy evaluation method. As there are two kinds of function to represent in function representation, there are also two kinds of losses: value loss and policy loss. The most general value loss is the mean squared error loss. In policy loss, there are various methods to estimating the loss. For example the policy gradient specifies a relation between the value estimates \(\hat{q}(s_t,a_t)\) and the policy \(\pi_{\theta}(a_t|s_t)\) by ensuring that actions with high values also get high policy probabilities assigned:
\begin{equation*}
\begin{split}L(\theta|s_t, a_t) = -\hat{q}(s_t,a_t) \cdot ln (\pi_{\theta}(a_t|s_t))\end{split}
\end{equation*}
\sphinxAtStartPar
Once the loss function is defined, the gradient\sphinxhyphen{}based updating rule can be applied. The updating again depends on the function representation for example the value update on a table for the mean squared loss function becomes:
\begin{equation*}
\begin{split} q(s,a) \leftarrow q(s,a) - \alpha \cdot \frac{\partial L(q(s,a))}{\partial q(s,a)} \end{split}
\end{equation*}\begin{equation*}
\begin{split} \frac{\partial L(q(s,a))}{\partial q(s,a)} = 2 \cdot \frac{1}{2}(q(s,a) - \hat{q}(s,a)) \end{split}
\end{equation*}\begin{equation*}
\begin{split} q(s,a) \leftarrow q(s,a) - \alpha(q(s,a) - \hat{q}(s,a)) \end{split}
\end{equation*}\begin{equation*}
\begin{split} q(s,a) \leftarrow (1- \alpha) \cdot q(s,a) + \alpha \cdot \hat{q}(s,a)  \end{split}
\end{equation*}
\sphinxAtStartPar
Where q(s,a) is a table entry. The same can be done for function approximation where the derivative of the loss function then becomes:
\begin{equation*}
\begin{split} \frac{\partial L(\theta)}{\partial \theta} = (q(s,a) - \hat{q}(s,a)) \cdot \frac{\partial q(s,a)}{\partial \theta} \end{split}
\end{equation*}
\sphinxAtStartPar
Where \(\frac{\partial q(s,a)}{\partial \theta}\) can be for example the derivatives in a neural newtork.

\sphinxAtStartPar
Gradient\sphinxhyphen{}free updating rules use a parametrized policy function and then repeatedly perturb the parameters in policy space, evaluate the new solution by sampling traces and decide whether the perturbed solution should be retained. they only require an evaluation function and treat the problem as a black\sphinxhyphen{}box optimization setting. Gradient\sphinxhyphen{}free updating methods are thus not fit for model\sphinxhyphen{}based RL. An overview of the different dimensions can be viewed in \hyperref[\detokenize{Reinforcement_learning:dim-fig}]{Figure \ref{\detokenize{Reinforcement_learning:dim-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen,height=250\sphinxpxdimen]{{dimensions}.png}
\caption{The dimensions of a reinforcment learning algorithm}\label{\detokenize{Reinforcement_learning:dim-fig}}\end{figure}


\section{Curse of Dimensionality and Model\sphinxhyphen{}based Deep Reinforcment Learning}
\label{\detokenize{Reinforcement_learning:curse-of-dimensionality-and-model-based-deep-reinforcment-learning}}
\sphinxAtStartPar
For continuous states and actions, which is the most relevant case for optimal control. The state and action dimension are infinite. This require function approximation methods to estimate the optimal value function \(v_{\pi^*}\) {[}\hyperlink{cite.Discussion:id53}{SBLL19}{]}. The most notable function approximations are neural networks. Especially deep neral networks (DNN) can significantly reduce the time and effort required to approximate the value function {[}\hyperlink{cite.Discussion:id53}{SBLL19}{]}. The use of parrallelization to speed up and stabilize the learning process {[}\hyperlink{cite.Discussion:id53}{SBLL19}{]}.


\section{G\sphinxhyphen{}learning, a stochastic adaptation on Q\sphinxhyphen{}learning}
\label{\detokenize{Reinforcement_learning:g-learning-a-stochastic-adaptation-on-q-learning}}
\sphinxAtStartPar
Q\sphinxhyphen{}learning learns extremely slow in noisy environments due to the minimization bias. In \eqref{equation:Reinforcement_learning:Q-learning} the minimum over the estimated values is used implicitly as an estimate of the minimum value, which can lead to significant positive bias in noisy environments. Consider, for example, a single state \(s\) where there are many actions \(a\) whose true values are all zero but whose estimated values are uncertain and thus distributed some above and some below zero. The minimum of the true values is zero, but the minimum of the estimates is negative. Consequently, introducing minimization bias \{cite\}´sutton2018reinforcement´. This can also be illustrated by the Jensen’s inequality for the concave min operator. Assume that \(Q(s,a)\) is an unbiased but noisy estimate of the optimal \(Q^*(s,a)\). Then it applieas that
\begin{equation*}
\begin{split} \mathbb{E}[min_aQ(s,a)] \leq min_aQ^*(s,a)\end{split}
\end{equation*}
\sphinxAtStartPar
This creates an optimistic bias, causing the cost\sphinxhyphen{}to\sphinxhyphen{}go to appear lower than it is. The minimization bias has an impact on the learning rate of the Q\sphinxhyphen{}learning policy. The impact depends on the gap \(Q^*(s,a') - V^*(s)\)  between the value of a non\sphinxhyphen{}optimal action \(a'\) and that of the optimal action. If the gap is large, \(a'\) seems suboptimal as desired. If the gap is small, confusing \(a'\) for the optimal action does not effect the learning process. However, when the gap is in the order of the noise term, the minimization bias has a significant impact, because \(a'\) does not appear to be suboptial and is thus still accepted as optimal. The optimstic bias is futher enhanced by propegating the bias between states and can lead to regions of the state space that are highly biased creating large\sphinxhyphen{}gap suboptimal actions. Although this problem hampers the learning rate, Q\sphinxhyphen{}learning can still learn in a stochstic environment due to the fact that the bias draws exploration towards the given state, leading to a decrease in variance, which in turn reduces the bias {[}\hyperlink{cite.Discussion:id43}{FPT15}{]}.

\sphinxAtStartPar
G\sphinxhyphen{}learning is an adaptation of Q\sphinxhyphen{}learning, specifically designed to handle noisy environments. It is also an off\sphinxhyphen{}policy approach in a model\sphinxhyphen{}free setting, but it regularizes the state\sphinxhyphen{}action value function learned by an agent. It regularizes the state\sphinxhyphen{}action value function by penalizing deterministic policies early in the optimization process. Penilazation is done early in the process, because there is still a small sample size and therefore a more randomized policy is prefered. When the sample size grows, one should expect to shifts to a more deterministic and exploiting policy. This is what G\sphinxhyphen{}learning effectively does. It adds a cost\sphinxhyphen{}to\sphinxhyphen{}go term to the value function that penalizes the early deterministic policies which diverge from a simple stochastic prior policy \(\rho(a|s)\). The prior stochastic policy sets up an information cost of a learned policy \(\pi(a|s)\), effectively penalizing deviations from the prior policy .
\begin{equation*}
\begin{split} g^{\pi}(s,a) = log(\frac{\pi(a,s)}{\rho(a,s)}) \end{split}
\end{equation*}
\sphinxAtStartPar
taken the expectation of the policy \(\pi\) gives us the Kullback\sphinxhyphen{}Leibler divergence of the two policies.
\begin{equation*}
\begin{split} \mathbb{E}_{\pi}[g^{\pi}(s,a)|s] = D_{KL}[\pi_s || \rho_s] \end{split}
\end{equation*}
\sphinxAtStartPar
Now consider the total discounted expected information cost
\begin{equation}\label{equation:Reinforcement_learning:information cost}
\begin{split}I^{\pi}(s) = \sum_{t\geq0} \gamma^t \mathbb{E}[g^{\pi}(s_t,a_t)|s_0=s]\end{split}
\end{equation}
\sphinxAtStartPar
Adding \eqref{equation:Reinforcement_learning:information cost} to the value function \eqref{equation:Reinforcement_learning:value} gives the free\sphinxhyphen{}energey function
\begin{equation*}
\begin{split}F^{\pi}(s)= V^{\pi}(s) + \frac{1}{\beta} I^{\pi}(s)\end{split}
\end{equation*}\begin{equation}\label{equation:Reinforcement_learning:free-energy}
\begin{split}\hspace{3cm} = \sum_{t\geq 0 } \gamma^t \mathbb{E}[\frac{1}{\beta} g^{\pi}(s_t,a_t) + R_t|s_0=s] \end{split}
\end{equation}
\sphinxAtStartPar
Where \(\beta\) is the parameter which sets the weight of the information cost in the value function. If \(\beta\) is small, \(\pi\) will act similar to \(\rho\). When \(\beta\) is large, \(\pi\) will diverge from the prior and will therefore approache the greedy policy of Q\sphinxhyphen{}learning. A smooth transition between small and large values for \textbackslash{}beta will allow the algorithm to avoid early deterministic policies and still be able to exploit the optimal values. The same approach can be done for the state\sphinxhyphen{}action value function \(q(s,a)\)
\begin{equation}\label{equation:Reinforcement_learning:free-q}
\begin{split}H^{\pi}(s,a) = \sum_{t\geq 0 } \gamma^t \mathbb{E}[R_t + \frac{\gamma}{\beta} g^{\pi}(s_{t+1}, a_{t+1})|s_0=s, a_0=a] \end{split}
\end{equation}
\sphinxAtStartPar
Notice that the information term at time \(t=0\) is not needed as the action \(a_0 =a\) is already known. Given \eqref{equation:Reinforcement_learning:free-energy} and \eqref{equation:Reinforcement_learning:free-q} it follows that
\begin{equation}\label{equation:Reinforcement_learning:free-energy2}
\begin{split}F^{\pi}(s) = \sum_a \pi(a|s)[\frac{1}{\beta}log\frac{\pi(a|s)}{\rho(a|s)} + H^{\pi}(s,a)]\end{split}
\end{equation}
\sphinxAtStartPar
The gradient of \(F^{\pi}\) at zero is
\begin{equation}\label{equation:Reinforcement_learning:soft-min}
\begin{split}\pi(a|s) = \frac{\rho(a|s)e^{-\beta H(s,a)}}{\sum_{a'} \rho(a'|s)e^{-\beta H(s,a')}}\end{split}
\end{equation}
\sphinxAtStartPar
\eqref{equation:Reinforcement_learning:soft-min} is the soft\sphinxhyphen{}min operator applied to H. Now evaluate \eqref{equation:Reinforcement_learning:free-energy2} at \eqref{equation:Reinforcement_learning:soft-min}
\begin{equation*}
\begin{split} F^{\pi}(s) = \frac{-1}{\beta} log(\sum_a \rho(a,s) e^{-\beta H^{\pi}(s,a)}) \end{split}
\end{equation*}
\sphinxAtStartPar
This expression can get pluged in \eqref{equation:Reinforcement_learning:free-q}and as a result the optimal \(H^*\) is achieved.
\begin{equation*}
\begin{split} H^*(s,a) = \mathbb{E}[R|s,a] -\frac{\gamma}{\beta} \mathbb{E}[log \sum_{a'} \rho(a'|s')e^{-\beta H^*(s',a')}] \end{split}
\end{equation*}
\sphinxAtStartPar
Given the above expression, the Q\sphinxhyphen{}learner is adapted to learn the optimal G\textasciicircum{}* from interaction with the environment by applying the update rule
\begin{equation*}
\begin{split} G(s_t, a_t) \leftarrow (1-\alpha_t)G(s_t,a_t) + \alpha_t(c_t - \frac{\gamma}{\beta}log(\sum_{a'}e^{-\beta H(s_{t+1}, a')})) \end{split}
\end{equation*}
\sphinxAtStartPar
The G\sphinxhyphen{}learner is applied by to

\sphinxAtStartPar
in the next subsection we will describe the general approach of the Deep BSDE and link it to the RL theory. Although this method is orginaly designed to only solve the equation at timestep \(t=0\), future research might be able to solve the PDE at each time point \(t\).


\section{The Deep Backward Stochastic Differential Equation Method}
\label{\detokenize{Reinforcement_learning:the-deep-backward-stochastic-differential-equation-method}}
\sphinxAtStartPar
The general PDEs that Deep BSDE method solves can be written as:
\begin{equation}\label{equation:Reinforcement_learning:gen_form}
\begin{split}\frac{\partial u}{\partial t} + \frac{1}{2} Tr(\sigma \sigma^T (Hess_xu) + \Delta u(t,x)  \mu(t,x) + f(t,x,u, \sigma^T(\Delta_x u)) = 0 \end{split}
\end{equation}
\sphinxAtStartPar
with some temrinal condition \(u(T,x) = g(x)\).

\sphinxAtStartPar
With \(u(T,x) = L(x)\). The key idea is to  reformulate the PDE as an appropriate stochastic problem {[}\hyperlink{cite.Discussion:id28}{WHJ20}{]} and {[}\hyperlink{cite.Discussion:id29}{WHJ17}{]}. Here the probability space (\(\Omega,\mathcal{F}, \mathbb{P}\)) is adapted to the high dimensional problem. So \(W: [0, T] \times \Omega \rightarrow \mathbb{R}^d\) becomes a d\sphinxhyphen{}dimensional standard Brownian motion on (\(\Omega,\mathcal{F}, \mathbb{P}\)) and let \(\mathcal{A}\) be the set of all \(\mathbb{F}\)\sphinxhyphen{}adapted \(\mathcal{R^d}\)\sphinxhyphen{}values stochastic processes with continuous sample paths. Let \(\{X_T\}_{0 \leq t \leq T}\) be a d\sphinxhyphen{}dimensional stochastic process which satisfies
\begin{equation*}
\begin{split} X_t = \varepsilon + \int_0^t \mu(s,X_s)ds + \int_0^t \sigma(s,X_s)dW_s \end{split}
\end{equation*}
\sphinxAtStartPar
Using Itô’s lemma, we obtain that
\begin{equation*}
\begin{split} y(t, X_t) - u(0,X_0) = - \int_0^t f(s,X_s,u(s,X_s), [\sigma(s,X_s)]^T(\Delta_x u)(s,X_s)) ds + \int_0^t[\Delta u(s,X_s)]^T\sigma(s,X_s)dW_s\end{split}
\end{equation*}
\sphinxAtStartPar
A backward stochstic differential equation can be written as
\begin{equation}\label{equation:Reinforcement_learning:stoch_con}
\begin{split}\begin{cases} X_t = \varepsilon + \int_0^t \mu(s,X_s) ds + \int_0^t\sigma(s,X_S)dW_S \\ 
Y_t = g(X_T) + \int_t^T f(s, X_s, Y_s, Z_s)ds - \int_t^T(Z_s)^T dW_s
\end{cases} \end{split}
\end{equation}
\sphinxAtStartPar
In the literature it was found that the solution of PDE and its spatial derivative are now the solution of the stochastic control problem \eqref{equation:Reinforcement_learning:stoch_con} {[}\hyperlink{cite.Discussion:id28}{WHJ20}{]}. The relationship between the PDE \eqref{equation:Reinforcement_learning:gen_form} and the BSDe \eqref{equation:Reinforcement_learning:stoch_con} is based on the nonlinear Feynman\sphinxhyphen{}Kac formula {[}\hyperlink{cite.Discussion:id25}{Blo18}{]} and {[}\hyperlink{cite.Discussion:id45}{GulerLP19}{]}. Under suitable additional regularity assumption on the nonlinearity \(f\) in the sense that for all \(t \in[0,T]\) it holds \(\mathbb{P}\)\sphinxhyphen{}a.s. that
\begin{equation}\label{equation:Reinforcement_learning:identity}
\begin{split}Y_t = u(t, \epsilon + W_t) \in \mathbb{R}  \hspace{0.2cm}\text{and}\hspace{0.2cm} Z_t = (\Delta_x u) (t, \epsilon + W_t) \in \mathbb{R}^d\end{split}
\end{equation}
\sphinxAtStartPar
The first identity in \eqref{equation:Reinforcement_learning:identity} is referred to as nonlinear Feynman\sphinxhyphen{}Kac formula {[}\hyperlink{cite.Discussion:id29}{WHJ17}{]}. \((Y_t, Z_t), t \in [0,T]\) is a solution for the BSDE and with \eqref{equation:Reinforcement_learning:identity} in mind the PDE problem can be formulated as the following variational problem:
\begin{equation*}
\begin{split} inf_{Y_0,\{Z_T\}_{0\leq t\leq T}} \mathbb{E}[|g(X_T) - Y_T|^2] \end{split}
\end{equation*}\begin{equation*}
\begin{split} s.t. \hspace{0.2cm}X_T = \varepsilon + \int_0^t \mu(s,X_s)ds + \int_0^t\sum(s,X_s)dW_s \end{split}
\end{equation*}\begin{equation*}
\begin{split}  \hspace{1.2cm}Y_t = Y_0 - \int_0^th(s,X_s,Y_s,Z_s)ds + \int_0^t(Z_s)^TdW_s\end{split}
\end{equation*}
\sphinxAtStartPar
The minimizer of this variational problem is the solution to the PDE {[}\hyperlink{cite.Discussion:id26}{Rai18}{]}. The main idea behind Deep BSDE method is to approximate the unknown function \(X_0 \rightarrow u(, X_0)\) and \(X_t \rightarrow [\sigma(t,X_t)]^T((\Delta_x u)(t,X_t)\) by two feedforward neural networks \(\psi\) and \(\phi\) {[}\hyperlink{cite.Discussion:id30}{HJW18}{]}. To achieve this we discretize time using Euler scheme on a grid \( 0 = t_0<t_1<...<T_N =T \)
\begin{equation*}
\begin{split} inf_{\psi_0,\{\phi_n\}^{N-1}_{n=0}} \mathbb{E}[|g(X_T) - Y_T|^2] \end{split}
\end{equation*}\begin{equation*}
\begin{split} s.t. \hspace{0.2cm} X_0 = \varepsilon, \hspace{0.2cm} Y_0 = \psi_0(\varepsilon)\end{split}
\end{equation*}\begin{equation*}
\begin{split} \hspace{3.2cm} X_{t_{n+1}} = X_{t_i} \mu(t_n,X_{t_n}) \Delta t + \sigma(t_n,X_{t_n}) \Delta W_n\end{split}
\end{equation*}\begin{equation*}
\begin{split} Z_{t_n} = \psi_(X_{t_n}) \hspace{0.4cm}\end{split}
\end{equation*}\begin{equation*}
\begin{split}  \hspace{1.2cm}Y_{t_{n+1}} = Y_{t_n} - f(t_n,X_{t_n},Y_{t_n},Z_{t_n})\Delta t  + (Z_{t_n})^T \Delta W_n\end{split}
\end{equation*}
\sphinxAtStartPar
At each time slide \(t_n\), a subnetwork is associated. These subnetworks are then stacked together to form a deep composite neural network {[}\hyperlink{cite.Discussion:id27}{HJW17}{]}. The network takes the paths  \(\{X_{t_n}\}_{0\leq n \leq N}\) and \(\{W_{t_n}\}_{0\leq n \leq N}\) as the input data and gives as final output, denoted by \(\hat{u}(\{ X_{t_n}\}_{0 \leq n \leq N}, \{W_{t_n}\}_{0 \leq n \leq N}\),  as an approximation to \(u(t_N, X_{t_N})\) (see \hyperref[\detokenize{Reinforcement_learning:bsdn-fig}]{Figure \ref{\detokenize{Reinforcement_learning:bsdn-fig}}}) {[}\hyperlink{cite.Discussion:id30}{HJW18}{]}. Thereby it is only solved a time step \(t=0\). The difference in the matching of a given terminal condition can be used to define the expected loss function {[}\hyperlink{cite.Discussion:id28}{WHJ20}{]} {[}\hyperlink{cite.Discussion:id27}{HJW17}{]}
\begin{equation*}
\begin{split} m(\theta) = \mathbb{E}[|g(X_{t_N}) - \hat{u}(\{ X_{t_n}\}_{0 \leq n \leq N}, \{W_{t_n}\}_{0 \leq n \leq N} |^2]\end{split}
\end{equation*}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=300\sphinxpxdimen]{{BSDE_NN}.png}
\caption{neural network for Deep BSDE method}\label{\detokenize{Reinforcement_learning:bsdn-fig}}\end{figure}

\sphinxAtStartPar
An other way to look at it is that the stochastic control problem is a model\sphinxhyphen{}based reinforcement learning problem {[}\hyperlink{cite.Discussion:id30}{HJW18}{]}. In this setting \(Z\) is viewed as the policy we try to approximate using a feedforward neural network. The process \(u(t, \varepsilon + W_t), t \in [0, T]\), corresponds to the value function associated with the stochastic control problem and can be approximately employed by the policy Z {[}\hyperlink{cite.Discussion:id29}{WHJ17}{]}. A benefit of using deep BSDE method is does not require us to generate training data beforehand. The paths play the role of the data and they are generated on the spot {[}\hyperlink{cite.Discussion:id28}{WHJ20}{]}.

\sphinxAtStartPar
The deep BSDE method solves the PDE for \(Y_0= u(0, X_0) = u(0, \varepsilon)\). This means that in order to obtain an approximate of \(Y_t = u(t,X_t)\) at a later time \(t>0\), we will have to retain our algorithm. {[}\hyperlink{cite.Discussion:id26}{Rai18}{]} solves this isue by directly placing a neural network on the object of interest, the unknown solution \(u(t,x)\)


\chapter{Financial Application of Reinforcement Learning}
\label{\detokenize{Financial_application:financial-application-of-reinforcement-learning}}\label{\detokenize{Financial_application::doc}}
\sphinxAtStartPar
Advancements in Reinforcement Learning can be applied to a financial setting. In this subsection, a model is proposed for optimal consumption, life insurance and investment. This model can be extended in multiple ways and can be applied in a broad range of financial scenario’s. The ony problem of the model is the curse of dimensionality which makes the Hamilton\sphinxhyphen{}Jacobi\sphinxhyphen{}Bellman (HJB) equation of the model in higher dimensions impossible to solve. A deep learning\sphinxhyphen{}based approach that can handle the HJB equation in higher dimensions is proposed. Although the method originates from optimal control literature, it resembles many feutures of the reinforcement algorithms and it solves the exact same problem.


\section{Optimal consumption, investment and life insurance in an intertemporal model}
\label{\detokenize{Financial_application:optimal-consumption-investment-and-life-insurance-in-an-intertemporal-model}}
\sphinxAtStartPar
The first person to include uncertain lifetime and life insurance decisions in a discrete life\sphinxhyphen{}cycle model was Yaari {[}\hyperlink{cite.Discussion:id2}{Yaa65}{]}. He explored the model using a utility function without bequest (Fisher Utility function) and a utility function with bequest (Marshall Utility function) in a bounded lifetime. In both cases, he looked at the implications of including life insurance. Although Yaari’s model was revolutionary in the sense that now the uncertainty of life could be modeled, Leung {[}\hyperlink{cite.Discussion:id6}{Leu94}{]} found that the constraints laid upon the Fisher utility function were not adequate and lead to terminal wealth depletion. Richard {[}\hyperlink{cite.Discussion:id5}{Ric75}{]} applied the methodology of Merton {[}\hyperlink{cite.Discussion:id3}{Mer69}, \hyperlink{cite.Discussion:id4}{Mer75}{]} to the problem setting of Yaari in a continuous time frame. Unfortunately, Richard’s model had one deficiency: The bounded lifetime is incompatible with the dynamic programming approach used in Merton’s model. As an individual approaches his maximal possible lifetime T, he will be inclined to buy an infinite amount of life insurance. To circumvent this Richard used an artificial condition on the terminal value. But due to the recursive nature of dynamic programming, modifying the last value would imply modifying the whole result. Ye {[}\hyperlink{cite.Discussion:id8}{Ye06}{]}  found a solution to the problem by abandoning the bounded random lifetime and replacing it with a random variable taking values in \([0,\infty)\). The models that replaced the bounded lifetime, are thereafter called intertemporal models as the models did not consider the whole lifetime of an individual but rather looked at the planning horizon of the consumer.  Note that the general setting of Ye {[}\hyperlink{cite.Discussion:id8}{Ye06}{]} has a wide range of theoretical variables, while still upholding a flexible approach to different financial settings. On this account, it is a good baseline to confront the issues concerning the current models of financial planning. However, one of the downsides of the model is the abstract representation of the consumer. Namely, the rational consumer is studied, instead of the actual consumer. To detach the model from the notion of rational consumer, I will more closely look at behavioral concepts that can be implemented. In the next paragraph various modification will be discussed and a further review is conducted on the behavioral modifications

\sphinxAtStartPar
After Ye {[}\hyperlink{cite.Discussion:id8}{Ye06}{]} various models have been proposed which all have given rise to unique solutions to the consumption, investment, and insurance problem. The first unique setting is a model with multiple agents involved. For example,  Bruhn and Steffensen {[}\hyperlink{cite.Discussion:id18}{BS11}{]} analyzed the optimization problem for couples with correlated lifetimes with their partner nominated as their beneficiary using a copula and common\sphinxhyphen{}shock model, while Wei et al.{[}\hyperlink{cite.Discussion:id21}{WCJW20}{]} studied optimization strategies for a household with economically and probabilistically dependent persons. Another setting is where certain constraints are used to better describe the financial situation of consumers. Namely, Kronborg and Steffensen {[}\hyperlink{cite.Discussion:id20}{KS15}{]} discussed two constraints. One constraint is a capital constraint on the savings in which savings cannot drop below zero. The other constrain involves a minimum return in savings. A third setting describes models who analyze the financial market and insurance market in a pragmatic environment. A good illustration is the study of Shen and Wei {[}\hyperlink{cite.Discussion:id22}{SW16}{]}. They incorporate all stochastic processes involved in the investment and insurance market where all randomness is described by a Brownian motion filtration. An interesting body of models is involved in time\sphinxhyphen{}inconsistent preferences. In this framework, consumers do not have a time\sphinxhyphen{}consistent rate of preference as assumed in the economic literature. There exists rather a divergence between earlier intentions and later choices De\sphinxhyphen{}Paz et al. {[}\hyperlink{cite.Discussion:id31}{DPMSNR14}{]}. This concept is predominantly described in psychology. Specifically, rewards presented closer to the present are discounted proportionally less than rewards further into the future. An application of time\sphinxhyphen{}inconsistent preferences in the consumption, investment, and insurance optimization can be found in Chen and Li {[}\hyperlink{cite.Discussion:id24}{CL20}{]} and De\sphinxhyphen{}Paz et al. {[}\hyperlink{cite.Discussion:id31}{DPMSNR14}{]}. These time\sphinxhyphen{}inconsistent preferences are rooted in a much deeper behavioral concept called future self\sphinxhyphen{}continuity. Future self\sphinxhyphen{}continuity can be described as how someone sees himself in the future. In classical economic theory, we assume that the degree to which you identify with yourself has no impact on the ultimate result. In the next subsection, the relationship of future self\sphinxhyphen{}continuity and time\sphinxhyphen{}inconsistent preferences are more closely looked at and future self\sphinxhyphen{}continuity is further examined in the behavioral life\sphinxhyphen{}cycle model.


\subsection{The model specifications}
\label{\detokenize{Financial_application:the-model-specifications}}
\sphinxAtStartPar
In this section, I will set the dynamics for the baseline model in place. The dynamics follow primarily from the paper of Ye {[}\hyperlink{cite.Discussion:id8}{Ye06}{]}.

\sphinxAtStartPar
Let the state of the economy be represented by a standard Brownian motion \(w(t)\), the state of the consumer’s wealth be characterized by a finite state multi\sphinxhyphen{}dimensional continuous\sphinxhyphen{}time Markov chain \(X(t)\) and let the time of death be defined by a non\sphinxhyphen{}negative random variable \(\tau\). All are defined on a given probability space (\(\Omega, \mathcal{F}, \mathbb{P} \)) and \(W(t)\) is independent of \(\tau\). Let \(T< \infty\) be a fixed planning horizon. This can be seen as the end of the working life for the consumer. \(\mathbb{F} = \{\mathcal{F}_t, t \in [0,T]\}\), be the P\sphinxhyphen{}augmentation of the filtration \(\sigma\)\{\(W(s), s<t \}, \forall t \in [0,T]\) , so \(\mathcal{F}_t\) represents the information at time t. The economy consist of a financial market and an insurance market. In the following section I will construct these markets separetly following Ye {[}\hyperlink{cite.Discussion:id8}{Ye06}{]}.

\sphinxAtStartPar
The financial market consist of a risk\sphinxhyphen{}free security \(B(t)\) and a risky security \(S(t)\), who evolve according to
\begin{equation*}
\begin{split} \frac{dB(t)}{B(t)}=r(t)dt \end{split}
\end{equation*}\begin{equation*}
\begin{split} \frac{dS(t)}{S(t)}=\mu(t)dt+\sigma(t)dW(t)\end{split}
\end{equation*}
\sphinxAtStartPar
Where \(\mu, \sigma, r > 0\) are constants and \(\mu(t), r(t), \sigma(t): [0,T] \to R\) are continous. With \(\sigma(t)\) satisfying \(\sigma^2(t) \ge k, \forall t \in [0,T]\)

\sphinxAtStartPar
The random variable \(\tau_d\) needs to be first modeled for the insurance  market. Assume that \(\tau\) has a probability density function \(f(t)\) and probability distribution function given by
\begin{equation*}
\begin{split} F(t) \triangleq P(\tau < t) = \int_0^t f(u) du \end{split}
\end{equation*}
\sphinxAtStartPar
Assuming \(\tau\) is independent of the filtration \(\mathbb{F}\)

\sphinxAtStartPar
Following on the probability distribution function we can define the survival function as followed
\begin{equation*}
\begin{split} \bar{F}(t)\triangleq P(\tau \ge t) = 1 -F(t) \end{split}
\end{equation*}
\sphinxAtStartPar
The hazard function is the  instantaneous death rate for the consumer at time t and is defined by
\begin{equation*}
\begin{split} \lambda(t) = \lim_{\Delta t\to 0} = \frac{P(t\le\tau < t+\Delta t| \tau \ge t)}{\Delta t} \end{split}
\end{equation*}
\sphinxAtStartPar
where \(\lambda(t): [0,\infty[ \to R^+\) is a continuous, deterministic function with \(\int_0^\infty \lambda(t) dt = \infty\).

\sphinxAtStartPar
Subsequently, the survival and probability density function can be characterized by
\begin{equation*}
\begin{split} \bar{F}(t)= {}_tp_0= e^{-\int_0^t \lambda(u)du} \end{split}
\end{equation*}\begin{equation*}
\begin{split} f(t)=\lambda(t) e^{-\int_0^t\lambda(u)du} \end{split}
\end{equation*}
\sphinxAtStartPar
With conditional probability described as
\begin{equation*}
\begin{split} f(s,t) \triangleq \frac{f(s)}{\bar{F}(t)}=\lambda(s) e^{-\int_t^s\lambda(u)dy} \end{split}
\end{equation*}\begin{equation*}
\begin{split} \bar{F}(s,t) = {}_sp_t \triangleq \frac{\bar{F}(s)}{\bar{F}(t)} = e^{-\int_t^s \lambda(u)du} \end{split}
\end{equation*}
\sphinxAtStartPar
Now that \(\tau\) has been modeled, the life insurance market can be constructed. Let’s assume that the life insurance is continuously offered and that it provides coverage for an infinitesimally small period of time. In return, the consumer pays a premium rate p when he enters into a life insurance contract, so that he might insure his future income. In compensation he will receive  a total benefit of \(\frac{p}{\eta(t)}\) when he dies at time t. Where \(\eta : [0,T] \to R^+ \) is a continuous, deterministic function.

\sphinxAtStartPar
Both markets are now described and the wealth process \(X(t)\) of the consumer can now be constructed. Given an initial wealth \(x_0\), the consumer receives a certain amount of income \(i(t)\) \(\forall t \in [0,\tau \wedge T]\) and satisfying \(\int_0^{\tau \wedge T} i(u)du < \infty\). He needs to choose at time t a certain premium rate \(p(t)\), a certain consumption rate \(c(t)\) and a certain amount of his wealth \(\theta (t)\) that he invest into the risky asset \(S(t)\). So given the processes \(\theta\), c, p and i, there is a wealth process \(X(t)\)  \(\forall t \in [0, \tau \wedge T] \) determined by
\begin{equation*}
\begin{split} dX(t) = r(t)X(t) + \theta(t)[( \mu(t) - r(t))dt +\sigma(t)dW(t)] -c(t)dt -p(t)dt + i(t)dt,   \quad t \in [0,\tau \wedge T] \end{split}
\end{equation*}
\sphinxAtStartPar
If \(t=\tau\) then the consumer will receive the insured amount \(\frac{p(t)}{\eta(t)}\). Given is wealth X(t) at time t his total legacy will be
\begin{equation*}
\begin{split} Z(t) = X(t) + \frac{p(t)}{\eta(t)} \end{split}
\end{equation*}
\sphinxAtStartPar
The predicament for the consumer is that he needs to chose the optimal rates for c, p , \(\theta\) from the set \(\mathcal{A}\) , called the set of admissible strategies, defined by
\begin{equation*}
\begin{split} \mathcal{A}(x) \triangleq  \textrm{set of all possible triplets (c,p,}\theta) \end{split}
\end{equation*}
\sphinxAtStartPar
such that his expected utility from consumption, from legacy when \(\tau > T\) and from terminal wealth when \(\tau \leq T \)  is maximized.
\begin{equation*}
\begin{split} V(x) \triangleq \sup_{(c,p,\theta) \in \mathcal{A}(x)} E\left[\int_0^{T \wedge \tau} U(c(s),s)ds + B(Z(\tau),\tau)1_{\{\tau \ge T\}} + L(X(T))1_{\{\tau>T\}}\right] \end{split}
\end{equation*}
\sphinxAtStartPar
Where \(U(c,t)\) is the utility function of consumption, \(B(Z,t)\) is the utility function of legacy and \(L(X)\) is the utility function for the terminal wealth. \(V(x)\) is called the value function and the consumers wants to maximize his value function by choosing the optimal set \(\mathcal{A} = (c,p,\theta)\). The optimal set \(\mathcal{A}\) is found by using the dynamic programming technique described in the following section.


\subsection{dynamic programming principle}
\label{\detokenize{Financial_application:dynamic-programming-principle}}
\sphinxAtStartPar
To solve the consumer’s problem the value function needs to be restated in a dynamic programming form.
\begin{equation*}
\begin{split}J(t, x; c, p, \theta) \triangleq E \left[\int_0^{T \wedge \tau} U(c(s),s)ds + B(Z(\tau),\tau)1_{\{\tau \ge T\}} + L(X(T))1_{\{\tau>T\}}| \tau> t, \mathcal{F}_t \right] \end{split}
\end{equation*}
\sphinxAtStartPar
The value function becomes
\begin{equation*}
\begin{split} V(t,x) \triangleq \sup_{\{c,p,\theta\} \in \mathcal{A}(t,x)} J(t, x; c, p, \theta)  \end{split}
\end{equation*}
\sphinxAtStartPar
Because \(\tau\) is independent of the filtration, the value function can be rewritten as
\begin{equation*}
\begin{split} E \left[\int_0^T  \bar{F}(s,t)U(c(s),s) + f(s,t)B(Z(\tau),\tau) ds  + \bar{F}(T,t)L(X(T))| \mathcal{F}_t \right]\end{split}
\end{equation*}
\sphinxAtStartPar
The optimization problem is now converted from a random  closing time point to a fixed closing time point. The mortality rate can also be seen as a discounting function for the consumer as he would value the utility on the probability of survival.

\sphinxAtStartPar
Following the dynamic programming principle we can rewrite this equation as the value function at time s plus the value created from time step t to time step s. This enables us to view the optimization problem into a time step setting, giving us the incremental value gained at each point in time.
\begin{equation*}
\begin{split} V(t,x) = \sup_{\{c,p,\theta\} \in \mathcal{A}(t,x)} E\left[e^{-\int_t^s\lambda(v)dv}V(s,X(s)) + \int_t^s f(s,t)B(Z(s),s) + \bar{F}(s,t)U(c(s),s)ds|\mathcal{F}_t\right] \end{split}
\end{equation*}
\sphinxAtStartPar
The Hamiltonian\sphinxhyphen{}Jacobi\sphinxhyphen{}bellman (HJB) equation can be derived from the dynamic programming principle and is as follows
\begin{equation}\label{equation:Financial_application:BELL}
\begin{split}\begin{cases} 
V_t(t,x) -\lambda V(t,x) + \sup_{(c,p,\theta)} \Psi(t,x;c,p,\theta)  = 0 \\ V(T,x) = L(x)  
\end{cases}\end{split}
\end{equation}
\sphinxAtStartPar
where
\begin{equation*}
\begin{split} \Psi(t,x; c,p,\theta) \triangleq r(t)x + \theta(\mu(t) -r(t)) + i(t) -c -p)V_x(t,x) + \\ \frac{1}{2}\sigma^2(t)\theta^2V_{xx}(t,x) + \lambda(t)B(x+ p/\eta(t),t) + U(c,t) \end{split}
\end{equation*}
\sphinxAtStartPar
Proofs for deriving the HJB equation, dynammic programming principle and converting from a random closing time point to a fixed closing time point can be found in Ye {[}\hyperlink{cite.Discussion:id8}{Ye06}{]}

\sphinxAtStartPar
A strategy is optimal if
\begin{gather*}
0 =V_t(t,x) -\lambda(t)V(t,x) + \sup_{c,p,\theta}(t,x;c,p,\theta)  \\
0 = V_t(t,x) -\lambda(t)V(t,x) + (r(t)x+ i(t))V_x + \sup_c\{U(c,t)-cV_x\} + \\ \sup_p\{\lambda(t)B(x + p/\eta(t),t) - pV_x\} + \sup_\theta \{ \frac{1}{2}\sigma^2(t)V_{xx}(t,x)\theta^2 +(\mu(t) - r(t))V_x(t,x)\theta\} 
\end{gather*}
\sphinxAtStartPar
The first order conditions for regular interior maximum are
\begin{equation}\label{equation:Financial_application:cons_cond}
\begin{split}\sup_c  \{ U(c,t) - cV_x\} = \Psi_c(t,x;c^*,p^*,\theta^*)  \rightarrow  0 = -V_x(t,x) + U_c(c*,t) \end{split}
\end{equation}\begin{equation}\label{equation:Financial_application:ins_cond}
\begin{split}\sup_p\{\lambda(t)B(x + p/\eta(t),t) - pV_x\} = \Psi_p(t,x;c^*,p^*,\theta^*) \\ \rightarrow 0 = -V_x(t,x) + \frac{\lambda(t)}{\eta{t}}B_Z(x + p^*/\eta(t),t)\end{split}
\end{equation}\begin{equation}\label{equation:Financial_application:inv_cond}
\begin{split}\sup_\theta \{ \frac{1}{2}\sigma^2(t)V_{xx}(t,x)\theta^2 +(\mu(t) - r(t))V_x(t,x)\theta\} = \Psi_\theta(t,x;c^*,p^*,\theta^*)\\ \rightarrow 0 = (\mu(t) -r(t))V_x(t,x) + \sigma^2(t)\theta^*V_{xx}(t,x)\end{split}
\end{equation}
\sphinxAtStartPar
The second order conditions are
\begin{equation*}
\begin{split} \Psi_{cc}, \Psi_{pp}, \Psi_{\theta \theta} < 0 \end{split}
\end{equation*}
\sphinxAtStartPar
This optimal control problem has been solved analytically by Ye {[}\hyperlink{cite.Discussion:id8}{Ye06}{]} for the Constant Relative Risk Aversion utility function. To solve \eqref{equation:Reinforcement_learning:BELL} the  BSDE method can be used. The Deep BSDE method was the first deep learning\sphinxhyphen{}based numerical algorithm to solve general nonlinear parabolic PDEs in high dimensions.

\sphinxAtStartPar
remember the general form of PDEs which the Deep BSDE method solves:
\begin{equation}\label{equation:Financial_application:gen_form}
\begin{split}\frac{\partial u}{\partial t} + \frac{1}{2} Tr(\sigma \sigma^T (Hess_xu) + \Delta u(t,x)  \mu(t,x) + f(t,x,u, \sigma^T(\Delta_x u)) = 0 \end{split}
\end{equation}
\sphinxAtStartPar
with some terminal condition \(u(T,x) = g(x)\). \eqref{equation:Reinforcement_learning:BELL} can thus be reformulated in the general form:
\begin{equation*}
\begin{split}\underbrace{V_t(t,x)}_{\frac{\partial u}{\partial t}} + \underbrace{\frac{1}{2}\sigma(t)^2\theta^2V_{xx}(t,x)}_{\frac{1}{2}Tr(\sigma \sigma^T(Hess_xu(t,x)))} + \underbrace{(r(t) x + \theta(\mu(t) -r(t)) + i(t) -c -p)V_x(t,x)}_{\Delta u(t,x)\mu (t,x)} \\ + \underbrace{\lambda(t)B(x+\frac{p}{\eta(t)},t) + U(t,x) - \lambda(t)V(t,x)}_{f(t,x,u(t,x), \sigma^T(t,x)\Delta u(t,x))}\end{split}
\end{equation*}
\sphinxAtStartPar
The BSDE method can thus be applied.


\chapter{discussion}
\label{\detokenize{Discussion:discussion}}\label{\detokenize{Discussion::doc}}
\sphinxAtStartPar
the

\sphinxAtStartPar



\chapter{Appendix}
\label{\detokenize{Appendix:appendix}}\label{\detokenize{Appendix::doc}}

\section{pseudocode algorithms}
\label{\detokenize{Appendix:pseudocode-algorithms}}
\sphinxAtStartPar
\sphinxincludegraphics{{policy_iteration}.png}

\sphinxAtStartPar
\sphinxincludegraphics{{value_iteration}.png}

\begin{sphinxthebibliography}{Franccoi}
\bibitem[ADBB17]{Discussion:id63}
\sphinxAtStartPar
Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief survey of deep reinforcement learning. \sphinxstyleemphasis{arXiv preprint arXiv:1708.05866}, 2017.
\bibitem[BHB+20]{Discussion:id40}
\sphinxAtStartPar
André Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement learning with generalized policy updates. \sphinxstyleemphasis{Proceedings of the National Academy of Sciences}, 117(48):30079–30087, 2020.
\bibitem[BFH17]{Discussion:id14}
\sphinxAtStartPar
Qianwen Bi, Michael Finke, and Sandra J Huston. Financial software use and retirement savings. \sphinxstyleemphasis{Journal of Financial Counseling and Planning}, 28(1):107–128, 2017.
\bibitem[BDTS20]{Discussion:id13}
\sphinxAtStartPar
Rachel Qianwen Bi, Lukas R Dean, Jingpeng Tang, and Hyrum L Smith. Limitations of retirement planning software: examining variance between inputs and outputs. \sphinxstyleemphasis{Journal of Financial Service Professionals}, 2020.
\bibitem[Blo18]{Discussion:id25}
\sphinxAtStartPar
Daniel Alexandre Bloch. Machine learning: models and algorithms. \sphinxstyleemphasis{Machine Learning: Models And Algorithms, Quantitative Analytics}, 2018.
\bibitem[BS11]{Discussion:id18}
\sphinxAtStartPar
Kenneth Bruhn and Mogens Steffensen. Household consumption, investment and life insurance. \sphinxstyleemphasis{Insurance: Mathematics and Economics}, 48(3):315–325, 2011.
\bibitem[CL20]{Discussion:id24}
\sphinxAtStartPar
Shou Chen and Guangbing Li. Time\sphinxhyphen{}inconsistent preferences, consumption, investment and life insurance decisions. \sphinxstyleemphasis{Applied Economics Letters}, 27(5):392–399, 2020.
\bibitem[DPMSNR14]{Discussion:id31}
\sphinxAtStartPar
Albert De\sphinxhyphen{}Paz, Jesus Marin\sphinxhyphen{}Solano, Jorge Navas, and Oriol Roch. Consumption, investment and life insurance strategies with heterogeneous discounting. \sphinxstyleemphasis{Insurance: Mathematics and Economics}, 54:66–75, 2014.
\bibitem[Dol10]{Discussion:id61}
\sphinxAtStartPar
Victor Dolk. Survey reinforcement learning. \sphinxstyleemphasis{Eindhoven University of Technology}, 2010.
\bibitem[DMBE18]{Discussion:id15}
\sphinxAtStartPar
Taft Dorman, Barry S Mulholland, Qianwen Bi, and Harold Evensky. The efficacy of publicly\sphinxhyphen{}available retirement planning tools. \sphinxstyleemphasis{Available at SSRN 2732927}, 2018.
\bibitem[EWC21]{Discussion:id69}
\sphinxAtStartPar
Maria K Eckstein, Linda Wilbrecht, and Anne GE Collins. What do reinforcement learning models measure? interpreting model parameters in cognition and neuroscience. \sphinxstyleemphasis{Current Opinion in Behavioral Sciences}, 41:128–137, 2021.
\bibitem[FPT15]{Discussion:id43}
\sphinxAtStartPar
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. \sphinxstyleemphasis{arXiv preprint arXiv:1512.08562}, 2015.
\bibitem[FranccoisLHI+18]{Discussion:id72}
\sphinxAtStartPar
Vincent François\sphinxhyphen{}Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An introduction to deep reinforcement learning. \sphinxstyleemphasis{arXiv preprint arXiv:1811.12560}, 2018.
\bibitem[GulerLP19]{Discussion:id45}
\sphinxAtStartPar
Batuhan Güler, Alexis Laignelet, and Panos Parpas. Towards robust and stable deep learning algorithms for forward backward stochastic differential equations. \sphinxstyleemphasis{arXiv preprint arXiv:1910.11623}, 2019.
\bibitem[Ham18]{Discussion:id60}
\sphinxAtStartPar
Ahmad Hammoudeh. A concise introduction to reinforcement learning. 2018.
\bibitem[HJW17]{Discussion:id27}
\sphinxAtStartPar
Jiequn Han, Arnulf Jentzen, and E Weinan. Overcoming the curse of dimensionality: solving high\sphinxhyphen{}dimensional partial differential equations using deep learning. \sphinxstyleemphasis{arXiv preprint arXiv:1707.02568}, pages 1–13, 2017.
\bibitem[HJW18]{Discussion:id30}
\sphinxAtStartPar
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high\sphinxhyphen{}dimensional partial differential equations using deep learning. \sphinxstyleemphasis{Proceedings of the National Academy of Sciences}, 115(34):8505–8510, 2018.
\bibitem[Her11]{Discussion:id17}
\sphinxAtStartPar
Hal E Hershfield. Future self\sphinxhyphen{}continuity: how conceptions of the future self transform intertemporal choice. \sphinxstyleemphasis{Annals of the New York Academy of Sciences}, 1235:30, 2011.
\bibitem[KSL19]{Discussion:id10}
\sphinxAtStartPar
Sung\sphinxhyphen{}Kyun Kim, Oren Salzman, and Maxim Likhachev. Pomhdp: search\sphinxhyphen{}based belief space planning using multiple heuristics. In \sphinxstyleemphasis{Proceedings of the International Conference on Automated Planning and Scheduling}, volume 29, 734–744. 2019.
\bibitem[KS15]{Discussion:id20}
\sphinxAtStartPar
Morten Tolver Kronborg and Mogens Steffensen. Optimal consumption, investment and life insurance with surrender option guarantee. \sphinxstyleemphasis{Scandinavian Actuarial Journal}, 2015(1):59–87, 2015.
\bibitem[Leu94]{Discussion:id6}
\sphinxAtStartPar
Siu Fai Leung. Uncertain lifetime, the theory of the consumer, and the life cycle hypothesis. 1994.
\bibitem[Li16]{Discussion:id47}
\sphinxAtStartPar
\sphinxstylestrong{missing journal in li2016survey}
\bibitem[MVHS14]{Discussion:id62}
\sphinxAtStartPar
Ashique Rupam Mahmood, Hado Van Hasselt, and Richard S Sutton. Weighted importance sampling for off\sphinxhyphen{}policy learning with linear function approximation. In \sphinxstyleemphasis{NIPS}, 3014–3022. 2014.
\bibitem[Mer69]{Discussion:id3}
\sphinxAtStartPar
Robert C Merton. Lifetime portfolio selection under uncertainty: the continuous\sphinxhyphen{}time case. \sphinxstyleemphasis{The review of Economics and Statistics}, pages 247–257, 1969.
\bibitem[Mer75]{Discussion:id4}
\sphinxAtStartPar
Robert C Merton. Optimum consumption and portfolio rules in a continuous\sphinxhyphen{}time model. In \sphinxstyleemphasis{Stochastic Optimization Models in Finance}, pages 621–661. Elsevier, 1975.
\bibitem[MBJ20a]{Discussion:id7}
\sphinxAtStartPar
Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. A framework for reinforcement learning and planning. \sphinxstyleemphasis{arXiv preprint arXiv:2006.15009}, 2020.
\bibitem[MBJ20b]{Discussion:id71}
\sphinxAtStartPar
Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model\sphinxhyphen{}based reinforcement learning: a survey. \sphinxstyleemphasis{arXiv preprint arXiv:2006.16712}, 2020.
\bibitem[MJ20]{Discussion:id52}
\sphinxAtStartPar
Amit Kumar Mondal and N Jamali. A survey of reinforcement learning techniques: strategies, recent development, and future directions. \sphinxstyleemphasis{arXiv preprint arXiv:2001.06921}, 2020.
\bibitem[NZKN19]{Discussion:id49}
\sphinxAtStartPar
Farzad Niroui, Kaicheng Zhang, Zendai Kashino, and Goldie Nejat. Deep reinforcement learning robot for search and rescue applications: exploration in unknown cluttered environments. \sphinxstyleemphasis{IEEE Robotics and Automation Letters}, 4(2):610–617, 2019. \sphinxhref{https://doi.org/10.1109/LRA.2019.2891991}{doi:10.1109/LRA.2019.2891991}.
\bibitem[PRD96]{Discussion:id41}
\sphinxAtStartPar
Elena Pashenkova, Irina Rish, and Rina Dechter. Value iteration and policy iteration algorithms for markov decision problem. In \sphinxstyleemphasis{AAAI’96: Workshop on Structural Issues in Planning and Temporal Reasoning}. Citeseer, 1996.
\bibitem[PVW11]{Discussion:id16}
\sphinxAtStartPar
James M Poterba, Steven F Venti, and David A Wise. Were they prepared for retirement? financial status at advanced ages in the hrs and ahead cohorts. In \sphinxstyleemphasis{Investigations in the Economics of Aging}, pages 21–69. University of Chicago Press, 2011.
\bibitem[Rai18]{Discussion:id26}
\sphinxAtStartPar
Maziar Raissi. Forward\sphinxhyphen{}backward stochastic neural networks: deep learning of high\sphinxhyphen{}dimensional partial differential equations. \sphinxstyleemphasis{arXiv preprint arXiv:1804.07010}, 2018.
\bibitem[Ric75]{Discussion:id5}
\sphinxAtStartPar
Scott F Richard. Optimal consumption, portfolio and life insurance rules for an uncertain lived individual in a continuous time model. \sphinxstyleemphasis{Journal of Financial Economics}, 2(2):187–203, 1975.
\bibitem[RMM18]{Discussion:id51}
\sphinxAtStartPar
Lev Rozonoer, Boris Mirkin, and Ilya Muchnik. Braverman readings in machine learning. In \sphinxstyleemphasis{Key Ideas from Inception to Current State: International Conference Commemorating the 40th Anniversary of Emmanuil Braverman's Decease, Boston, MA Invited Talks. Cham: Springer International Publishing}. Springer, 2018.
\bibitem[San21]{Discussion:id50}
\sphinxAtStartPar
Nimish Sanghi. \sphinxstyleemphasis{Deep Reinforcement Learning with Python: With Pytorch, TensorFlow and OpenAI Gym}. Apress L. P, Berkeley, CA, 2021. ISBN 1484268083.
\bibitem[SW16]{Discussion:id22}
\sphinxAtStartPar
Yang Shen and Jiaqin Wei. Optimal investment\sphinxhyphen{}consumption\sphinxhyphen{}insurance with random parameters. \sphinxstyleemphasis{Scandinavian Actuarial Journal}, 2016(1):37–62, 2016.
\bibitem[SBLL19]{Discussion:id53}
\sphinxAtStartPar
Joohyun Shin, Thomas A Badgwell, Kuang\sphinxhyphen{}Hung Liu, and Jay H Lee. Reinforcement learning–overview of recent progress and implications for process control. \sphinxstyleemphasis{Computers \& Chemical Engineering}, 127:282–294, 2019.
\bibitem[SB18]{Discussion:id70}
\sphinxAtStartPar
Richard S Sutton and Andrew G Barto. \sphinxstyleemphasis{Reinforcement learning: An introduction}. MIT press, 2018.
\bibitem[VOW12]{Discussion:id42}
\sphinxAtStartPar
Martijn Van Otterlo and Marco Wiering. Reinforcement learning and markov decision processes. In \sphinxstyleemphasis{Reinforcement learning}, pages 3–42. Springer, 2012.
\bibitem[WZZ19]{Discussion:id38}
\sphinxAtStartPar
Haoran Wang, Thaleia Zariphopoulou, and Xun Yu Zhou. Exploration versus exploitation in reinforcement learning: a stochastic control approach. \sphinxstyleemphasis{Available at SSRN 3316387}, 2019.
\bibitem[WCJW20]{Discussion:id21}
\sphinxAtStartPar
Jiaqin Wei, Xiang Cheng, Zhuo Jin, and Hao Wang. Optimal consumption–investment and life\sphinxhyphen{}insurance purchase strategy for couples with correlated lifetimes. \sphinxstyleemphasis{Insurance: Mathematics and Economics}, 91:244–256, 2020.
\bibitem[WHJ17]{Discussion:id29}
\sphinxAtStartPar
E Weinan, Jiequn Han, and Arnulf Jentzen. Deep learning\sphinxhyphen{}based numerical methods for high\sphinxhyphen{}dimensional parabolic partial differential equations and backward stochastic differential equations. \sphinxstyleemphasis{Communications in Mathematics and Statistics}, 5(4):349–380, 2017.
\bibitem[WHJ20]{Discussion:id28}
\sphinxAtStartPar
\sphinxstylestrong{missing journal in weinan2020algorithms}
\bibitem[Yaa65]{Discussion:id2}
\sphinxAtStartPar
Menahem E Yaari. Uncertain lifetime, life insurance, and the theory of the consumer. \sphinxstyleemphasis{The Review of Economic Studies}, 32(2):137–150, 1965.
\bibitem[YLL+]{Discussion:id48}
\sphinxAtStartPar
\sphinxstylestrong{missing journal in yasuiempirical}
\bibitem[Ye06]{Discussion:id8}
\sphinxAtStartPar
Jinchun Ye. \sphinxstyleemphasis{Optimal life insurance purchase, consumption and portfolio under an uncertain life}. University of Illinois at Chicago, 2006.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}