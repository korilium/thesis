\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\selectlanguage *{english}
\@writefile{toc}{\selectlanguage *{english}}
\@writefile{lof}{\selectlanguage *{english}}
\@writefile{lot}{\selectlanguage *{english}}
\HyPL@Entry{2<</S/r>>}
\HyPL@Entry{4<</S/D>>}
\newlabel{abstract::doc}{{}{1}{}{section*.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Introduction:introduction}{{1}{3}{Introduction}{chapter.1}{}}
\newlabel{Introduction::doc}{{1}{3}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reinforcement Learning}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Reinforcement_learning:reinforcement-learning}{{2}{5}{Reinforcement Learning}{chapter.2}{}}
\newlabel{Reinforcement_learning::doc}{{2}{5}{Reinforcement Learning}{chapter.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces research fields involved in reinforcement learning}}{5}{figure.2.1}\protected@file@percent }
\newlabel{Reinforcement_learning:tree-fig}{{2.1}{5}{research fields involved in reinforcement learning}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Finite Markov Decision Processes}{6}{section.2.1}\protected@file@percent }
\newlabel{Reinforcement_learning:finite-markov-decision-processes}{{2.1}{6}{Finite Markov Decision Processes}{section.2.1}{}}
\newlabel{equation:Reinforcement_learning:return}{{2.1}{6}{Finite Markov Decision Processes}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces standard model reinforcement learning}}{6}{figure.2.2}\protected@file@percent }
\newlabel{Reinforcement_learning:standard-model-fig}{{2.2}{6}{standard model reinforcement learning}{figure.2.2}{}}
\newlabel{equation:Reinforcement_learning:value}{{2.2}{7}{Finite Markov Decision Processes}{equation.2.1.2}{}}
\newlabel{equation:Reinforcement_learning:BELL}{{2.3}{7}{Finite Markov Decision Processes}{equation.2.1.3}{}}
\newlabel{equation:Reinforcement_learning:state-action}{{2.4}{7}{Finite Markov Decision Processes}{equation.2.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces General backup diagram}}{8}{figure.2.3}\protected@file@percent }
\newlabel{Reinforcement_learning:backup-diagram-fig}{{2.3}{8}{General backup diagram}{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Generelized Policy Iteration, Model\sphinxhyphen {}based RL and Model\sphinxhyphen {}free RL}{8}{section.2.2}\protected@file@percent }
\newlabel{Reinforcement_learning:generelized-policy-iteration-model-based-rl-and-model-free-rl}{{2.2}{8}{Generelized Policy Iteration, Model\sphinxhyphen {}based RL and Model\sphinxhyphen {}free RL}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Generalized policy iteration}}{9}{figure.2.4}\protected@file@percent }
\newlabel{Reinforcement_learning:gpi-fig}{{2.4}{9}{Generalized policy iteration}{figure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Model\sphinxhyphen {}based Reinforcement Learning}}{9}{figure.2.5}\protected@file@percent }
\newlabel{Reinforcement_learning:model-based-rl}{{2.5}{9}{Model\sphinxhyphen {}based Reinforcement Learning}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Dynamic Programming, Monte Carlo Methods and Temporal\sphinxhyphen {}Difference Learning}{10}{subsection.2.2.1}\protected@file@percent }
\newlabel{Reinforcement_learning:dynamic-programming-monte-carlo-methods-and-temporal-difference-learning}{{2.2.1}{10}{Dynamic Programming, Monte Carlo Methods and Temporal\sphinxhyphen {}Difference Learning}{subsection.2.2.1}{}}
\newlabel{equation:Reinforcement_learning:Q-learning}{{2.5}{12}{Dynamic Programming, Monte Carlo Methods and Temporal\sphinxhyphen {}Difference Learning}{equation.2.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces monte carlo temporal difference and dynammic programming}}{12}{figure.2.6}\protected@file@percent }
\newlabel{Reinforcement_learning:diff-meth-fig}{{2.6}{12}{monte carlo temporal difference and dynammic programming}{figure.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Dimensions of a model\sphinxhyphen {}based reinforcement algorithm}{13}{subsection.2.2.2}\protected@file@percent }
\newlabel{Reinforcement_learning:dimensions-of-a-model-based-reinforcement-algorithm}{{2.2.2}{13}{Dimensions of a model\sphinxhyphen {}based reinforcement algorithm}{subsection.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces state\_space dimensions}}{13}{figure.2.7}\protected@file@percent }
\newlabel{Reinforcement_learning:state-space-fig}{{2.7}{13}{state\_space dimensions}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces consideration in calculating the cumulative return estimation}}{14}{figure.2.8}\protected@file@percent }
\newlabel{Reinforcement_learning:backups-bootstrap-fig}{{2.8}{14}{consideration in calculating the cumulative return estimation}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The dimensions of a reinforcment learning algorithm}}{16}{figure.2.9}\protected@file@percent }
\newlabel{Reinforcement_learning:dim-fig}{{2.9}{16}{The dimensions of a reinforcment learning algorithm}{figure.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Curse of Dimensionality and Model\sphinxhyphen {}based Deep Reinforcment Learning}{16}{section.2.3}\protected@file@percent }
\newlabel{Reinforcement_learning:curse-of-dimensionality-and-model-based-deep-reinforcment-learning}{{2.3}{16}{Curse of Dimensionality and Model\sphinxhyphen {}based Deep Reinforcment Learning}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}G\sphinxhyphen {}learning, a stochastic adaptation on Q\sphinxhyphen {}learning}{16}{section.2.4}\protected@file@percent }
\newlabel{Reinforcement_learning:g-learning-a-stochastic-adaptation-on-q-learning}{{2.4}{16}{G\sphinxhyphen {}learning, a stochastic adaptation on Q\sphinxhyphen {}learning}{section.2.4}{}}
\newlabel{equation:Reinforcement_learning:information cost}{{2.6}{17}{G\sphinxhyphen {}learning, a stochastic adaptation on Q\sphinxhyphen {}learning}{equation.2.4.6}{}}
\newlabel{equation:Reinforcement_learning:free-energy}{{2.7}{17}{G\sphinxhyphen {}learning, a stochastic adaptation on Q\sphinxhyphen {}learning}{equation.2.4.7}{}}
\newlabel{equation:Reinforcement_learning:free-q}{{2.8}{17}{G\sphinxhyphen {}learning, a stochastic adaptation on Q\sphinxhyphen {}learning}{equation.2.4.8}{}}
\newlabel{equation:Reinforcement_learning:free-energy2}{{2.9}{17}{G\sphinxhyphen {}learning, a stochastic adaptation on Q\sphinxhyphen {}learning}{equation.2.4.9}{}}
\newlabel{equation:Reinforcement_learning:soft-min}{{2.10}{17}{G\sphinxhyphen {}learning, a stochastic adaptation on Q\sphinxhyphen {}learning}{equation.2.4.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}The Deep Backward Stochastic Differential Equation Method}{18}{section.2.5}\protected@file@percent }
\newlabel{Reinforcement_learning:the-deep-backward-stochastic-differential-equation-method}{{2.5}{18}{The Deep Backward Stochastic Differential Equation Method}{section.2.5}{}}
\newlabel{equation:Reinforcement_learning:gen_form}{{2.11}{18}{The Deep Backward Stochastic Differential Equation Method}{equation.2.5.11}{}}
\newlabel{equation:Reinforcement_learning:stoch_con}{{2.12}{18}{The Deep Backward Stochastic Differential Equation Method}{equation.2.5.12}{}}
\newlabel{equation:Reinforcement_learning:identity}{{2.13}{18}{The Deep Backward Stochastic Differential Equation Method}{equation.2.5.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces neural network for Deep BSDE method}}{19}{figure.2.10}\protected@file@percent }
\newlabel{Reinforcement_learning:bsdn-fig}{{2.10}{19}{neural network for Deep BSDE method}{figure.2.10}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Financial Application of Reinforcement Learning}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Financial_application:financial-application-of-reinforcement-learning}{{3}{21}{Financial Application of Reinforcement Learning}{chapter.3}{}}
\newlabel{Financial_application::doc}{{3}{21}{Financial Application of Reinforcement Learning}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Optimal consumption, investment and life insurance in an intertemporal model}{21}{section.3.1}\protected@file@percent }
\newlabel{Financial_application:optimal-consumption-investment-and-life-insurance-in-an-intertemporal-model}{{3.1}{21}{Optimal consumption, investment and life insurance in an intertemporal model}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The model specifications}{22}{subsection.3.1.1}\protected@file@percent }
\newlabel{Financial_application:the-model-specifications}{{3.1.1}{22}{The model specifications}{subsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}dynamic programming principle}{23}{subsection.3.1.2}\protected@file@percent }
\newlabel{Financial_application:dynamic-programming-principle}{{3.1.2}{23}{dynamic programming principle}{subsection.3.1.2}{}}
\newlabel{equation:Financial_application:BELL}{{3.1}{24}{dynamic programming principle}{equation.3.1.1}{}}
\newlabel{equation:Financial_application:cons_cond}{{3.2}{24}{dynamic programming principle}{equation.3.1.2}{}}
\newlabel{equation:Financial_application:ins_cond}{{3.3}{24}{dynamic programming principle}{equation.3.1.3}{}}
\newlabel{equation:Financial_application:inv_cond}{{3.4}{24}{dynamic programming principle}{equation.3.1.4}{}}
\newlabel{equation:Financial_application:gen_form}{{3.5}{25}{dynamic programming principle}{equation.3.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}discussion}{27}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Discussion:discussion}{{4}{27}{discussion}{chapter.4}{}}
\newlabel{Discussion::doc}{{4}{27}{discussion}{chapter.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Appendix}{29}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Appendix:appendix}{{5}{29}{Appendix}{chapter.5}{}}
\newlabel{Appendix::doc}{{5}{29}{Appendix}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}pseudocode algorithms}{29}{section.5.1}\protected@file@percent }
\newlabel{Appendix:pseudocode-algorithms}{{5.1}{29}{pseudocode algorithms}{section.5.1}{}}
\bibcite{Discussion:id63}{ADBB17}
\bibcite{Discussion:id40}{BHB+20}
\bibcite{Discussion:id14}{BFH17}
\bibcite{Discussion:id13}{BDTS20}
\bibcite{Discussion:id25}{Blo18}
\bibcite{Discussion:id18}{BS11}
\bibcite{Discussion:id24}{CL20}
\bibcite{Discussion:id31}{DPMSNR14}
\bibcite{Discussion:id61}{Dol10}
\bibcite{Discussion:id15}{DMBE18}
\bibcite{Discussion:id69}{EWC21}
\bibcite{Discussion:id43}{FPT15}
\bibcite{Discussion:id72}{FranccoisLHI+18}
\bibcite{Discussion:id45}{GulerLP19}
\bibcite{Discussion:id60}{Ham18}
\bibcite{Discussion:id27}{HJW17}
\bibcite{Discussion:id30}{HJW18}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{31}{chapter*.3}\protected@file@percent }
\bibcite{Discussion:id17}{Her11}
\bibcite{Discussion:id10}{KSL19}
\bibcite{Discussion:id20}{KS15}
\bibcite{Discussion:id6}{Leu94}
\bibcite{Discussion:id47}{Li16}
\bibcite{Discussion:id62}{MVHS14}
\bibcite{Discussion:id3}{Mer69}
\bibcite{Discussion:id4}{Mer75}
\bibcite{Discussion:id7}{MBJ20a}
\bibcite{Discussion:id71}{MBJ20b}
\bibcite{Discussion:id52}{MJ20}
\bibcite{Discussion:id49}{NZKN19}
\bibcite{Discussion:id41}{PRD96}
\bibcite{Discussion:id16}{PVW11}
\bibcite{Discussion:id26}{Rai18}
\bibcite{Discussion:id5}{Ric75}
\bibcite{Discussion:id51}{RMM18}
\bibcite{Discussion:id50}{San21}
\bibcite{Discussion:id22}{SW16}
\bibcite{Discussion:id53}{SBLL19}
\bibcite{Discussion:id70}{SB18}
\bibcite{Discussion:id42}{VOW12}
\bibcite{Discussion:id38}{WZZ19}
\bibcite{Discussion:id21}{WCJW20}
\bibcite{Discussion:id29}{WHJ17}
\bibcite{Discussion:id28}{WHJ20}
\bibcite{Discussion:id2}{Yaa65}
\bibcite{Discussion:id48}{YLL+}
\bibcite{Discussion:id8}{Ye06}
\gdef \@abspage@last{37}
