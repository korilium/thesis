cd(@__DIR__)
using Pkg; Pkg.activate("."); Pkg.instantiate
using Flux, LinearAlgebra, Statistics
using Test, NeuralNetDiffEq
#parameters 

i = 50000f0
r = 0.04f0
ρ = 0.03f0
λ = 0.006125f0
η = 0.006125f0
γ = -3.0f0
T = 40f0

#solved  
d = 1 # number of dimensions
X0 = [0.3f0]
tspan = (0.0f0,40.0f0)
dt = 0.2f0
ts = tspan[1]:dt:tspan[2]
time_steps = length(ts) -1
m = 20f0


g(X) = (exp(-ρ*T)/(γ))*sum(X.^γ)
#f(X, u, σᵀ∇u, p, t) = λ * (exp(-ρ*t)/γ*sum(X)^γ - u) + (sum(σᵀ∇u)^γ * exp(ρ*t))^(-1/(1 -γ))*(1 - exp(γ))
f(X, u, σᵀ∇u, p, t) = λ .*(exp(-ρ*t)/γ*sum(X)^γ .- u) .+ (sum(σᵀ∇u)^γ * exp(ρ*t))^(-1/(1-γ)) * (exp(-ρ*t + γ)/γ - 1)
f(X, u, σᵀ∇u, p, t) = λ*exp(-ρ)/γ * sum(X^γ) .- u.*λ .+ (sum(σᵀ∇u.^(γ/(1-γ))) * exp(-ρ/(1-γ))) *  (exp(-ρ*t + γ)/γ - 1)
f(X, u, σᵀ∇u, p, t) =  sum(σᵀ∇u)
μ_f(X,p,t) = r* sum(X) + i
σ_f(X,p,t) = 0 
prob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)

hls = 12 + d #hidden layer size
opt = Flux.ADAM(0.03)  #optimizer
#sub-neural network approximating solutions at the desired point
u0 = Flux.Chain(Dense(d,hls,relu),
                Dense(hls,hls,relu),
                Dense(hls,1))
# sub-neural network approximating the spatial gradients at time point
σᵀ∇u = [Flux.Chain(Dense(d,hls,relu),
                   Dense(hls,hls,relu),
                   Dense(hls,hls,relu),
                   Dense(hls,d)) for i in 1 : time_steps]

alg = NNPDEHan(u0, σᵀ∇u, opt = opt)

ans = solve(prob, alg, verbose = true, abstol=1e-8, maxiters = 400, dt=dt, trajectories=m)


##### examples of other FBSTDE ######



# one-dimensional heat equation
x0 = [11.0f0]  # initial points
tspan = (0.0f0,5.0f0)
dt = 0.5   # time step
time_steps = div(tspan[2]-tspan[1],dt)
d = 1      # number of dimensions
m = 50     # number of trajectories (batch size)

g(X) = sum(X.^2)   # terminal condition
f(X,u,σᵀ∇u,p,t) = 0.0  # function from solved equation
μ(X,p,t) = 0.0
σ(X,p,t) = 1.0
prob = TerminalPDEProblem(g, f, μ, σ, x0, tspan)

hls = 10 + d #hidden layer size
opt = Flux.ADAM(0.005)  #optimizer
#sub-neural network approximating solutions at the desired point
u0 = Flux.Chain(Dense(d,hls,relu),
                Dense(hls,hls,relu),
                Dense(hls,1))
# sub-neural network approximating the spatial gradients at time point
σᵀ∇u = [Flux.Chain(Dense(d,hls,relu),
                  Dense(hls,hls,relu),
                  Dense(hls,d)) for i in 1:time_steps]

alg = NNPDEHan(u0, σᵀ∇u, opt = opt)

ans = solve(prob, alg, verbose = true, abstol=1e-8, maxiters = 200, dt=dt, trajectories=m)

u_analytical(x,t) = sum(x.^2) .+ d*t
analytical_ans = u_analytical(x0, tspan[end])

error_l2 = sqrt((ans-analytical_ans)^2/ans^2)

println("one-dimensional heat equation")
# println("numerical = ", ans)
# println("analytical = " ,analytical_ans)
println("error_l2 = ", error_l2, "\n")
@test error_l2 < 0.1





#example  Black scholes model 
d = 100 # number of dimensions
x0 = repeat([1.0f0, 0.5f0], div(d,2))
tspan = (0.0f0,1.0f0)
dt = 0.25
time_steps = div(tspan[2]-tspan[1],dt)
m = 100 # number of trajectories (batch size)

r = 0.05
sigma = 0.4
f(X,u,σᵀ∇u,p,t) = r * (u .- sum(X.*σᵀ∇u))
g(X) = sum(X.^2)
μ(X,p,t) = 0.0
σ(X,p,t) = Diagonal(sigma*X)
prob = TerminalPDEProblem(g, f, μ, σ, x0, tspan)

hls  = 10 + d #hide layer size
opt = Flux.ADAM(0.001)
u0 = Flux.Chain(Dense(d,hls,relu),
                Dense(hls,hls,relu),
                Dense(hls,1))
σᵀ∇u = [Flux.Chain(Dense(d,hls,relu),
                  Dense(hls,hls,relu),
                  Dense(hls,hls,relu),
                  Dense(hls,d)) for i in 1:time_steps]

alg = NNPDEHan(u0, σᵀ∇u, opt = opt)

ans = solve(prob, alg, verbose = true, abstol=1e-8, maxiters = 150, dt=dt, trajectories=m)

u_analytical(x, t) = exp((r + sigma^2).*(tspan[end] .- tspan[1])).*sum(x.^2)
analytical_ans = u_analytical(x0, tspan[1])
error_l2 = sqrt((ans .- analytical_ans)^2/ans^2)


#Hamilton Jacobi Bellman Equation
d = 20 # number of dimensions
x0 = fill(0.0f0,d)
tspan = (0.0f0, 1.0f0)
dt = 0.2
ts = tspan[1]:dt:tspan[2]
time_steps = length(ts)-1
m = 20 # number of trajectories (batch size)
λ = 1.0f0

g(X) = log(0.5 + 0.5*sum(X.^2))
f(X,u,σᵀ∇u,p,t) = -λ*sum(σᵀ∇u.^2)
μ_f(X,p,t) = 0.0
σ_f(X,p,t) = sqrt(2)
prob = TerminalPDEProblem(g, f, μ_f, σ_f, x0, tspan)

hls = 12 + d #hidden layer size
opt = Flux.ADAM(0.03)  #optimizer
#sub-neural network approximating solutions at the desired point
u0 = Flux.Chain(Dense(d,hls,relu),
                Dense(hls,hls,relu),
                Dense(hls,1))

# sub-neural network approximating the spatial gradients at time point
σᵀ∇u = [Flux.Chain(Dense(d,hls,relu),
                   Dense(hls,hls,relu),
                   Dense(hls,hls,relu),
                   Dense(hls,d)) for i in 1 : time_steps]

alg = NNPDEHan(u0, σᵀ∇u, opt = opt)

ans = solve(prob, alg, verbose = true, abstol=1e-8, maxiters = 200, dt=dt, trajectories=m)

T = tspan[2]
MC = 10^5
W() = randn(d,1)
u_analytical(x, t) = -(1/λ)*log(mean(exp(-λ*g(x .+ sqrt(2.0)*abs.(T-t).*W())) for _ = 1:MC))
analytical_ans = u_analytical(x0, tspan[1])

error_l2 = sqrt((ans - analytical_ans)^2/ans^2)

println("Hamilton Jacobi Bellman Equation")
# println("numerical = ", ans)
# println("analytical = " , analytical_ans)
println("error_l2 = ", error_l2, "\n")
@test error_l2 < 2.