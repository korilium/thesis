basic understanding for neural networks

Flux documentation : https://fluxml.ai/Flux.jl/stable/
    - long short term memory networks (LSTM) : https://colah.github.io/posts/2015-08-Understanding-LSTMs/
    - perceptron : https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6
    - Sigmoid Neuron : https://towardsdatascience.com/sigmoid-neuron-deep-neural-networks-a4cd35b629d7 
                        https://en.wikipedia.org/wiki/Sigmoid_function 
    - Convolution neural network : https://www.ibm.com/cloud/learn/convolutional-neural-networks
                                    http://courses.csail.mit.edu/18.337/2015/projects/Victor_Jakubiuk/fully_conv_networks_writeup.pdf 
    - one hot coding : https://www.educative.io/blog/one-hot-encoding 
    - basic layers of Flux : https://fluxml.ai/Flux.jl/stable/models/layers/ 
    - recurrent neural networks : http://learningjulia.com/2019/10/11/a-basic-rnn.html 
    - rectified Linear unit (ReLu): https://www.mygreatlearning.com/blog/relu-activation-function/ 

basic understanding for differential equations in julia

DifferentialEquation documentation: https://diffeq.sciml.ai/latest/
    - first tutorial introduction :  https://tutorials.sciml.ai/html/introduction/01-ode_introduction.html
    - second tutorial stiffness and best ODE algorithm: https://tutorials.sciml.ai/html/introduction/02-choosing_algs.html
    - 3th tutorial optimize DiffEq Code : https://tutorials.sciml.ai/html/introduction/03-optimizing_diffeq_code.html
        + integrator interface : https://diffeq.sciml.ai/dev/basics/integrator/
    - 4th tutorial callbacks and events : https://tutorials.sciml.ai/html/introduction/04-callbacks_and_events.html
    - exercises DifferentialEquation : https://tutorials.sciml.ai/html/exercises/01-workshop_exercises.html 
    - tutorial stochastic Differential Equations : https://diffeq.sciml.ai/dev/tutorials/sde_example/ 
    - tutorial Discrete Stochastic Equations : https://diffeq.sciml.ai/dev/tutorials/discrete_stochastic_example/
    - tutorial Bayesian Parameter Estimation : https://diffeqbayes.sciml.ai/dev/
    - automatic differentiation : https://mitmath.github.io/18337/lecture8/automatic_differentiation 
        + introduction to AD : https://www.youtube.com/watch?v=FtnkqIsfNQc for dummies like me 
        + https://mitmath.github.io/18337/lecture10/estimation_identification 
        + jupyter notebook for AD: https://github.com/MikeInnes/diff-zoo 
        + tutorial Solving Stiff Ordinary Differential Equations : https://mitmath.github.io/18337/lecture9/stiff_odes 
        + tutorial forward mode AD via High Dimensional Algebras: https://mitmath.github.io/18337/lecture8/automatic_differentiation 


basic understanding of combining flux with diffEq 

DifferentialEquationFlux documentation: https://diffeqflux.sciml.ai/dev/examples/optimization_ode/ 
    - tutorials ordinary Differential Equation : https://diffeqflux.sciml.ai/dev/examples/optimization_ode/ 
    - Local Sensitivity Analysis : https://diffeq.sciml.ai/latest/analysis/sensitivity/ 
    - tutorial Introduction to Julia for Scientific Machine Learning : https://mitmath.github.io/18S096SciML/lecture2/ml
        + Mathematics of Sensitivity Analysis : https://diffeq.sciml.ai/latest/extras/sensitivity_math/#sensitivity_math 
    - A Julia Library for Neural Differential Equations: https://julialang.org/blog/2019/01/fluxdiffeq/
    - tutorial Mixing Differential Equations and Machine Learning : https://mitmath.github.io/18S096SciML/lecture3/diffeq_ml 
    - Numerically Solving Partial Differential Equations : https://mitmath.github.io/18S096SciML/lecture4/pde_stiff 
    - 



extras: 
    - difference between Heap and Stack : http://net-informations.com/faq/net/stack-heap.htm
    - broadcasting - syntactic loop fusion - : https://julialang.org/blog/2017/01/moredots/
    - zygote :  https://fluxml.ai/Zygote.jl/latest/profiling/#Memory-Profiling-1 
    - Type-Dispatch Design : http://www.stochasticlifestyle.com/neural-jump-sdes-jump-diffusions-and-neural-pdes/ 
    - Optimizing Serial Code : https://mitmath.github.io/18337/lecture2/optimizing
